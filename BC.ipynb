{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "116382bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "Data path: Expert_data/expert_data_rllib_simple_push.pickle\n"
     ]
    }
   ],
   "source": [
    "#Imports & hyperparameters\n",
    "\"\"\"\n",
    "This cell imports the libraries needed for data loading and model training, \n",
    "sets up the compute device, specifies the path to the expert data, \n",
    "and defines key hyperparameters for the behavior cloning pipeline\n",
    "\"\"\"\n",
    "\n",
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Training device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Path to expert trajectories (simple_push)\n",
    "data_path = \"Expert_data/expert_data_rllib_simple_push.pickle\"\n",
    "\n",
    "# BC hyperparameters\n",
    "batch_size = 64\n",
    "lr = 1e-3\n",
    "epochs = 50\n",
    "hidden_dim = 64\n",
    "\n",
    "print(\"Device:\", device)\n",
    "print(\"Data path:\", data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dca6f47a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expert_data type: <class 'dict'>\n",
      "Agents available: ['adversary_0', 'agent_0']\n",
      "  adversary_0: 1200 transitions, state shape=(8,), action shape=()\n",
      "  agent_0: 1200 transitions, state shape=(19,), action shape=()\n"
     ]
    }
   ],
   "source": [
    "# Load & inspect expert_data\n",
    "\"\"\"\n",
    "This cell:\n",
    "1. Opens and loads the pickled expert trajectory data from disk.\n",
    "2. Prints a sanity check showing the data type and list of agents.\n",
    "3. Iterates over each agent’s data to display:\n",
    "   - The total number of (state, action) transitions collected.\n",
    "   - The shape of one example state and action for verification.\n",
    "\"\"\"\n",
    "\n",
    "# Load the pickled expert trajectories\n",
    "with open(data_path, \"rb\") as f:\n",
    "    expert_data = pickle.load(f)\n",
    "\n",
    "# Sanity check\n",
    "print(\"expert_data type:\", type(expert_data))\n",
    "print(\"Agents available:\", list(expert_data.keys()))\n",
    "\n",
    "# For each agent, print number of transitions and example shapes\n",
    "for agent, data in expert_data.items():\n",
    "    n_states  = len(data[\"states\"])\n",
    "    n_actions = len(data[\"actions\"])\n",
    "    # Peek at shapes\n",
    "    state_shape  = np.array(data[\"states\"][0]).shape\n",
    "    action_shape = np.array(data[\"actions\"][0]).shape\n",
    "    print(f\"  {agent}: {n_states} transitions, state shape={state_shape}, action shape={action_shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "70549eb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adversary_0 batch shapes → states: torch.Size([64, 8]), actions: torch.Size([64])\n",
      "agent_0 batch shapes → states: torch.Size([64, 19]), actions: torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "#Define Dataset & DataLoader for Each Agent\n",
    "\n",
    "\"\"\"\n",
    "This cell:\n",
    "1. Defines a PyTorch Dataset (`SingleAgentExpertDataset`) that wraps a single agent’s expert trajectories:\n",
    "   - Converts lists of NumPy state and action arrays into Torch tensors.\n",
    "   - Implements `__len__` and `__getitem__` for DataLoader compatibility.\n",
    "2. Creates one dataset and DataLoader per agent in `expert_data`, using the specified batch size and shuffling.\n",
    "3. Prints the shape of a sample batch (states and actions) for each agent to verify correct batching.\n",
    "\"\"\"\n",
    "class SingleAgentExpertDataset(Dataset):\n",
    "    def __init__(self, states, actions):\n",
    "        # Convert lists of numpy arrays into torch tensors\n",
    "        self.states  = torch.from_numpy(np.array(states)).float()\n",
    "        self.actions = torch.from_numpy(np.array(actions)).long()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.states)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.states[idx], self.actions[idx]\n",
    "\n",
    "# Instantiate datasets and loaders\n",
    "datasets = {}\n",
    "loaders  = {}\n",
    "\n",
    "for agent, data in expert_data.items():\n",
    "    ds     = SingleAgentExpertDataset(data[\"states\"], data[\"actions\"])\n",
    "    loader = DataLoader(ds, batch_size=batch_size, shuffle=True)\n",
    "    datasets[agent] = ds\n",
    "    loaders[agent]  = loader\n",
    "\n",
    "    # Print a batch shape for sanity\n",
    "    s_batch, a_batch = next(iter(loader))\n",
    "    print(f\"{agent} batch shapes → states: {s_batch.shape}, actions: {a_batch.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "34fcfc17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adversary_0 policy → obs_dim: 8, act_dim: 5\n",
      "BCPolicy(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=8, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=64, out_features=5, bias=True)\n",
      "  )\n",
      ")\n",
      "agent_0 policy → obs_dim: 19, act_dim: 5\n",
      "BCPolicy(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=19, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=64, out_features=5, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Behavior Cloning Policy Network Definition & Instantiation\n",
    "\"\"\"\n",
    "This cell:\n",
    "1. Imports PyTorch’s neural network module (`nn`) for layer definitions.\n",
    "2. Defines the `BCPolicy` class as a multi-layer perceptron with:\n",
    "   - An input layer matching the observation dimension (`obs_dim`).\n",
    "   - Two hidden layers of size `hidden_dim` with ReLU activations.\n",
    "   - An output layer producing raw logits for each discrete action (`act_dim`).\n",
    "3. Iterates over each agent’s dataset (`datasets`):\n",
    "   a. Reads the observation dimension from `ds.states.shape[1]`.\n",
    "   b. Computes the number of actions as `max(action) + 1`.\n",
    "   c. Instantiates a `BCPolicy` with those dimensions and moves it to the compute device.\n",
    "   d. Stores each policy in the `policies` dictionary for subsequent training.\n",
    "4. Prints each agent’s network architecture and parameter dimensions for verification.\n",
    "\"\"\"\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class BCPolicy(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, act_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# Instantiate one BCPolicy per agent\n",
    "policies = {}\n",
    "for agent, ds in datasets.items():\n",
    "    obs_dim = ds.states.shape[1]\n",
    "    # assume actions are 0...act_dim-1\n",
    "    act_dim = int(ds.actions.max().item()) + 1\n",
    "    policy = BCPolicy(obs_dim, act_dim, hidden_dim).to(device)\n",
    "    policies[agent] = policy\n",
    "    print(f\"{agent} policy → obs_dim: {obs_dim}, act_dim: {act_dim}\")\n",
    "    print(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "32abc797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training BC policy for adversary_0 ===\n",
      "adversary_0  Epoch  1/50: loss = 1.5706\n",
      "adversary_0  Epoch 10/50: loss = 0.9721\n",
      "adversary_0  Epoch 20/50: loss = 0.7313\n",
      "adversary_0  Epoch 30/50: loss = 0.6237\n",
      "adversary_0  Epoch 40/50: loss = 0.5498\n",
      "adversary_0  Epoch 50/50: loss = 0.5015\n",
      "\n",
      "=== Training BC policy for agent_0 ===\n",
      "agent_0  Epoch  1/50: loss = 1.5799\n",
      "agent_0  Epoch 10/50: loss = 1.0096\n",
      "agent_0  Epoch 20/50: loss = 0.6132\n",
      "agent_0  Epoch 30/50: loss = 0.4235\n",
      "agent_0  Epoch 40/50: loss = 0.3311\n",
      "agent_0  Epoch 50/50: loss = 0.2738\n"
     ]
    }
   ],
   "source": [
    "#Supervised Training Loop for Behavior Cloning Policies\n",
    "\n",
    "\"\"\"\n",
    "This cell:\n",
    "1. Imports PyTorch’s `nn` module for loss definition.\n",
    "2. Creates an Adam optimizer for each agent’s policy network with the specified learning rate.\n",
    "3. Defines the cross-entropy loss function to train the policy to match expert actions.\n",
    "4. Iterates through each agent:\n",
    "   a. Prints a header indicating which agent is being trained.\n",
    "   b. Runs a training loop over the specified number of epochs.\n",
    "   c. For each batch of expert states and actions:\n",
    "      - Moves data to the correct device.\n",
    "      - Computes action logits via the agent’s BCPolicy.\n",
    "      - Calculates the cross-entropy loss between logits and expert actions.\n",
    "      - Performs a backward pass and optimizer step.\n",
    "      - Accumulates batch losses.\n",
    "   d. Computes and prints the average loss at epoch 1 and every 10 epochs thereafter for monitoring.\n",
    "\"\"\"\n",
    "import torch.nn as nn\n",
    "\n",
    "# Set up optimizers and loss\n",
    "optimizers = {\n",
    "    agent: torch.optim.Adam(policy.parameters(), lr=lr)\n",
    "    for agent, policy in policies.items()\n",
    "}\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "for agent in policies:\n",
    "    print(f\"\\n=== Training BC policy for {agent} ===\")\n",
    "    policy    = policies[agent]\n",
    "    optimizer = optimizers[agent]\n",
    "    loader    = loaders[agent]\n",
    "    \n",
    "    for epoch in range(1, epochs+1):\n",
    "        total_loss = 0.0\n",
    "        for states, actions in loader:\n",
    "            states, actions = states.to(device), actions.to(device)\n",
    "            logits = policy(states)\n",
    "            loss   = loss_fn(logits, actions)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        avg_loss = total_loss / len(loader)\n",
    "        # Print every 10 epochs (and first epoch)\n",
    "        if epoch == 1 or epoch % 10 == 0:\n",
    "            print(f\"{agent}  Epoch {epoch:>2}/{epochs}: loss = {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ecf9df8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved adversary_0 policy to bc_adversary_0.pt\n",
      "Saved agent_0 policy to bc_agent_0.pt\n",
      "Reloaded adversary_0 policy from bc_adversary_0.pt successfully.\n",
      "Reloaded agent_0 policy from bc_agent_0.pt successfully.\n"
     ]
    }
   ],
   "source": [
    "#Save & Verify Behavior Cloning Policy Checkpoints\n",
    "\"\"\"\n",
    "This cell:\n",
    "1. Iterates over each trained policy in `policies`:\n",
    "   a. Constructs a filename `bc_<agent>.pt`.\n",
    "   b. Saves the policy’s state dictionary to disk.\n",
    "   c. Records the file path in `model_paths`.\n",
    "   d. Prints a confirmation message.\n",
    "2. Reloads each saved policy to ensure integrity:\n",
    "   a. Retrieves the observation and action dimensions from the corresponding dataset.\n",
    "   b. Reinstantiates a fresh `BCPolicy` with the same dimensions.\n",
    "   c. Loads the saved state dictionary into this new model.\n",
    "   d. Switches the model to evaluation mode.\n",
    "   e. Prints a success message confirming that the checkpoint loads correctly.\n",
    "\"\"\"\n",
    "\n",
    "model_paths = {}\n",
    "\n",
    "for agent, policy in policies.items():\n",
    "    path = f\"bc_{agent}.pt\"\n",
    "    torch.save(policy.state_dict(), path)\n",
    "    model_paths[agent] = path\n",
    "    print(f\"Saved {agent} policy to {path}\")\n",
    "\n",
    "# Reload to verify\n",
    "for agent, path in model_paths.items():\n",
    "    obs_dim = datasets[agent].states.shape[1]\n",
    "    act_dim = int(datasets[agent].actions.max().item()) + 1\n",
    "    # recreate the model\n",
    "    check_policy = BCPolicy(obs_dim, act_dim, hidden_dim).to(device)\n",
    "    check_policy.load_state_dict(torch.load(path))\n",
    "    check_policy.eval()\n",
    "    print(f\"Reloaded {agent} policy from {path} successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e485a0a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Skipping environment‐based evaluation: `pygame` (and SDL2) not installed.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Cell 7: Environment‐based Evaluation Stub for BC Policies (Guarded)\n",
    "\n",
    "This cell attempts to perform true environment rollouts of your BC policies \n",
    "in the `simple_push` environment. If the required `pygame` dependency is missing, \n",
    "it will skip rather than error out, allowing the rest of the notebook to run smoothly.\n",
    "\n",
    "Steps:\n",
    "1. Try importing the MPE `simple_push_v3` environment.\n",
    "2. If the import fails (pygame not installed), set a flag and print a warning.\n",
    "3. Only if the environment is available, run `evaluate_bc_agents`:\n",
    "   a. Create the parallel environment.\n",
    "   b. Loop over episodes, collecting actions from each BC policy.\n",
    "   c. Step the environment, accumulate per‐agent returns.\n",
    "   d. After all episodes, print average returns for each agent.\n",
    "\"\"\"\n",
    "# Attempt to import the environment\n",
    "try:\n",
    "    from pettingzoo.mpe import simple_push_v3\n",
    "    env_available = True\n",
    "except ImportError:\n",
    "    env_available = False\n",
    "    print(\"⚠️ Skipping environment‐based evaluation: `pygame` (and SDL2) not installed.\")\n",
    "\n",
    "# Only run rollouts if the env is actually available\n",
    "if env_available:\n",
    "    def evaluate_bc_agents(policies, num_episodes=20):\n",
    "        \"\"\"\n",
    "        Roll out BC policies in parallel simple_push env and print avg. returns.\n",
    "        \"\"\"\n",
    "        env = simple_push_v3.parallel_env(continuous_actions=True, max_cycles=25)\n",
    "        returns = {agent: [] for agent in env.agents}\n",
    "\n",
    "        for ep in range(num_episodes):\n",
    "            obs, _ = env.reset()\n",
    "            done = {a: False for a in env.agents}\n",
    "            ep_rewards = {a: 0.0 for a in env.agents}\n",
    "\n",
    "            while not all(done.values()):\n",
    "                actions = {}\n",
    "                for agent in env.agents:\n",
    "                    state = torch.from_numpy(obs[agent]).float().to(device)\n",
    "                    logits = policies[agent](state)\n",
    "                    actions[agent] = torch.argmax(logits).item()\n",
    "                obs, rewards, terminations, truncations, _ = env.step(actions)\n",
    "                for agent in env.agents:\n",
    "                    ep_rewards[agent] += rewards.get(agent, 0)\n",
    "                done = {\n",
    "                    a: terminations.get(a, False) or truncations.get(a, False)\n",
    "                    for a in env.agents\n",
    "                }\n",
    "\n",
    "            for agent in env.agents:\n",
    "                returns[agent].append(ep_rewards[agent])\n",
    "\n",
    "        env.close()\n",
    "\n",
    "        # Print average returns\n",
    "        for agent, vals in returns.items():\n",
    "            avg_ret = sum(vals) / len(vals)\n",
    "            print(f\"{agent} BC avg return over {num_episodes} eps: {avg_ret:.2f}\")\n",
    "\n",
    "    # Execute the evaluation\n",
    "    evaluate_bc_agents(policies, num_episodes=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "428f1718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Held-out expert classification accuracy per agent:\n",
      "  adversary_0: 86.00%\n",
      "  agent_0: 93.00%\n"
     ]
    }
   ],
   "source": [
    "# Held-out Expert Accuracy (no env needed)\n",
    "\"\"\"This cell:\n",
    "1. Ensures scikit-learn’s `accuracy_score` is available, installing it via pip if missing.\n",
    "2. Prints a header for the held-out classification accuracy evaluation.\n",
    "3. Defines `n_test` as the number of final samples to reserve for testing.\n",
    "4. Iterates over each agent’s dataset:\n",
    "   a. Converts the last `n_test` states and actions into tensors and arrays.\n",
    "   b. Runs a forward pass through the trained BC policy to obtain predicted actions.\n",
    "   c. Computes classification accuracy against the held-out expert actions.\n",
    "   d. Prints each agent’s accuracy as a percentage, giving a quick numeric measure of BC performance.\n",
    "\"\"\"\n",
    "# Install scikit-learn if missing\n",
    "import sys\n",
    "try:\n",
    "    from sklearn.metrics import accuracy_score\n",
    "except ImportError:\n",
    "    !{sys.executable} -m pip install scikit-learn\n",
    "    from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(\"Held-out expert classification accuracy per agent:\")\n",
    "\n",
    "# Use the last 200 samples as a “test” split\n",
    "n_test = 200\n",
    "\n",
    "for agent, ds in datasets.items():\n",
    "    states  = ds.states.cpu().numpy()\n",
    "    actions = ds.actions.cpu().numpy()\n",
    "\n",
    "    test_states  = torch.from_numpy(states[-n_test:]).to(device)\n",
    "    test_actions = actions[-n_test:]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = policies[agent](test_states)\n",
    "        preds  = torch.argmax(logits, dim=-1).cpu().numpy()\n",
    "\n",
    "    acc = accuracy_score(test_actions, preds)\n",
    "    print(f\"  {agent}: {acc*100:5.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe59e54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
