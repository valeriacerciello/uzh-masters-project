{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expert Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.0451982 ,  0.02113391, -0.01404097, -0.0441334 ],\n",
       "        [-0.04477552, -0.17378391, -0.01492364,  0.24408661],\n",
       "        [-0.0482512 , -0.36868957, -0.0100419 ,  0.53202516],\n",
       "        ...,\n",
       "        [ 0.18665619,  1.9400018 , -0.09427878, -2.3084128 ],\n",
       "        [ 0.22545622,  1.7458605 , -0.14044704, -2.046171  ],\n",
       "        [ 0.26037344,  1.552432  , -0.18137045, -1.8000408 ]],\n",
       "       dtype=float32),\n",
       " array([0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n",
       "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "def generate_expert_data(num_trajectories=10):\n",
    "    env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "    expert_states = []\n",
    "    expert_actions = []\n",
    "    \n",
    "    for _ in range(num_trajectories):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            # Heuristic expert: move left if pole leans left, else right\n",
    "            angle = state[2]  # Pole angle\n",
    "            action = 0 if angle < 0 else 1  # Simplified expert policy\n",
    "            expert_states.append(state)\n",
    "            expert_actions.append(action)\n",
    "            state, _, done, _, _ = env.step(action)\n",
    "    \n",
    "    return np.array(expert_states), np.array(expert_actions)\n",
    "\n",
    "\n",
    "\n",
    "expert_states, expert_actions = generate_expert_data()\n",
    "expert_states, expert_actions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Policy and Discriminator Networks\n",
    "The discriminator networks Concatenate state and action, This allows the network to learn a joint representation of the state and action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, action_dim),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, state):\n",
    "        return self.net(state)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim = 64):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim + action_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    # Concatenate state and action, This allows the network to learn a joint representation of the state and action\n",
    "    def forward(self, state, action):\n",
    "        sa = torch.cat([state, action], dim=-1)\n",
    "        return self.net(sa)\n",
    "\n",
    "# Initialize networks\n",
    "state_dim = 4  # CartPole state dimension\n",
    "action_dim = 2 # CartPole action space (0 or 1)\n",
    "policy = Policy(state_dim, action_dim)\n",
    "discriminator = Discriminator(state_dim, 1)  # Action is 0/1, so we encode as 0 or 1\n",
    "\n",
    "# # Test networks\n",
    "# state = torch.randn(1, state_dim)\n",
    "# action = torch.randint(0, 2, (1, 1))\n",
    "# print(\"Policy output:\", policy(state).shape)\n",
    "# print(\"Discriminator output:\", discriminator(state, action).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train GAIL\n",
    "\n",
    "#### Loss in GAN\n",
    "The discriminator's loss in GANs is usually something like maximizing log(D(real)) + log(1 - D(fake)), where D(real) is the probability that real data is real, and D(fake) is the probability that fake data is real\n",
    "\n",
    "Entropy Term\n",
    "$$\n",
    "H(\\pi(\\cdot \\mid s)) = - \\sum_{a \\in \\mathcal{A}} \\pi(a \\mid s) \\log (\\pi(a \\mid s))\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gail(expert_states, expert_actions, num_epochs=100, batch_size=32, λ=0.1):\n",
    "    # Initialize optimizers\n",
    "    policy_optim = optim.Adam(policy.parameters(), lr=1e-3)\n",
    "    disc_optim = optim.Adam(discriminator.parameters(), lr=1e-3)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # --- Step 1: Sample trajectories from current policy ---\n",
    "        env = gym.make(\"CartPole-v1\")\n",
    "        policy_states, policy_actions = [], []\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            state_tensor = torch.FloatTensor(state)\n",
    "            action_probs = policy(state_tensor)  # Get action probabilities from policy\n",
    "            action = torch.distributions.Categorical(action_probs).sample().item()  # Sample action\n",
    "            policy_states.append(state)\n",
    "            policy_actions.append(action)\n",
    "            state, _, done, _, _ = env.step(action)  # Execute action in environment\n",
    "        \n",
    "        # --- Step 2: Update Discriminator ---\n",
    "        # Prepare expert and policy data as state-action pairs\n",
    "        expert_sa = torch.cat([\n",
    "            torch.FloatTensor(expert_states),\n",
    "            torch.FloatTensor(expert_actions).unsqueeze(1)  # Shape: (N, state_dim + 1)\n",
    "        ], dim=1)\n",
    "        policy_sa = torch.cat([\n",
    "            torch.FloatTensor(policy_states),\n",
    "            torch.FloatTensor(policy_actions).unsqueeze(1)  # Shape: (M, state_dim + 1)\n",
    "        ], dim=1)\n",
    "        \n",
    "        # Discriminator loss components:\n",
    "        # - Maximize log(D(expert_sa)): Expert labeled as \"real\"\n",
    "        # - Maximize log(1 - D(policy_sa)): Policy labeled as \"fake\"\n",
    "        real_output = discriminator(expert_sa[:, :4], expert_sa[:, 4:])  # D(expert_sa)\n",
    "        fake_output = discriminator(policy_sa[:, :4], policy_sa[:, 4:])   # D(policy_sa)\n",
    "        real_loss = -torch.log(real_output).mean()  # -E[log(D(expert))]\n",
    "        fake_loss = -torch.log(1 - fake_output).mean()  # -E[log(1 - D(policy))]\n",
    "        disc_loss = real_loss + fake_loss  # Total loss\n",
    "        \n",
    "        # Update discriminator\n",
    "        disc_optim.zero_grad()\n",
    "        disc_loss.backward()\n",
    "        disc_optim.step()\n",
    "        \n",
    "        # --- Step 3: Update Policy using Discriminator as Reward ---\n",
    "        # Convert policy data to tensors\n",
    "        policy_states_tensor = torch.FloatTensor(policy_states)\n",
    "        policy_actions_tensor = torch.FloatTensor(policy_actions).unsqueeze(1)\n",
    "        \n",
    "        # Compute rewards: log(D(s,a)) \n",
    "        # the discriminator is not exactly a reward function, \n",
    "        # but rather a way to estimate the likelihood of a state-action pair being from the expert's policy.\n",
    "        # If the discriminator thinks it is more likely from expert then it will give a higher reward\n",
    "        # Thus we can use this as a reward signal to reinforce the actions that can fool the discriminator better\n",
    "        with torch.no_grad():\n",
    "            rewards = torch.log(discriminator(policy_states_tensor, policy_actions_tensor))\n",
    "        \n",
    "        # Compute policy loss:\n",
    "        # L = -E[log(π(a|s)) * reward] - λ * entropy(π)\n",
    "        action_probs = policy(policy_states_tensor)\n",
    "        entropy = -torch.sum(action_probs * torch.log(action_probs + 1e-10), dim=-1).mean()  # Avoid log(0)\n",
    "        log_probs = torch.log(action_probs.gather(1, policy_actions_tensor.long()))  # log(π(a|s))\n",
    "        # TODO Here the entropy term should also be negated?\n",
    "        policy_loss = -((log_probs * rewards).mean() - λ * entropy)\n",
    "        \n",
    "        # Update policy\n",
    "        policy_optim.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        policy_optim.step()\n",
    "        \n",
    "        print(f\"Epoch {epoch}: Disc Loss = {disc_loss.item():.3f}, Policy Loss = {policy_loss.item():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wangy\\AppData\\Local\\Temp\\ipykernel_29468\\2878004780.py:27: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:257.)\n",
      "  torch.FloatTensor(policy_states),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Disc Loss = 1.509, Policy Loss = -0.486\n",
      "Epoch 1: Disc Loss = 1.489, Policy Loss = -0.504\n",
      "Epoch 2: Disc Loss = 1.444, Policy Loss = -0.551\n",
      "Epoch 3: Disc Loss = 1.386, Policy Loss = -0.605\n",
      "Epoch 4: Disc Loss = 1.472, Policy Loss = -0.499\n",
      "Epoch 5: Disc Loss = 1.459, Policy Loss = -0.506\n",
      "Epoch 6: Disc Loss = 1.402, Policy Loss = -0.562\n",
      "Epoch 7: Disc Loss = 1.458, Policy Loss = -0.489\n",
      "Epoch 8: Disc Loss = 1.385, Policy Loss = -0.574\n",
      "Epoch 9: Disc Loss = 1.449, Policy Loss = -0.477\n",
      "Epoch 10: Disc Loss = 1.443, Policy Loss = -0.479\n",
      "Epoch 11: Disc Loss = 1.377, Policy Loss = -0.553\n",
      "Epoch 12: Disc Loss = 1.396, Policy Loss = -0.511\n",
      "Epoch 13: Disc Loss = 1.420, Policy Loss = -0.484\n",
      "Epoch 14: Disc Loss = 1.442, Policy Loss = -0.446\n",
      "Epoch 15: Disc Loss = 1.420, Policy Loss = -0.462\n",
      "Epoch 16: Disc Loss = 1.442, Policy Loss = -0.433\n",
      "Epoch 17: Disc Loss = 1.392, Policy Loss = -0.479\n",
      "Epoch 18: Disc Loss = 1.439, Policy Loss = -0.424\n",
      "Epoch 19: Disc Loss = 1.382, Policy Loss = -0.480\n",
      "Epoch 20: Disc Loss = 1.416, Policy Loss = -0.443\n",
      "Epoch 21: Disc Loss = 1.380, Policy Loss = -0.479\n",
      "Epoch 22: Disc Loss = 1.384, Policy Loss = -0.462\n",
      "Epoch 23: Disc Loss = 1.418, Policy Loss = -0.420\n",
      "Epoch 24: Disc Loss = 1.407, Policy Loss = -0.429\n",
      "Epoch 25: Disc Loss = 1.379, Policy Loss = -0.462\n",
      "Epoch 26: Disc Loss = 1.376, Policy Loss = -0.464\n",
      "Epoch 27: Disc Loss = 1.402, Policy Loss = -0.423\n",
      "Epoch 28: Disc Loss = 1.355, Policy Loss = -0.461\n",
      "Epoch 29: Disc Loss = 1.388, Policy Loss = -0.424\n",
      "Epoch 30: Disc Loss = 1.393, Policy Loss = -0.415\n",
      "Epoch 31: Disc Loss = 1.380, Policy Loss = -0.445\n",
      "Epoch 32: Disc Loss = 1.378, Policy Loss = -0.445\n",
      "Epoch 33: Disc Loss = 1.378, Policy Loss = -0.446\n",
      "Epoch 34: Disc Loss = 1.366, Policy Loss = -0.437\n",
      "Epoch 35: Disc Loss = 1.393, Policy Loss = -0.406\n",
      "Epoch 36: Disc Loss = 1.388, Policy Loss = -0.406\n",
      "Epoch 37: Disc Loss = 1.373, Policy Loss = -0.443\n",
      "Epoch 38: Disc Loss = 1.377, Policy Loss = -0.429\n",
      "Epoch 39: Disc Loss = 1.380, Policy Loss = -0.409\n",
      "Epoch 40: Disc Loss = 1.375, Policy Loss = -0.406\n",
      "Epoch 41: Disc Loss = 1.347, Policy Loss = -0.434\n",
      "Epoch 42: Disc Loss = 1.368, Policy Loss = -0.439\n",
      "Epoch 43: Disc Loss = 1.382, Policy Loss = -0.397\n",
      "Epoch 44: Disc Loss = 1.358, Policy Loss = -0.424\n",
      "Epoch 45: Disc Loss = 1.391, Policy Loss = -0.387\n",
      "Epoch 46: Disc Loss = 1.396, Policy Loss = -0.386\n",
      "Epoch 47: Disc Loss = 1.368, Policy Loss = -0.429\n",
      "Epoch 48: Disc Loss = 1.374, Policy Loss = -0.401\n",
      "Epoch 49: Disc Loss = 1.364, Policy Loss = -0.404\n",
      "Epoch 50: Disc Loss = 1.366, Policy Loss = -0.403\n",
      "Epoch 51: Disc Loss = 1.384, Policy Loss = -0.423\n",
      "Epoch 52: Disc Loss = 1.359, Policy Loss = -0.408\n",
      "Epoch 53: Disc Loss = 1.351, Policy Loss = -0.415\n",
      "Epoch 54: Disc Loss = 1.348, Policy Loss = -0.421\n",
      "Epoch 55: Disc Loss = 1.351, Policy Loss = -0.421\n",
      "Epoch 56: Disc Loss = 1.360, Policy Loss = -0.409\n",
      "Epoch 57: Disc Loss = 1.383, Policy Loss = -0.398\n",
      "Epoch 58: Disc Loss = 1.350, Policy Loss = -0.421\n",
      "Epoch 59: Disc Loss = 1.355, Policy Loss = -0.408\n",
      "Epoch 60: Disc Loss = 1.345, Policy Loss = -0.422\n",
      "Epoch 61: Disc Loss = 1.381, Policy Loss = -0.393\n",
      "Epoch 62: Disc Loss = 1.404, Policy Loss = -0.397\n",
      "Epoch 63: Disc Loss = 1.322, Policy Loss = -0.433\n",
      "Epoch 64: Disc Loss = 1.336, Policy Loss = -0.420\n",
      "Epoch 65: Disc Loss = 1.315, Policy Loss = -0.438\n",
      "Epoch 66: Disc Loss = 1.390, Policy Loss = -0.376\n",
      "Epoch 67: Disc Loss = 1.425, Policy Loss = -0.409\n",
      "Epoch 68: Disc Loss = 1.328, Policy Loss = -0.429\n",
      "Epoch 69: Disc Loss = 1.378, Policy Loss = -0.405\n",
      "Epoch 70: Disc Loss = 1.318, Policy Loss = -0.437\n",
      "Epoch 71: Disc Loss = 1.416, Policy Loss = -0.394\n",
      "Epoch 72: Disc Loss = 1.334, Policy Loss = -0.427\n",
      "Epoch 73: Disc Loss = 1.466, Policy Loss = -0.373\n",
      "Epoch 74: Disc Loss = 1.455, Policy Loss = -0.385\n",
      "Epoch 75: Disc Loss = 1.318, Policy Loss = -0.435\n",
      "Epoch 76: Disc Loss = 1.310, Policy Loss = -0.496\n",
      "Epoch 77: Disc Loss = 1.310, Policy Loss = -0.444\n",
      "Epoch 78: Disc Loss = 1.392, Policy Loss = -0.392\n",
      "Epoch 79: Disc Loss = 1.302, Policy Loss = -0.447\n",
      "Epoch 80: Disc Loss = 1.402, Policy Loss = -0.402\n",
      "Epoch 81: Disc Loss = 1.371, Policy Loss = -0.401\n",
      "Epoch 82: Disc Loss = 1.307, Policy Loss = -0.445\n",
      "Epoch 83: Disc Loss = 1.308, Policy Loss = -0.470\n",
      "Epoch 84: Disc Loss = 1.301, Policy Loss = -0.455\n",
      "Epoch 85: Disc Loss = 1.380, Policy Loss = -0.394\n",
      "Epoch 86: Disc Loss = 1.341, Policy Loss = -0.423\n",
      "Epoch 87: Disc Loss = 1.380, Policy Loss = -0.395\n",
      "Epoch 88: Disc Loss = 1.401, Policy Loss = -0.377\n",
      "Epoch 89: Disc Loss = 1.409, Policy Loss = -0.388\n",
      "Epoch 90: Disc Loss = 1.330, Policy Loss = -0.445\n",
      "Epoch 91: Disc Loss = 1.290, Policy Loss = -0.453\n",
      "Epoch 92: Disc Loss = 1.393, Policy Loss = -0.382\n",
      "Epoch 93: Disc Loss = 1.461, Policy Loss = -0.405\n",
      "Epoch 94: Disc Loss = 1.375, Policy Loss = -0.389\n",
      "Epoch 95: Disc Loss = 1.281, Policy Loss = -0.481\n",
      "Epoch 96: Disc Loss = 1.300, Policy Loss = -0.450\n",
      "Epoch 97: Disc Loss = 1.291, Policy Loss = -0.440\n",
      "Epoch 98: Disc Loss = 1.366, Policy Loss = -0.399\n",
      "Epoch 99: Disc Loss = 1.288, Policy Loss = -0.510\n"
     ]
    }
   ],
   "source": [
    "train_gail(expert_states, expert_actions, num_epochs=100, batch_size=32, λ=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_policy(policy, num_episodes=10):\n",
    "    env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "    for _ in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            with torch.no_grad():\n",
    "                action_probs = policy(torch.FloatTensor(state))\n",
    "            action = torch.argmax(action_probs).item()  # Take most probable action\n",
    "            state, _, done, _, _ = env.step(action)\n",
    "    env.close()\n",
    "\n",
    "test_policy(policy)  # Visualize the learned policy!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DQN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
