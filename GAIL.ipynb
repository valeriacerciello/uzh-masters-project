{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expert Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.0451982 ,  0.02113391, -0.01404097, -0.0441334 ],\n",
       "        [-0.04477552, -0.17378391, -0.01492364,  0.24408661],\n",
       "        [-0.0482512 , -0.36868957, -0.0100419 ,  0.53202516],\n",
       "        ...,\n",
       "        [ 0.18665619,  1.9400018 , -0.09427878, -2.3084128 ],\n",
       "        [ 0.22545622,  1.7458605 , -0.14044704, -2.046171  ],\n",
       "        [ 0.26037344,  1.552432  , -0.18137045, -1.8000408 ]],\n",
       "       dtype=float32),\n",
       " array([0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n",
       "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "def generate_expert_data(num_trajectories=10):\n",
    "    env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "    expert_states = []\n",
    "    expert_actions = []\n",
    "    \n",
    "    for _ in range(num_trajectories):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            # Heuristic expert: move left if pole leans left, else right\n",
    "            angle = state[2]  # Pole angle\n",
    "            action = 0 if angle < 0 else 1  # Simplified expert policy\n",
    "            expert_states.append(state)\n",
    "            expert_actions.append(action)\n",
    "            state, _, done, _, _ = env.step(action)\n",
    "    \n",
    "    return np.array(expert_states), np.array(expert_actions)\n",
    "\n",
    "\n",
    "\n",
    "expert_states, expert_actions = generate_expert_data()\n",
    "expert_states, expert_actions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Policy and Discriminator Networks\n",
    "The discriminator networks Concatenate state and action, This allows the network to learn a joint representation of the state and action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, action_dim),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, state):\n",
    "        return self.net(state)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim = 64):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim + action_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    # Concatenate state and action, This allows the network to learn a joint representation of the state and action\n",
    "    def forward(self, state, action):\n",
    "        sa = torch.cat([state, action], dim=-1)\n",
    "        return self.net(sa)\n",
    "\n",
    "# Initialize networks\n",
    "state_dim = 4  # CartPole state dimension\n",
    "action_dim = 2 # CartPole action space (0 or 1)\n",
    "policy = Policy(state_dim, action_dim)\n",
    "discriminator = Discriminator(state_dim, 1)  # Action is 0/1, so we encode as 0 or 1\n",
    "\n",
    "# # Test networks\n",
    "# state = torch.randn(1, state_dim)\n",
    "# action = torch.randint(0, 2, (1, 1))\n",
    "# print(\"Policy output:\", policy(state).shape)\n",
    "# print(\"Discriminator output:\", discriminator(state, action).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train GAIL\n",
    "\n",
    "#### Loss in GAN\n",
    "The discriminator's loss in GANs is usually something like maximizing log(D(real)) + log(1 - D(fake)), where D(real) is the probability that real data is real, and D(fake) is the probability that fake data is real\n",
    "\n",
    "Entropy Term\n",
    "$$\n",
    "H(\\pi(\\cdot \\mid s)) = - \\sum_{a \\in \\mathcal{A}} \\pi(a \\mid s) \\log (\\pi(a \\mid s))\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gail(expert_states, expert_actions, num_epochs=100, batch_size=32, λ=0.1):\n",
    "    # Initialize optimizers\n",
    "    policy_optim = optim.Adam(policy.parameters(), lr=1e-3)\n",
    "    disc_optim = optim.Adam(discriminator.parameters(), lr=1e-3)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # --- Step 1: Sample trajectories from current policy ---\n",
    "        env = gym.make(\"CartPole-v1\")\n",
    "        policy_states, policy_actions = [], []\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            state_tensor = torch.FloatTensor(state)\n",
    "            action_probs = policy(state_tensor)  # Get action probabilities from policy\n",
    "            action = torch.distributions.Categorical(action_probs).sample().item()  # Sample action\n",
    "            policy_states.append(state)\n",
    "            policy_actions.append(action)\n",
    "            state, _, done, _, _ = env.step(action)  # Execute action in environment\n",
    "        \n",
    "        # --- Step 2: Update Discriminator ---\n",
    "        # Prepare expert and policy data as state-action pairs\n",
    "        expert_sa = torch.cat([\n",
    "            torch.FloatTensor(expert_states),\n",
    "            torch.FloatTensor(expert_actions).unsqueeze(1)  # Shape: (N, state_dim + 1)\n",
    "        ], dim=1)\n",
    "        policy_sa = torch.cat([\n",
    "            torch.FloatTensor(policy_states),\n",
    "            torch.FloatTensor(policy_actions).unsqueeze(1)  # Shape: (M, state_dim + 1)\n",
    "        ], dim=1)\n",
    "        \n",
    "        # Discriminator loss components:\n",
    "        # - Maximize log(D(expert_sa)): Expert labeled as \"real\"\n",
    "        # - Maximize log(1 - D(policy_sa)): Policy labeled as \"fake\"\n",
    "        real_output = discriminator(expert_sa[:, :4], expert_sa[:, 4:])  # D(expert_sa)\n",
    "        fake_output = discriminator(policy_sa[:, :4], policy_sa[:, 4:])   # D(policy_sa)\n",
    "        real_loss = -torch.log(real_output).mean()  # -E[log(D(expert))]\n",
    "        fake_loss = -torch.log(1 - fake_output).mean()  # -E[log(1 - D(policy))]\n",
    "        disc_loss = real_loss + fake_loss  # Total loss\n",
    "        \n",
    "        # Update discriminator\n",
    "        disc_optim.zero_grad()\n",
    "        disc_loss.backward()\n",
    "        disc_optim.step()\n",
    "        \n",
    "        # --- Step 3: Update Policy using Discriminator as Reward ---\n",
    "        # Convert policy data to tensors\n",
    "        policy_states_tensor = torch.FloatTensor(policy_states)\n",
    "        policy_actions_tensor = torch.FloatTensor(policy_actions).unsqueeze(1)\n",
    "        \n",
    "        # Compute rewards: log(D(s,a)) \n",
    "        # the discriminator is not exactly a reward function, \n",
    "        # but rather a way to estimate the likelihood of a state-action pair being from the expert's policy.\n",
    "        # If the discriminator thinks it is more likely from expert then it will give a higher reward\n",
    "        # Thus we can use this as a reward signal to reinforce the actions that can fool the discriminator better\n",
    "        with torch.no_grad():\n",
    "            rewards = torch.log(discriminator(policy_states_tensor, policy_actions_tensor))\n",
    "        \n",
    "        # Compute policy loss:\n",
    "        # L = -E[log(π(a|s)) * reward] - λ * entropy(π)\n",
    "        action_probs = policy(policy_states_tensor)\n",
    "        entropy = -torch.sum(action_probs * torch.log(action_probs + 1e-10), dim=-1).mean()  # Avoid log(0)\n",
    "        log_probs = torch.log(action_probs.gather(1, policy_actions_tensor.long()))  # log(π(a|s))\n",
    "        # TODO mind the sign of entropy here\n",
    "        # Here we flip the sign cuz we wanna minimizing\n",
    "        # We wanna maximizing/reinforce the action with high reward, that equals to minimizing the negative reward\n",
    "        # The entropy is positive, thus minimizing the negative entropy is maximizing the entropy\n",
    "        policy_loss = -(log_probs * rewards).mean() - λ * entropy\n",
    "        \n",
    "        # Update policy\n",
    "        policy_optim.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        policy_optim.step()\n",
    "        \n",
    "        print(f\"Epoch {epoch}: Disc Loss = {disc_loss.item():.3f}, Policy Loss = {policy_loss.item():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Disc Loss = 1.343, Policy Loss = -0.412\n",
      "Epoch 1: Disc Loss = 1.345, Policy Loss = -0.430\n",
      "Epoch 2: Disc Loss = 1.389, Policy Loss = -0.479\n",
      "Epoch 3: Disc Loss = 1.222, Policy Loss = -0.492\n",
      "Epoch 4: Disc Loss = 1.252, Policy Loss = -0.436\n",
      "Epoch 5: Disc Loss = 1.256, Policy Loss = -0.431\n",
      "Epoch 6: Disc Loss = 1.286, Policy Loss = -0.435\n",
      "Epoch 7: Disc Loss = 1.215, Policy Loss = -0.476\n",
      "Epoch 8: Disc Loss = 1.186, Policy Loss = -0.486\n",
      "Epoch 9: Disc Loss = 1.199, Policy Loss = -0.490\n",
      "Epoch 10: Disc Loss = 1.310, Policy Loss = -0.481\n",
      "Epoch 11: Disc Loss = 1.235, Policy Loss = -0.439\n",
      "Epoch 12: Disc Loss = 1.223, Policy Loss = -0.455\n",
      "Epoch 13: Disc Loss = 1.226, Policy Loss = -0.479\n",
      "Epoch 14: Disc Loss = 1.289, Policy Loss = -0.486\n",
      "Epoch 15: Disc Loss = 1.321, Policy Loss = -0.431\n",
      "Epoch 16: Disc Loss = 1.271, Policy Loss = -0.479\n",
      "Epoch 17: Disc Loss = 1.274, Policy Loss = -0.420\n",
      "Epoch 18: Disc Loss = 1.235, Policy Loss = -0.505\n",
      "Epoch 19: Disc Loss = 1.217, Policy Loss = -0.483\n",
      "Epoch 20: Disc Loss = 1.333, Policy Loss = -0.425\n",
      "Epoch 21: Disc Loss = 1.332, Policy Loss = -0.473\n",
      "Epoch 22: Disc Loss = 1.258, Policy Loss = -0.433\n",
      "Epoch 23: Disc Loss = 1.247, Policy Loss = -0.448\n",
      "Epoch 24: Disc Loss = 1.238, Policy Loss = -0.445\n",
      "Epoch 25: Disc Loss = 1.184, Policy Loss = -0.501\n",
      "Epoch 26: Disc Loss = 1.198, Policy Loss = -0.534\n",
      "Epoch 27: Disc Loss = 1.198, Policy Loss = -0.498\n",
      "Epoch 28: Disc Loss = 1.288, Policy Loss = -0.426\n",
      "Epoch 29: Disc Loss = 1.237, Policy Loss = -0.443\n",
      "Epoch 30: Disc Loss = 1.368, Policy Loss = -0.414\n",
      "Epoch 31: Disc Loss = 1.168, Policy Loss = -0.511\n",
      "Epoch 32: Disc Loss = 1.238, Policy Loss = -0.442\n",
      "Epoch 33: Disc Loss = 1.264, Policy Loss = -0.436\n",
      "Epoch 34: Disc Loss = 1.407, Policy Loss = -0.360\n",
      "Epoch 35: Disc Loss = 1.187, Policy Loss = -0.488\n",
      "Epoch 36: Disc Loss = 1.195, Policy Loss = -0.478\n",
      "Epoch 37: Disc Loss = 1.196, Policy Loss = -0.474\n",
      "Epoch 38: Disc Loss = 1.268, Policy Loss = -0.479\n",
      "Epoch 39: Disc Loss = 1.288, Policy Loss = -0.416\n",
      "Epoch 40: Disc Loss = 1.174, Policy Loss = -0.504\n",
      "Epoch 41: Disc Loss = 1.296, Policy Loss = -0.435\n",
      "Epoch 42: Disc Loss = 1.160, Policy Loss = -0.508\n",
      "Epoch 43: Disc Loss = 1.186, Policy Loss = -0.510\n",
      "Epoch 44: Disc Loss = 1.194, Policy Loss = -0.500\n",
      "Epoch 45: Disc Loss = 1.196, Policy Loss = -0.488\n",
      "Epoch 46: Disc Loss = 1.191, Policy Loss = -0.490\n",
      "Epoch 47: Disc Loss = 1.235, Policy Loss = -0.427\n",
      "Epoch 48: Disc Loss = 1.205, Policy Loss = -0.453\n",
      "Epoch 49: Disc Loss = 1.197, Policy Loss = -0.455\n",
      "Epoch 50: Disc Loss = 1.332, Policy Loss = -0.425\n",
      "Epoch 51: Disc Loss = 1.179, Policy Loss = -0.505\n",
      "Epoch 52: Disc Loss = 1.250, Policy Loss = -0.441\n",
      "Epoch 53: Disc Loss = 1.274, Policy Loss = -0.464\n",
      "Epoch 54: Disc Loss = 1.172, Policy Loss = -0.460\n",
      "Epoch 55: Disc Loss = 1.172, Policy Loss = -0.489\n",
      "Epoch 56: Disc Loss = 1.152, Policy Loss = -0.478\n",
      "Epoch 57: Disc Loss = 1.151, Policy Loss = -0.493\n",
      "Epoch 58: Disc Loss = 1.189, Policy Loss = -0.457\n",
      "Epoch 59: Disc Loss = 1.194, Policy Loss = -0.442\n",
      "Epoch 60: Disc Loss = 1.258, Policy Loss = -0.424\n",
      "Epoch 61: Disc Loss = 1.248, Policy Loss = -0.496\n",
      "Epoch 62: Disc Loss = 1.246, Policy Loss = -0.530\n",
      "Epoch 63: Disc Loss = 1.243, Policy Loss = -0.424\n",
      "Epoch 64: Disc Loss = 1.160, Policy Loss = -0.469\n",
      "Epoch 65: Disc Loss = 1.216, Policy Loss = -0.430\n",
      "Epoch 66: Disc Loss = 1.310, Policy Loss = -0.466\n",
      "Epoch 67: Disc Loss = 1.271, Policy Loss = -0.452\n",
      "Epoch 68: Disc Loss = 1.277, Policy Loss = -0.405\n",
      "Epoch 69: Disc Loss = 1.160, Policy Loss = -0.515\n",
      "Epoch 70: Disc Loss = 1.245, Policy Loss = -0.426\n",
      "Epoch 71: Disc Loss = 1.188, Policy Loss = -0.469\n",
      "Epoch 72: Disc Loss = 1.177, Policy Loss = -0.514\n",
      "Epoch 73: Disc Loss = 1.148, Policy Loss = -0.532\n",
      "Epoch 74: Disc Loss = 1.240, Policy Loss = -0.485\n",
      "Epoch 75: Disc Loss = 1.229, Policy Loss = -0.433\n",
      "Epoch 76: Disc Loss = 1.176, Policy Loss = -0.501\n",
      "Epoch 77: Disc Loss = 1.157, Policy Loss = -0.524\n",
      "Epoch 78: Disc Loss = 1.152, Policy Loss = -0.484\n",
      "Epoch 79: Disc Loss = 1.295, Policy Loss = -0.439\n",
      "Epoch 80: Disc Loss = 1.366, Policy Loss = -0.470\n",
      "Epoch 81: Disc Loss = 1.287, Policy Loss = -0.454\n",
      "Epoch 82: Disc Loss = 1.178, Policy Loss = -0.520\n",
      "Epoch 83: Disc Loss = 1.195, Policy Loss = -0.477\n",
      "Epoch 84: Disc Loss = 1.149, Policy Loss = -0.506\n",
      "Epoch 85: Disc Loss = 1.183, Policy Loss = -0.500\n",
      "Epoch 86: Disc Loss = 1.185, Policy Loss = -0.509\n",
      "Epoch 87: Disc Loss = 1.250, Policy Loss = -0.502\n",
      "Epoch 88: Disc Loss = 1.190, Policy Loss = -0.498\n",
      "Epoch 89: Disc Loss = 1.124, Policy Loss = -0.537\n",
      "Epoch 90: Disc Loss = 1.168, Policy Loss = -0.483\n",
      "Epoch 91: Disc Loss = 1.165, Policy Loss = -0.501\n",
      "Epoch 92: Disc Loss = 1.141, Policy Loss = -0.532\n",
      "Epoch 93: Disc Loss = 1.277, Policy Loss = -0.417\n",
      "Epoch 94: Disc Loss = 1.101, Policy Loss = -0.596\n",
      "Epoch 95: Disc Loss = 1.231, Policy Loss = -0.476\n",
      "Epoch 96: Disc Loss = 1.139, Policy Loss = -0.505\n",
      "Epoch 97: Disc Loss = 1.161, Policy Loss = -0.502\n",
      "Epoch 98: Disc Loss = 1.203, Policy Loss = -0.472\n",
      "Epoch 99: Disc Loss = 1.095, Policy Loss = -0.508\n"
     ]
    }
   ],
   "source": [
    "train_gail(expert_states, expert_actions, num_epochs=100, batch_size=32, λ=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m             state, _, done, _, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m     11\u001b[0m     env\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m---> 13\u001b[0m \u001b[43mtest_policy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Visualize the learned policy!\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[14], line 10\u001b[0m, in \u001b[0;36mtest_policy\u001b[1;34m(policy, num_episodes)\u001b[0m\n\u001b[0;32m      8\u001b[0m             action_probs \u001b[38;5;241m=\u001b[39m policy(torch\u001b[38;5;241m.\u001b[39mFloatTensor(state))\n\u001b[0;32m      9\u001b[0m         action \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(action_probs)\u001b[38;5;241m.\u001b[39mitem()  \u001b[38;5;66;03m# Take most probable action\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m         state, _, done, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m env\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\wangy\\anaconda3\\envs\\DQN\\Lib\\site-packages\\gymnasium\\wrappers\\common.py:125\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: ActType\n\u001b[0;32m    114\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[ObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[0;32m    115\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    123\u001b[0m \n\u001b[0;32m    124\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 125\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    126\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    128\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[1;32mc:\\Users\\wangy\\anaconda3\\envs\\DQN\\Lib\\site-packages\\gymnasium\\wrappers\\common.py:393\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[0;32m    392\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 393\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\wangy\\anaconda3\\envs\\DQN\\Lib\\site-packages\\gymnasium\\core.py:322\u001b[0m, in \u001b[0;36mWrapper.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\n\u001b[0;32m    319\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: WrapperActType\n\u001b[0;32m    320\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[0;32m    321\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 322\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\wangy\\anaconda3\\envs\\DQN\\Lib\\site-packages\\gymnasium\\wrappers\\common.py:285\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    283\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\wangy\\anaconda3\\envs\\DQN\\Lib\\site-packages\\gymnasium\\envs\\classic_control\\cartpole.py:223\u001b[0m, in \u001b[0;36mCartPoleEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    220\u001b[0m     reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sutton_barto_reward \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 223\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    225\u001b[0m \u001b[38;5;66;03m# truncation=False as the time limit is handled by the `TimeLimit` wrapper added during `make`\u001b[39;00m\n\u001b[0;32m    226\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32), reward, terminated, \u001b[38;5;28;01mFalse\u001b[39;00m, {}\n",
      "File \u001b[1;32mc:\\Users\\wangy\\anaconda3\\envs\\DQN\\Lib\\site-packages\\gymnasium\\envs\\classic_control\\cartpole.py:337\u001b[0m, in \u001b[0;36mCartPoleEnv.render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    335\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    336\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mevent\u001b[38;5;241m.\u001b[39mpump()\n\u001b[1;32m--> 337\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrender_fps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    338\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mdisplay\u001b[38;5;241m.\u001b[39mflip()\n\u001b[0;32m    340\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrgb_array\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def test_policy(policy, num_episodes=10):\n",
    "    env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "    for _ in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            with torch.no_grad():\n",
    "                action_probs = policy(torch.FloatTensor(state))\n",
    "            action = torch.argmax(action_probs).item()  # Take most probable action\n",
    "            state, _, done, _, _ = env.step(action)\n",
    "    env.close()\n",
    "\n",
    "test_policy(policy)  # Visualize the learned policy!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DQN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
