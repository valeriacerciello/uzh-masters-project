{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3x3 gridworld with two agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When multiple agents simultaneously interact, the environment is no longer stationary because each agent’s policy influences the environment dynamics. This setting is modeled as a general-sum stochastic game:\n",
    "\n",
    "\n",
    "$\\mathcal{G} = \\langle S, (A_1, A_2), (r_1, r_2), p, \\gamma \\rangle$\n",
    "\n",
    "where:\n",
    "\n",
    "Each agent i has its own action space $A_i$.\n",
    "\n",
    "Each agent receives a reward \n",
    "    $r_i$: $S \\times A_1 \\times A_2 \\to \\mathbb{R}$.\n",
    "\n",
    "The state transition depends on both agents’ actions.\n",
    "\n",
    "A major problem is how to define optimality when there are multiple agents.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Nash Q-learning, agents do not maximize their own Q-values independently, but instead assume that both agents will play according to Nash equilibria at each stage of the game.\n",
    "\n",
    "Instead of:\n",
    "\n",
    "\n",
    "$Q(s, a) \\leftarrow (1 - \\alpha) Q(s, a) + \\alpha \\Big( r + \\gamma \\max_{a{\\prime}} Q(s{\\prime}, a{\\prime}) \\Big)$\n",
    "\n",
    "\n",
    "it updates according to:\n",
    "\n",
    "\n",
    "$Q_i(s, a_1, a_2) \\leftarrow (1 - \\alpha) Q_i(s, a_1, a_2) + \\alpha \\Big( r_i + \\gamma \\cdot \\text{Nash}(Q(s{\\prime})) \\Big)$\n",
    "\n",
    "\n",
    "where Nash(Q(s’)) represents the expected Q-value under a Nash equilibrium strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each state  s , a stage game is formed from the current Q-values:\n",
    "\n",
    "A payoff matrix is constructed for each agent from $Q(s, a_1, a_2)$.\n",
    "\n",
    "The Lemke-Howson algorithm is used to compute a Nash equilibrium strategy.\n",
    "\n",
    "The Nash equilibrium expected values are used for Q-value updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The gridworld game is a 3x3 board where two agent start from random positions and try to reach their goals\n",
    "\n",
    "# possible actions\n",
    "ACTION_UP = 0\n",
    "ACTION_DOWN = 1\n",
    "ACTION_LEFT = 2\n",
    "ACTION_RIGHT = 3\n",
    "\n",
    "# state representation: \n",
    "# the state space is represented by a 9-element grid indexed from 0 to 8\n",
    "# each state is a tuple: (agent1's position, agent2's position)\n",
    "WORLD_HEIGHT = 3\n",
    "WORLD_WIDTH = 3\n",
    "gridIndexList = []\n",
    "for i in range(0, WORLD_HEIGHT):\n",
    "    for j in range(0, WORLD_WIDTH):\n",
    "        gridIndexList.append(WORLD_WIDTH * i + j)\n",
    "\n",
    "# valid moves are stored for each position to prevent illegal actions\n",
    "locationValidActions = {}\n",
    "for i in gridIndexList:\n",
    "    locationValidActions[i] = []\n",
    "\n",
    "for i in range(0, WORLD_HEIGHT):\n",
    "    for j in range(0, WORLD_WIDTH):\n",
    "        gridIndexNumber = WORLD_WIDTH * i + j\n",
    "        if i != WORLD_HEIGHT - 1:\n",
    "            locationValidActions[gridIndexNumber].append(ACTION_UP)\n",
    "        if i != 0:\n",
    "            locationValidActions[gridIndexNumber].append(ACTION_DOWN)\n",
    "        if j != 0:\n",
    "            locationValidActions[gridIndexNumber].append(ACTION_LEFT)\n",
    "        if j != WORLD_WIDTH - 1:\n",
    "            locationValidActions[gridIndexNumber].append(ACTION_RIGHT)\n",
    "\n",
    "# each agent maintains:\n",
    "# a Q-table storing joint action values\n",
    "# a learning rate for updating values\n",
    "# a policy strategy (not used now)\n",
    "class agent:\n",
    "    def __init__(self, agentIndex=0, startLocationIndex=0):\n",
    "        self.qTable = {}  # Stores Q-values\n",
    "        self.alpha = {}  # Learning rates\n",
    "        self.timeNumber = {}  # Number of visits\n",
    "        self.strategy = {}  # Policy (not used in this experiment)\n",
    "        self.agentIndex = agentIndex  # Identifies agent 0 or 1\n",
    "        self.locationIndex = startLocationIndex  # Initial position\n",
    "\n",
    "# each agent keeps Q-values for both agents\n",
    "# joint actions (a1, a2) are stored\n",
    "def initialSelfQTable(self):\n",
    "    self.qTable[0] = {}\n",
    "    self.qTable[1] = {}\n",
    "    for i in statesAllOne:\n",
    "        self.qTable[0][i] = {}\n",
    "        self.qTable[1][i] = {}\n",
    "        for j_1 in locationValidActions[i[0]]:\n",
    "            for j_2 in locationValidActions[i[1]]:\n",
    "                self.qTable[0][i][(j_1, j_2)] = 0\n",
    "                self.qTable[1][i][(j_1, j_2)] = 0\n",
    "\n",
    "# construct payoff matrices for both agents\n",
    "# use lemke-howson algorithm to compute nash equilibrium\n",
    "def constructPayoffTable(self, state):\n",
    "    actions0 = locationValidActions[state[0]]\n",
    "    actions1 = locationValidActions[state[1]]\n",
    "    m0 = matrix.Matrix(len(actions0), len(actions1))\n",
    "    m1 = matrix.Matrix(len(actions0), len(actions1))\n",
    "    for i in range(len(actions0)):\n",
    "        for j in range(len(actions1)):\n",
    "            m0.setItem(i+1, j+1, self.qTable[0][state][(actions0[i], actions1[j])])\n",
    "            m1.setItem(i+1, j+1, self.qTable[1][state][(actions0[i], actions1[j])])\n",
    "    return (m0, m1)\n",
    "\n",
    "# agents randomly explore actions \n",
    "# nash Q-learning updates are applied\n",
    "def playGameOne(agent_0 = agent, agent_1 = agent):\n",
    "    gamma = 0.9\n",
    "    agent_0.initialSelfQTable()\n",
    "    agent_1.initialSelfQTable()\n",
    "    agent_0.initialSelfAlpha()\n",
    "    agent_1.initialSelfAlpha()\n",
    "    while episodes < 100:\n",
    "        while True:\n",
    "            agent0Action = agent_0.chooseActionRandomly(currentState)\n",
    "            agent1Action = agent_1.chooseActionRandomly(currentState)\n",
    "            reward_0, reward_1, nextState, endGameFlag = gridGameOne(agent0Action, agent1Action, currentState)\n",
    "            agent_0.nashQLearning(gamma, agent0Action, reward_0, currentState, nextState, agent1Action, reward_1)\n",
    "            agent_1.nashQLearning(gamma, agent0Action, reward_0, currentState, nextState, agent1Action, reward_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ace_tools'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mitertools\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mace_tools\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtools\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Define constants for actions\u001b[39;00m\n\u001b[1;32m      8\u001b[0m ACTION_UP \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ace_tools'"
     ]
    }
   ],
   "source": [
    "# Re-import necessary libraries after execution state reset\n",
    "import random\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import ace_tools as tools\n",
    "\n",
    "# Define constants for actions\n",
    "ACTION_UP = 0\n",
    "ACTION_DOWN = 1\n",
    "ACTION_LEFT = 2\n",
    "ACTION_RIGHT = 3\n",
    "\n",
    "# Define world size\n",
    "WORLD_HEIGHT = 3\n",
    "WORLD_WIDTH = 3\n",
    "\n",
    "# Generate grid indices\n",
    "gridIndexList = [WORLD_WIDTH * i + j for i in range(WORLD_HEIGHT) for j in range(WORLD_WIDTH)]\n",
    "\n",
    "# Define valid actions for each position\n",
    "locationValidActions = {i: [] for i in gridIndexList}\n",
    "for i in range(WORLD_HEIGHT):\n",
    "    for j in range(WORLD_WIDTH):\n",
    "        gridIndexNumber = WORLD_WIDTH * i + j\n",
    "        if i != WORLD_HEIGHT - 1:\n",
    "            locationValidActions[gridIndexNumber].append(ACTION_UP)\n",
    "        if i != 0:\n",
    "            locationValidActions[gridIndexNumber].append(ACTION_DOWN)\n",
    "        if j != 0:\n",
    "            locationValidActions[gridIndexNumber].append(ACTION_LEFT)\n",
    "        if j != WORLD_WIDTH - 1:\n",
    "            locationValidActions[gridIndexNumber].append(ACTION_RIGHT)\n",
    "\n",
    "# Define all possible states as (agent1_position, agent2_position)\n",
    "statesAllOne = list(itertools.product(gridIndexList, repeat=2))\n",
    "\n",
    "# Agent class with learning capabilities\n",
    "class Agent:\n",
    "    def __init__(self, agentIndex=0, startLocationIndex=0):\n",
    "        self.qTable = {}  # Stores Q-values\n",
    "        self.alpha = {}  # Learning rates\n",
    "        self.timeNumber = {}  # Number of visits\n",
    "        self.strategy = {}  # Policy (not used in this experiment)\n",
    "        self.agentIndex = agentIndex  # Identifies agent 0 or 1\n",
    "        self.locationIndex = startLocationIndex  # Initial position\n",
    "\n",
    "    def initialSelfQTable(self):\n",
    "        \"\"\" Initialize Q-table with zero values for all state-action pairs \"\"\"\n",
    "        self.qTable[0] = {}\n",
    "        self.qTable[1] = {}\n",
    "        for state in statesAllOne:\n",
    "            self.qTable[0][state] = {}\n",
    "            self.qTable[1][state] = {}\n",
    "            for action1 in locationValidActions[state[0]]:\n",
    "                for action2 in locationValidActions[state[1]]:\n",
    "                    self.qTable[0][state][(action1, action2)] = 0\n",
    "                    self.qTable[1][state][(action1, action2)] = 0\n",
    "\n",
    "    def initialSelfAlpha(self):\n",
    "        \"\"\" Initialize learning rates \"\"\"\n",
    "        for state in statesAllOne:\n",
    "            self.alpha[state] = 1.0  # Set initial learning rate\n",
    "\n",
    "    def chooseActionRandomly(self, currentState):\n",
    "        \"\"\" Choose a random valid action \"\"\"\n",
    "        return random.choice(locationValidActions[currentState[self.agentIndex]])\n",
    "\n",
    "    def nashQLearning(self, gamma, action0, reward0, currentState, nextState, action1, reward1):\n",
    "        \"\"\" Update Q-table using Nash Q-learning (simplified update) \"\"\"\n",
    "        alpha = self.alpha[currentState]\n",
    "        self.qTable[0][currentState][(action0, action1)] += alpha * (\n",
    "            reward0 + gamma * max(self.qTable[0][nextState].values()) - self.qTable[0][currentState][(action0, action1)]\n",
    "        )\n",
    "        self.qTable[1][currentState][(action0, action1)] += alpha * (\n",
    "            reward1 + gamma * max(self.qTable[1][nextState].values()) - self.qTable[1][currentState][(action0, action1)]\n",
    "        )\n",
    "\n",
    "# Game environment\n",
    "def gridGameOne(action0, action1, currentState):\n",
    "    \"\"\" Simulates one step in the grid game \"\"\"\n",
    "    agent0_new = currentState[0]\n",
    "    agent1_new = currentState[1]\n",
    "\n",
    "    # Compute new positions based on actions\n",
    "    if action0 == ACTION_UP and agent0_new + WORLD_WIDTH in gridIndexList:\n",
    "        agent0_new += WORLD_WIDTH\n",
    "    elif action0 == ACTION_DOWN and agent0_new - WORLD_WIDTH in gridIndexList:\n",
    "        agent0_new -= WORLD_WIDTH\n",
    "    elif action0 == ACTION_LEFT and (agent0_new - 1) in gridIndexList and (agent0_new % WORLD_WIDTH != 0):\n",
    "        agent0_new -= 1\n",
    "    elif action0 == ACTION_RIGHT and (agent0_new + 1) in gridIndexList and ((agent0_new + 1) % WORLD_WIDTH != 0):\n",
    "        agent0_new += 1\n",
    "\n",
    "    if action1 == ACTION_UP and agent1_new + WORLD_WIDTH in gridIndexList:\n",
    "        agent1_new += WORLD_WIDTH\n",
    "    elif action1 == ACTION_DOWN and agent1_new - WORLD_WIDTH in gridIndexList:\n",
    "        agent1_new -= WORLD_WIDTH\n",
    "    elif action1 == ACTION_LEFT and (agent1_new - 1) in gridIndexList and (agent1_new % WORLD_WIDTH != 0):\n",
    "        agent1_new -= 1\n",
    "    elif action1 == ACTION_RIGHT and (agent1_new + 1) in gridIndexList and ((agent1_new + 1) % WORLD_WIDTH != 0):\n",
    "        agent1_new += 1\n",
    "\n",
    "    nextState = (agent0_new, agent1_new)\n",
    "\n",
    "    # Reward system (arbitrary for now)\n",
    "    reward_0 = 1 if agent0_new == 8 else 0  # Reward if agent 0 reaches bottom-right\n",
    "    reward_1 = 1 if agent1_new == 8 else 0  # Reward if agent 1 reaches bottom-right\n",
    "\n",
    "    endGameFlag = agent0_new == 8 and agent1_new == 8  # Game ends if both reach the goal\n",
    "\n",
    "    return reward_0, reward_1, nextState, endGameFlag\n",
    "\n",
    "# Main training loop\n",
    "def playGameOne():\n",
    "    \"\"\" Runs the game loop for Nash Q-learning \"\"\"\n",
    "    gamma = 0.9\n",
    "    agent_0 = Agent(0, random.choice(gridIndexList))\n",
    "    agent_1 = Agent(1, random.choice(gridIndexList))\n",
    "\n",
    "    agent_0.initialSelfQTable()\n",
    "    agent_1.initialSelfQTable()\n",
    "    agent_0.initialSelfAlpha()\n",
    "    agent_1.initialSelfAlpha()\n",
    "\n",
    "    episodes = 0\n",
    "    history = []  # Store history for visualization\n",
    "\n",
    "    while episodes < 100:\n",
    "        currentState = (agent_0.locationIndex, agent_1.locationIndex)\n",
    "\n",
    "        while True:\n",
    "            action0 = agent_0.chooseActionRandomly(currentState)\n",
    "            action1 = agent_1.chooseActionRandomly(currentState)\n",
    "            reward_0, reward_1, nextState, endGameFlag = gridGameOne(action0, action1, currentState)\n",
    "\n",
    "            agent_0.nashQLearning(gamma, action0, reward_0, currentState, nextState, action1, reward_1)\n",
    "            agent_1.nashQLearning(gamma, action0, reward_0, currentState, nextState, action1, reward_1)\n",
    "\n",
    "            history.append((episodes, currentState, action0, action1, reward_0, reward_1, nextState))\n",
    "\n",
    "            if endGameFlag:\n",
    "                break\n",
    "\n",
    "            currentState = nextState\n",
    "\n",
    "        episodes += 1\n",
    "\n",
    "    # Convert history to DataFrame for analysis\n",
    "    df_history = pd.DataFrame(history, columns=['Episode', 'State', 'Action0', 'Action1', 'Reward0', 'Reward1', 'NextState'])\n",
    "    tools.display_dataframe_to_user(name=\"Game History\", dataframe=df_history)\n",
    "\n",
    "# Run the simulation\n",
    "playGameOne()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
