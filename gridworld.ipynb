{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3x3 gridworld with two agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When multiple agents simultaneously interact, the environment is no longer stationary because each agent’s policy influences the environment dynamics. This setting is modeled as a general-sum stochastic game:\n",
    "\n",
    "\n",
    "$\\mathcal{G} = \\langle S, (A_1, A_2), (r_1, r_2), p, \\gamma \\rangle$\n",
    "\n",
    "where:\n",
    "\n",
    "Each agent i has its own action space $A_i$.\n",
    "\n",
    "Each agent receives a reward \n",
    "    $r_i$: $S \\times A_1 \\times A_2 \\to \\mathbb{R}$.\n",
    "\n",
    "The state transition depends on both agents’ actions.\n",
    "\n",
    "A major problem is how to define optimality when there are multiple agents.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Nash Q-learning, agents do not maximize their own Q-values independently, but instead assume that both agents will play according to Nash equilibria at each stage of the game.\n",
    "\n",
    "Instead of:\n",
    "\n",
    "\n",
    "$Q(s, a) \\leftarrow (1 - \\alpha) Q(s, a) + \\alpha \\Big( r + \\gamma \\max_{a{\\prime}} Q(s{\\prime}, a{\\prime}) \\Big)$\n",
    "\n",
    "\n",
    "it updates according to:\n",
    "\n",
    "\n",
    "$Q_i(s, a_1, a_2) \\leftarrow (1 - \\alpha) Q_i(s, a_1, a_2) + \\alpha \\Big( r_i + \\gamma \\cdot \\text{Nash}(Q(s{\\prime})) \\Big)$\n",
    "\n",
    "\n",
    "where Nash(Q(s’)) represents the expected Q-value under a Nash equilibrium strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each state  s , a stage game is formed from the current Q-values:\n",
    "\n",
    "A payoff matrix is constructed for each agent from $Q(s, a_1, a_2)$.\n",
    "\n",
    "The Lemke-Howson algorithm is used to compute a Nash equilibrium strategy.\n",
    "\n",
    "The Nash equilibrium expected values are used for Q-value updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Episode   State  Action0  Action1  Reward0  Reward1 NextState\n",
      "0           0  (4, 4)        3        0        0        0    (5, 7)\n",
      "1           0  (5, 7)        2        1        0        0    (4, 4)\n",
      "2           0  (4, 4)        1        3        0        0    (1, 5)\n",
      "3           0  (1, 5)        3        0        0        1    (2, 8)\n",
      "4           0  (2, 8)        0        1        0        0    (5, 5)\n",
      "...       ...     ...      ...      ...      ...      ...       ...\n",
      "7819       99  (5, 3)        1        0        0        0    (2, 6)\n",
      "7820       99  (2, 6)        0        3        0        0    (5, 7)\n",
      "7821       99  (5, 7)        1        3        0        1    (2, 8)\n",
      "7822       99  (2, 8)        0        1        0        0    (5, 5)\n",
      "7823       99  (5, 5)        0        0        1        1    (8, 8)\n",
      "\n",
      "[7824 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import itertools\n",
    "import pandas as pd\n",
    "\n",
    "# possible actions\n",
    "ACTION_UP = 0\n",
    "ACTION_DOWN = 1\n",
    "ACTION_LEFT = 2\n",
    "ACTION_RIGHT = 3\n",
    "\n",
    "# state representation: \n",
    "# the state space is represented by a 9-element grid indexed from 0 to 8\n",
    "# each state is a tuple: (agent1's position, agent2's position)\n",
    "WORLD_HEIGHT = 3\n",
    "WORLD_WIDTH = 3\n",
    "gridIndexList = [WORLD_WIDTH * i + j for i in range(WORLD_HEIGHT) for j in range(WORLD_WIDTH)]\n",
    "\n",
    "# valid moves are stored for each position to prevent illegal actions\n",
    "locationValidActions = {i: [] for i in gridIndexList}\n",
    "for i in range(WORLD_HEIGHT):\n",
    "    for j in range(WORLD_WIDTH):\n",
    "        gridIndexNumber = WORLD_WIDTH * i + j\n",
    "        if i != WORLD_HEIGHT - 1:\n",
    "            locationValidActions[gridIndexNumber].append(ACTION_UP)\n",
    "        if i != 0:\n",
    "            locationValidActions[gridIndexNumber].append(ACTION_DOWN)\n",
    "        if j != 0:\n",
    "            locationValidActions[gridIndexNumber].append(ACTION_LEFT)\n",
    "        if j != WORLD_WIDTH - 1:\n",
    "            locationValidActions[gridIndexNumber].append(ACTION_RIGHT)\n",
    "\n",
    "# Define all possible states as (agent1_position, agent2_position)\n",
    "statesAllOne = list(itertools.product(gridIndexList, repeat=2))\n",
    "\n",
    "# each agent maintains:\n",
    "# a Q-table storing joint action values\n",
    "# a learning rate for updating values\n",
    "# a policy strategy (not used now)\n",
    "class Agent:\n",
    "    def __init__(self, agentIndex=0, startLocationIndex=0):\n",
    "        self.qTable = {}  # Stores Q-values\n",
    "        self.alpha = {}  # Learning rates\n",
    "        self.timeNumber = {}  # Number of visits\n",
    "        self.strategy = {}  # Policy (not used in this experiment)\n",
    "        self.agentIndex = agentIndex  # Identifies agent 0 or 1\n",
    "        self.locationIndex = startLocationIndex  # Initial position\n",
    "\n",
    "    def initialSelfQTable(self):\n",
    "        \"\"\" Initialize Q-table with zero values for all state-action pairs \"\"\"\n",
    "        self.qTable[0] = {}\n",
    "        self.qTable[1] = {}\n",
    "        for state in statesAllOne:\n",
    "            self.qTable[0][state] = {}\n",
    "            self.qTable[1][state] = {}\n",
    "            for action1 in locationValidActions[state[0]]:\n",
    "                for action2 in locationValidActions[state[1]]:\n",
    "                    self.qTable[0][state][(action1, action2)] = 0\n",
    "                    self.qTable[1][state][(action1, action2)] = 0\n",
    "\n",
    "    def initialSelfAlpha(self):\n",
    "        \"\"\" Initialize learning rates \"\"\"\n",
    "        for state in statesAllOne:\n",
    "            self.alpha[state] = 1.0  # initial learning rate\n",
    "\n",
    "    def chooseActionRandomly(self, currentState):\n",
    "        \"\"\" Choose a random valid action \"\"\"\n",
    "        return random.choice(locationValidActions[currentState[self.agentIndex]])\n",
    "\n",
    "    def nashQLearning(self, gamma, action0, reward0, currentState, nextState, action1, reward1):\n",
    "        \"\"\" Update Q-table using Nash Q-learning (simplified update) \"\"\"\n",
    "        alpha = self.alpha[currentState]\n",
    "        self.qTable[0][currentState][(action0, action1)] += alpha * (\n",
    "            reward0 + gamma * max(self.qTable[0][nextState].values()) - self.qTable[0][currentState][(action0, action1)]\n",
    "        )\n",
    "        self.qTable[1][currentState][(action0, action1)] += alpha * (\n",
    "            reward1 + gamma * max(self.qTable[1][nextState].values()) - self.qTable[1][currentState][(action0, action1)]\n",
    "        )\n",
    "\n",
    "# Game environment\n",
    "def gridGameOne(action0, action1, currentState):\n",
    "    \"\"\" Simulates one step in the grid game \"\"\"\n",
    "    agent0_new = currentState[0]\n",
    "    agent1_new = currentState[1]\n",
    "\n",
    "    # Compute new positions based on actions\n",
    "    if action0 == ACTION_UP and agent0_new + WORLD_WIDTH in gridIndexList:\n",
    "        agent0_new += WORLD_WIDTH\n",
    "    elif action0 == ACTION_DOWN and agent0_new - WORLD_WIDTH in gridIndexList:\n",
    "        agent0_new -= WORLD_WIDTH\n",
    "    elif action0 == ACTION_LEFT and (agent0_new - 1) in gridIndexList and (agent0_new % WORLD_WIDTH != 0):\n",
    "        agent0_new -= 1\n",
    "    elif action0 == ACTION_RIGHT and (agent0_new + 1) in gridIndexList and ((agent0_new + 1) % WORLD_WIDTH != 0):\n",
    "        agent0_new += 1\n",
    "\n",
    "    if action1 == ACTION_UP and agent1_new + WORLD_WIDTH in gridIndexList:\n",
    "        agent1_new += WORLD_WIDTH\n",
    "    elif action1 == ACTION_DOWN and agent1_new - WORLD_WIDTH in gridIndexList:\n",
    "        agent1_new -= WORLD_WIDTH\n",
    "    elif action1 == ACTION_LEFT and (agent1_new - 1) in gridIndexList and (agent1_new % WORLD_WIDTH != 0):\n",
    "        agent1_new -= 1\n",
    "    elif action1 == ACTION_RIGHT and (agent1_new + 1) in gridIndexList and ((agent1_new + 1) % WORLD_WIDTH != 0):\n",
    "        agent1_new += 1\n",
    "\n",
    "    nextState = (agent0_new, agent1_new)\n",
    "\n",
    "    # Reward system (arbitrary for now)\n",
    "    reward_0 = 1 if agent0_new == 8 else 0  # Reward if agent 0 reaches bottom-right\n",
    "    reward_1 = 1 if agent1_new == 8 else 0  # Reward if agent 1 reaches bottom-right\n",
    "\n",
    "    endGameFlag = agent0_new == 8 and agent1_new == 8  # game ends if both reach the goal\n",
    "\n",
    "    return reward_0, reward_1, nextState, endGameFlag\n",
    "\n",
    "# agents randomly explore actions \n",
    "# nash Q-learning updates are applied\n",
    "def playGameOne():\n",
    "    \"\"\" Runs the game loop for Nash Q-learning \"\"\"\n",
    "    gamma = 0.9\n",
    "    agent_0 = Agent(0, random.choice(gridIndexList))\n",
    "    agent_1 = Agent(1, random.choice(gridIndexList))\n",
    "\n",
    "    agent_0.initialSelfQTable()\n",
    "    agent_1.initialSelfQTable()\n",
    "    agent_0.initialSelfAlpha()\n",
    "    agent_1.initialSelfAlpha()\n",
    "\n",
    "    episodes = 0\n",
    "    history = []  # store history for visualization\n",
    "\n",
    "    while episodes < 100:\n",
    "        currentState = (agent_0.locationIndex, agent_1.locationIndex)\n",
    "\n",
    "        while True:\n",
    "            action0 = agent_0.chooseActionRandomly(currentState)\n",
    "            action1 = agent_1.chooseActionRandomly(currentState)\n",
    "            reward_0, reward_1, nextState, endGameFlag = gridGameOne(action0, action1, currentState)\n",
    "\n",
    "            agent_0.nashQLearning(gamma, action0, reward_0, currentState, nextState, action1, reward_1)\n",
    "            agent_1.nashQLearning(gamma, action0, reward_0, currentState, nextState, action1, reward_1)\n",
    "\n",
    "            history.append((episodes, currentState, action0, action1, reward_0, reward_1, nextState))\n",
    "\n",
    "            if endGameFlag:\n",
    "                break\n",
    "\n",
    "            currentState = nextState\n",
    "\n",
    "        episodes += 1\n",
    "\n",
    "    df_history = pd.DataFrame(history, columns=['Episode', 'State', 'Action0', 'Action1', 'Reward0', 'Reward1', 'NextState'])\n",
    "    print(df_history) \n",
    "    #df_history.to_csv(\"game_history.csv\") \n",
    "\n",
    "playGameOne()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do:\n",
    "\n",
    "I have applied standard Q-Learning instead of Nash Q-learning --> Study the lemke-Howson algorithm and fix code (Instead of max(Q), use a Nash solver to find the equilibrium Q-values at nextState)\n",
    "\n",
    "I keep alpha constant. This can prevent convergence. In Nash Q-learning, the learning rate decreases over time ($\n",
    "\\alpha(s, a_0, a_1) = \\frac{1}{N(s, a_0, a_1)}\n",
    "$) \n",
    "\n",
    "My agent policies are purely random (should I use an epsilon greedy policy?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
