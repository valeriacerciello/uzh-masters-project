{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating expert trajectories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom 2-agent grid world using Gymnasium.\n",
    "\n",
    "*Features*:\n",
    "\n",
    "Grid size: 3x3\n",
    "\n",
    "A1 starts at (0,0), A2 starts at (0,2).\n",
    "\n",
    "Goals: A1 reach row 2, A2 reach column 0\n",
    "\n",
    "Actions: [UP, DOWN, LEFT, RIGHT] (4 actions per agent).\n",
    "\n",
    "Observations: Positions of both agents.\n",
    "\n",
    "Rewards:\n",
    "\n",
    "A1:   \n",
    "|   |   |   |       \n",
    "| - | - | - |       \n",
    "| 1 | 4 | 7 |       \n",
    "| 2 | 5 | 8 |      \n",
    "| 3 | 6 | 9 |\n",
    "\n",
    "A2:   \n",
    "|    |    |    |       \n",
    "| -- | -- | -- |       \n",
    "| -1 | -4 | -7 |       \n",
    "| -2 | -5 | -8 |      \n",
    "| -3 | -6 | -9 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "\n",
    "grid_size = 3\n",
    "max_steps = 100000\n",
    "\n",
    "class GridGame1(gym.Env):\n",
    "    metadata = {'render_modes': ['human'], 'render_fps': 4}\n",
    "\n",
    "    def __init__(self):\n",
    "        self.grid_size = grid_size\n",
    "        self.n_agents = 2\n",
    "        self.max_steps = max_steps\n",
    "\n",
    "        # Updated start positions\n",
    "        self.init_positions = [(0, 0), (0, 2)]  # A1 at (0,0), A2 at (0,2)\n",
    "\n",
    "        # Goals\n",
    "        self.goal_row_a1 = 2  # A1 needs to reach row 2\n",
    "        self.goal_col_a2 = 0  # A2 needs to reach column 0\n",
    "\n",
    "        # Action space: 4 actions per agent\n",
    "        self.action_space = spaces.MultiDiscrete([4, 4])\n",
    "\n",
    "        # Observation space: positions of both agents\n",
    "        self.observation_space = spaces.Dict({\n",
    "            \"agent1\": spaces.Box(0, self.grid_size - 1, shape=(2,), dtype=int),\n",
    "            \"agent2\": spaces.Box(0, self.grid_size - 1, shape=(2,), dtype=int)\n",
    "        })\n",
    "\n",
    "        # Reset environment\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.agent1_pos, self.agent2_pos = self.init_positions\n",
    "        self.steps = 0\n",
    "        return self._get_obs(), {}\n",
    "\n",
    "    def _get_obs(self):\n",
    "        return {\n",
    "            \"agent1\": np.array(self.agent1_pos),\n",
    "            \"agent2\": np.array(self.agent2_pos)\n",
    "        }\n",
    "\n",
    "    def step(self, actions):\n",
    "        self.steps += 1\n",
    "\n",
    "        # Move agents\n",
    "        new_pos1 = self._move(self.agent1_pos, actions[0])\n",
    "        new_pos2 = self._move(self.agent2_pos, actions[1])\n",
    "\n",
    "        # Check for collision\n",
    "        if new_pos1 == new_pos2:\n",
    "            new_pos1 = self.agent1_pos\n",
    "            new_pos2 = self.agent2_pos\n",
    "            reward1 = reward2 = -1  # penalty for collision\n",
    "            done = False\n",
    "        else:\n",
    "            reward1 = self._compute_reward_agent1(new_pos1)\n",
    "            reward2 = self._compute_reward_agent2(new_pos2)\n",
    "\n",
    "            self.agent1_pos = new_pos1\n",
    "            self.agent2_pos = new_pos2\n",
    "\n",
    "            done_a1 = self.agent1_pos[0] == self.goal_row_a1\n",
    "            done_a2 = self.agent2_pos[1] == self.goal_col_a2\n",
    "            done = done_a1 or done_a2\n",
    "\n",
    "        # Force episode end after max_steps\n",
    "        done = done or self.steps >= self.max_steps\n",
    "\n",
    "        return self._get_obs(), (reward1, reward2), done, {}, {}\n",
    "\n",
    "    def _compute_reward_agent1(self, pos):\n",
    "        x, y = pos\n",
    "        # A1 reward matrix values (flipped row-wise)\n",
    "        reward_map = [\n",
    "            [1, 4, 7],\n",
    "            [2, 5, 8],\n",
    "            [3, 6, 9]\n",
    "        ]\n",
    "        return reward_map[x][y]\n",
    "\n",
    "    def _compute_reward_agent2(self, pos):\n",
    "        x, y = pos\n",
    "        # A2 reward matrix values (flipped and negative)\n",
    "        reward_map = [\n",
    "            [-1, -4, -7],\n",
    "            [-2, -5, -8],\n",
    "            [-3, -6, -9]\n",
    "        ]\n",
    "        return reward_map[x][y]\n",
    "\n",
    "    def _move(self, pos, action):\n",
    "        x, y = pos\n",
    "        if action == 0:  # UP\n",
    "            x = max(0, x - 1)\n",
    "        elif action == 1:  # DOWN\n",
    "            x = min(self.grid_size - 1, x + 1)\n",
    "        elif action == 2:  # LEFT\n",
    "            y = max(0, y - 1)\n",
    "        elif action == 3:  # RIGHT\n",
    "            y = min(self.grid_size - 1, y + 1)\n",
    "        return (x, y)\n",
    "\n",
    "    def render(self):\n",
    "        grid = np.full((3, 3), '路', dtype='<U3')\n",
    "\n",
    "        # Mark goals\n",
    "        for col in range(self.grid_size):\n",
    "            grid[self.goal_row_a1, col] = 'G1'\n",
    "        for row in range(self.grid_size):\n",
    "            if grid[row, self.goal_col_a2] == 'G1':\n",
    "                grid[row, self.goal_col_a2] = 'G1/2'\n",
    "            else:\n",
    "                grid[row, self.goal_col_a2] = 'G2'\n",
    "\n",
    "        # Mark agents\n",
    "        ax1, ay1 = self.agent1_pos\n",
    "        ax2, ay2 = self.agent2_pos\n",
    "        if (ax1, ay1) == (ax2, ay2):\n",
    "            grid[ax1, ay1] = 'A*'\n",
    "        else:\n",
    "            grid[ax1, ay1] = 'A1'\n",
    "            grid[ax2, ay2] = 'A2'\n",
    "\n",
    "        print(\"Current State:\")\n",
    "        for row in grid:\n",
    "            print(\" \".join(f\"{cell:^5}\" for cell in row))\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current State:\n",
      " A1     路    A2  \n",
      " G2     路     路  \n",
      " G1/   G1    G1  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = GridGame1()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "def plot_env_state(env):\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "    # 1) Draw grid lines\n",
    "    for x in range(env.grid_size + 1):\n",
    "        ax.axvline(x, color='gray', linestyle='-', linewidth=1)\n",
    "    for y in range(env.grid_size + 1):\n",
    "        ax.axhline(y, color='gray', linestyle='-', linewidth=1)\n",
    "\n",
    "    # 2) Set axis limits and ticks\n",
    "    ax.set_xlim(0, env.grid_size)\n",
    "    ax.set_ylim(0, env.grid_size)\n",
    "    ax.set_xticks([i + 0.5 for i in range(env.grid_size)])\n",
    "    ax.set_yticks([i + 0.5 for i in range(env.grid_size)])\n",
    "    ax.set_xticklabels([str(i) for i in range(env.grid_size)])\n",
    "    ax.set_yticklabels([str(i) for i in range(env.grid_size)])\n",
    "\n",
    "    ax.xaxis.set_ticks_position('top')\n",
    "    ax.xaxis.set_label_position('top')\n",
    "    ax.invert_yaxis()\n",
    "    ax.set_title(\"Grid Zero-Sum Game\")\n",
    "\n",
    "    # Helper for plotting cells\n",
    "    def cell_coords(row, col):\n",
    "        return (col, row)\n",
    "\n",
    "    def center_coords(row, col):\n",
    "        return (col + 0.5, row + 0.5)\n",
    "\n",
    "    # 3) Highlight A1's goal row (row 2)\n",
    "    for col in range(env.grid_size):\n",
    "        current_label = \"G1\"\n",
    "        if col == env.goal_col_a2 and 0 < env.grid_size:\n",
    "            current_label = \"G1 & G2\"\n",
    "        ax.add_patch(patches.Rectangle(cell_coords(env.goal_row_a1, col), 1, 1, facecolor='red', alpha=0.5))\n",
    "        ax.text(*center_coords(env.goal_row_a1, col), current_label, ha='center', va='center', fontsize=10, color='red')\n",
    "\n",
    "    # 4) Highlight A2's goal column (col 0)\n",
    "    for row in range(env.grid_size):\n",
    "        current_label = \"G2\"\n",
    "        # If G1 already in the cell, merge label\n",
    "        if row == env.goal_row_a1 and 0 < env.grid_size:\n",
    "            current_label = \"G1 & G2\"\n",
    "        ax.add_patch(patches.Rectangle(cell_coords(row, env.goal_col_a2), 1, 1, facecolor='lightblue', alpha=0.5))\n",
    "        ax.text(*center_coords(row, env.goal_col_a2), current_label, ha='center', va='center', fontsize=10, color='blue')\n",
    "\n",
    "    # 5) Plot agents\n",
    "    a1_row, a1_col = env.agent1_pos\n",
    "    a2_row, a2_col = env.agent2_pos\n",
    "\n",
    "    if (a1_row, a1_col) == (a2_row, a2_col):\n",
    "        ax.plot(*center_coords(a1_row, a1_col), 'purple', marker='o', markersize=20)\n",
    "        ax.text(*center_coords(a1_row, a1_col), \"A1&A2\", fontsize=10, ha='center', va='center', color='white', fontweight='bold')\n",
    "    else:\n",
    "        ax.plot(*center_coords(a1_row, a1_col), 'ro', markersize=20)\n",
    "        ax.text(*center_coords(a1_row, a1_col), \"A1\", fontsize=12, ha='center', va='center', color='white', fontweight='bold')\n",
    "\n",
    "        ax.plot(*center_coords(a2_row, a2_col), 'bo', markersize=20)\n",
    "        ax.text(*center_coords(a2_row, a2_col), \"A2\", fontsize=12, ha='center', va='center', color='white', fontweight='bold')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Test with updated env\n",
    "env = GridGame1()\n",
    "env.reset()\n",
    "plot_env_state(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nash Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nashpy import Game\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class NashQLearner:\n",
    "    def __init__(self, env, alpha=0.1, gamma=0.99, epsilon=0.5, min_epsilon=0.01, decay_rate=0.0005):\n",
    "        self.env = env\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.min_epsilon = min_epsilon\n",
    "        self.decay_rate = decay_rate\n",
    "        self.epsilon_history = []\n",
    "\n",
    "        self.grid_size = env.grid_size\n",
    "        self.Q1 = np.zeros((self.grid_size, self.grid_size, self.grid_size, self.grid_size, 4, 4))\n",
    "        self.Q2 = np.zeros((self.grid_size, self.grid_size, self.grid_size, self.grid_size, 4, 4))\n",
    "\n",
    "    def decay_epsilon(self, episode):\n",
    "        self.epsilon = max(self.min_epsilon, self.epsilon * np.exp(-self.decay_rate * episode))\n",
    "        self.epsilon_history.append(self.epsilon)\n",
    "\n",
    "    def get_action(self, state, explore=True):\n",
    "        s = self._state_to_index(state)\n",
    "    \n",
    "        game = Game(self.Q1[s], self.Q2[s])\n",
    "        equilibria = list(game.support_enumeration())\n",
    "    \n",
    "        if len(equilibria) > 1:\n",
    "            print(f\"\\n Multiple equilibria found at state {s}:\")\n",
    "            print_equilibria(self.Q1[s], self.Q2[s])\n",
    "    \n",
    "        if not equilibria or (explore and np.random.rand() < self.epsilon):\n",
    "            return np.random.randint(0, 4), np.random.randint(0, 4)\n",
    "    \n",
    "        pi1, pi2 = equilibria[0]\n",
    "        a1 = np.random.choice(4, p=pi1)\n",
    "        a2 = np.random.choice(4, p=pi2)\n",
    "        return a1, a2\n",
    "\n",
    "    def update(self, state, actions, rewards, next_state):\n",
    "        s = self._state_to_index(state)\n",
    "        s_next = self._state_to_index(next_state)\n",
    "        a1, a2 = actions\n",
    "        r1, r2 = rewards\n",
    "\n",
    "        game_next = Game(self.Q1[s_next], self.Q2[s_next])\n",
    "        equilibria_next = list(game_next.support_enumeration())\n",
    "\n",
    "        if not equilibria_next:\n",
    "            nash_q1 = 0\n",
    "            nash_q2 = 0\n",
    "        else:\n",
    "            pi1_next, pi2_next = equilibria_next[0]  # Again, just take the first\n",
    "            nash_q1 = np.sum(np.outer(pi1_next, pi2_next) * self.Q1[s_next])\n",
    "            nash_q2 = np.sum(np.outer(pi1_next, pi2_next) * self.Q2[s_next])\n",
    "\n",
    "        self.Q1[s][a1][a2] += self.alpha * (r1 + self.gamma * nash_q1 - self.Q1[s][a1][a2])\n",
    "        self.Q2[s][a1][a2] += self.alpha * (r2 + self.gamma * nash_q2 - self.Q2[s][a1][a2])\n",
    "\n",
    "    def _state_to_index(self, state):\n",
    "        return (\n",
    "            state[\"agent1\"][0], state[\"agent1\"][1],\n",
    "            state[\"agent2\"][0], state[\"agent2\"][1]\n",
    "        )\n",
    "\n",
    "    def plot_learning_curve(self, episode_rewards):\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(np.arange(len(episode_rewards)), [r[0] for r in episode_rewards], label=\"Agent 1\")\n",
    "        plt.plot(np.arange(len(episode_rewards)), [r[1] for r in episode_rewards], label=\"Agent 2\")\n",
    "        plt.xlabel(\"Episode\")\n",
    "        plt.ylabel(\"Total Reward\")\n",
    "        plt.title(\"Learning Curve\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lents\\anaconda3\\lib\\site-packages\\nashpy\\algorithms\\support_enumeration.py:260: RuntimeWarning: \n",
      "An even number of (16) equilibria was returned. This\n",
      "indicates that the game is degenerate. Consider using another algorithm\n",
      "to investigate.\n",
      "                  \n",
      "  warnings.warn(warning, RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Multiple equilibria found at state (0, 0, 0, 2):\n",
      "16 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 12:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 13:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 14:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 15:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 16:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 1, 0, 2):\n",
      "16 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 12:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 13:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 14:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 15:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 16:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 1, 1, 2):\n",
      "16 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 12:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 13:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 14:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 15:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 16:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 1, 0, 2):\n",
      "12 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 12:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lents\\anaconda3\\lib\\site-packages\\nashpy\\algorithms\\support_enumeration.py:260: RuntimeWarning: \n",
      "An even number of (12) equilibria was returned. This\n",
      "indicates that the game is degenerate. Consider using another algorithm\n",
      "to investigate.\n",
      "                  \n",
      "  warnings.warn(warning, RuntimeWarning)\n",
      "c:\\Users\\lents\\anaconda3\\lib\\site-packages\\nashpy\\algorithms\\support_enumeration.py:260: RuntimeWarning: \n",
      "An even number of (8) equilibria was returned. This\n",
      "indicates that the game is degenerate. Consider using another algorithm\n",
      "to investigate.\n",
      "                  \n",
      "  warnings.warn(warning, RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Multiple equilibria found at state (0, 1, 0, 2):\n",
      "8 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 0, 0, 2):\n",
      "12 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 12:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 0, 0, 2):\n",
      "11 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 0, 0, 2):\n",
      "7 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lents\\anaconda3\\lib\\site-packages\\nashpy\\algorithms\\support_enumeration.py:260: RuntimeWarning: \n",
      "An even number of (4) equilibria was returned. This\n",
      "indicates that the game is degenerate. Consider using another algorithm\n",
      "to investigate.\n",
      "                  \n",
      "  warnings.warn(warning, RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Multiple equilibria found at state (0, 1, 0, 2):\n",
      "4 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 1, 0, 2):\n",
      "3 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (1, 1, 0, 1):\n",
      "16 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 12:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 13:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 14:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 15:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 16:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (1, 2, 0, 1):\n",
      "16 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 12:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 13:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 14:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 15:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 16:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 2, 0, 1):\n",
      "16 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 12:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 13:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 14:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 15:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 16:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Episode 000 | Avg Reward: A1: 47.00, A2: -65.00\n",
      "\n",
      " Multiple equilibria found at state (0, 0, 0, 2):\n",
      "7 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 0, 0, 2):\n",
      "7 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (1, 0, 1, 2):\n",
      "16 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 12:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 13:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 14:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 15:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 16:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (1, 1, 0, 2):\n",
      "16 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 12:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 13:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 14:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 15:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 16:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 0, 0, 2):\n",
      "3 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 2, 0, 1):\n",
      "12 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 12:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (1, 2, 0, 1):\n",
      "12 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 12:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 2, 1, 1):\n",
      "16 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 12:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 13:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 14:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 15:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 16:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 2, 0, 1):\n",
      "8 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 2, 1, 1):\n",
      "12 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 12:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 2, 2, 1):\n",
      "16 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 12:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 13:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 14:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 15:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 16:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 2, 1, 1):\n",
      "8 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 1, 1, 2):\n",
      "12 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 12:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 1, 2, 2):\n",
      "16 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 12:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 13:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 14:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 15:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 16:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 0, 2, 2):\n",
      "16 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 12:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 13:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 14:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 15:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 16:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (1, 0, 1, 2):\n",
      "12 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 12:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 0, 1, 1):\n",
      "16 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 12:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 13:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 14:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 15:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 16:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 0, 0, 1):\n",
      "16 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 12:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 13:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 14:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 15:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 16:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 0, 0, 1):\n",
      "12 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 12:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 0, 0, 2):\n",
      "3 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 0, 0, 1):\n",
      "8 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (1, 0, 0, 1):\n",
      "16 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 12:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 13:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 14:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 15:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 16:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 0, 0, 1):\n",
      "8 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 0, 1, 1):\n",
      "12 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 12:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 0, 2, 1):\n",
      "16 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 12:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 13:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 14:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 15:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 16:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (1, 0, 2, 1):\n",
      "16 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 12:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 13:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 14:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 15:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 16:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 0, 1, 1):\n",
      "8 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 1, 2, 1):\n",
      "16 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 12:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 13:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 14:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 15:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 16:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 1, 2, 2):\n",
      "12 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 12:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 1, 1, 2):\n",
      "8 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 1, 1, 1):\n",
      "16 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 12:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 13:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 14:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 15:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 16:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 2, 1, 2):\n",
      "16 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 12:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 13:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 14:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 15:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 16:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (1, 2, 1, 1):\n",
      "16 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 12:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 13:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 14:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 15:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 16:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (1, 2, 2, 1):\n",
      "16 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 12:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 13:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 14:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 15:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 16:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (1, 0, 0, 2):\n",
      "16 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 12:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 13:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 14:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 15:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 16:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 0, 0, 1):\n",
      "4 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lents\\anaconda3\\lib\\site-packages\\nashpy\\algorithms\\support_enumeration.py:260: RuntimeWarning: \n",
      "An even number of (0) equilibria was returned. This\n",
      "indicates that the game is degenerate. Consider using another algorithm\n",
      "to investigate.\n",
      "                  \n",
      "  warnings.warn(warning, RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Multiple equilibria found at state (0, 0, 1, 2):\n",
      "16 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 12:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 13:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 14:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 15:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 16:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (1, 0, 0, 1):\n",
      "12 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 12:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 0, 1, 1):\n",
      "8 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 0, 1, 1):\n",
      "4 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 0, 1, 2):\n",
      "12 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 12:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 0, 2, 2):\n",
      "12 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 12:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 0, 2, 1):\n",
      "12 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 12:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (1, 0, 0, 1):\n",
      "8 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (1, 0, 0, 1):\n",
      "7 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (1, 0, 1, 2):\n",
      "8 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (1, 1, 2, 2):\n",
      "16 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 12:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 13:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 14:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 15:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 16:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (1, 0, 2, 1):\n",
      "12 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 12:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 0, 2, 1):\n",
      "8 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 0, 1, 2):\n",
      "8 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (1, 0, 1, 2):\n",
      "4 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (1, 1, 0, 2):\n",
      "12 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 12:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 1, 1, 2):\n",
      "4 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 1, 1, 1):\n",
      "12 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 12:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 1, 1, 1):\n",
      "11 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 1, 2, 1):\n",
      "12 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 12:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (1, 0, 0, 2):\n",
      "12 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 12:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (1, 0, 0, 2):\n",
      "8 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 0, 1, 2):\n",
      "8 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 1, 2, 1):\n",
      "8 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (1, 1, 2, 1):\n",
      "16 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 12:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 13:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 14:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 15:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 16:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 1, 1, 1):\n",
      "7 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (1, 0, 0, 1):\n",
      "3 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 0, 1, 2):\n",
      "4 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (1, 0, 2, 2):\n",
      "16 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 12:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 13:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 14:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 15:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 16:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (1, 0, 1, 2):\n",
      "4 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (1, 1, 1, 2):\n",
      "16 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 12:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 13:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 14:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 15:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 16:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (1, 1, 0, 2):\n",
      "8 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (1, 1, 0, 2):\n",
      "7 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 1, 0, 2):\n",
      "3 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [0.44203136 0.55796864 0.         0.        ]\n",
      "  Agent 2 strategy: [0.         0.         0.38107382 0.61892618]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [0.40732615 0.         0.         0.59267385]\n",
      "  Agent 2 strategy: [0.         0.68417003 0.31582997 0.        ]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0.40732615 0.4674196  0.         0.12525425]\n",
      "  Agent 2 strategy: [0.         0.10661155 0.3800474  0.51334105]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 1, 0, 2):\n",
      "3 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [0.44203136 0.55796864 0.         0.        ]\n",
      "  Agent 2 strategy: [0.         0.         0.38107382 0.61892618]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [0.40732615 0.         0.         0.59267385]\n",
      "  Agent 2 strategy: [0.         0.68417003 0.31582997 0.        ]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0.40732615 0.4674196  0.         0.12525425]\n",
      "  Agent 2 strategy: [0.         0.10661155 0.3800474  0.51334105]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 2, 0, 1):\n",
      "4 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 2, 0, 1):\n",
      "4 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0.8 0.2 0.  0. ]\n",
      "  Agent 2 strategy: [0.46666667 0.         0.53333333 0.        ]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (1, 0, 0, 2):\n",
      "4 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (1, 0, 0, 2):\n",
      "4 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (1, 0, 1, 1):\n",
      "16 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 12:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 13:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 14:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 15:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 16:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Episode 010 | Avg Reward: A1: 28.80, A2: -62.00\n",
      "\n",
      " Multiple equilibria found at state (1, 0, 1, 1):\n",
      "12 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 12:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 0, 2, 1):\n",
      "4 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 1, 2, 2):\n",
      "8 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (1, 1, 2, 2):\n",
      "12 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 12:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (1, 2, 2, 2):\n",
      "16 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 12:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 13:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 14:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 15:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 16:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (1, 2, 2, 2):\n",
      "12 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 12:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 2, 1, 2):\n",
      "12 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 12:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 2, 1, 2):\n",
      "8 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 2, 1, 2):\n",
      "7 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 1, 1, 1):\n",
      "3 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 0, 2, 1):\n",
      "4 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 0, 2, 2):\n",
      "8 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 0, 2, 2):\n",
      "4 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Episode 020 | Avg Reward: A1: 11.70, A2: -27.50\n",
      "\n",
      " Multiple equilibria found at state (1, 1, 0, 1):\n",
      "12 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 12:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 1, 1, 1):\n",
      "3 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (1, 1, 0, 1):\n",
      "8 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (1, 1, 0, 2):\n",
      "7 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 2, 0, 1):\n",
      "4 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0.8 0.2 0.  0. ]\n",
      "  Agent 2 strategy: [0.46666667 0.         0.53333333 0.        ]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (1, 2, 0, 2):\n",
      "16 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 12:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 13:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 14:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 15:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 16:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (1, 2, 0, 2):\n",
      "15 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 12:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 13:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 14:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 15:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 2, 1, 2):\n",
      "7 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 2, 2, 2):\n",
      "16 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 12:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 13:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 14:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 15:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 16:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 2, 1, 2):\n",
      "3 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (1, 2, 2, 2):\n",
      "8 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 2, 2, 1):\n",
      "12 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 12:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 2, 1, 1):\n",
      "4 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 2, 1, 2):\n",
      "3 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (1, 2, 0, 2):\n",
      "11 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (1, 2, 0, 1):\n",
      "8 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (1, 0, 1, 1):\n",
      "8 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Episode 030 | Avg Reward: A1: 19.00, A2: -23.80\n",
      "\n",
      " Multiple equilibria found at state (0, 0, 1, 2):\n",
      "4 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 0, 1, 2):\n",
      "4 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (1, 2, 0, 1):\n",
      "4 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (1, 1, 0, 1):\n",
      "4 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (1, 1, 1, 2):\n",
      "12 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 12:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 1, 2, 2):\n",
      "4 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 1, 2, 1):\n",
      "4 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 2, 2, 1):\n",
      "12 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 12:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (1, 2, 1, 1):\n",
      "12 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 12:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Episode 040 | Avg Reward: A1: 14.70, A2: -23.90\n",
      "\n",
      " Multiple equilibria found at state (0, 0, 2, 2):\n",
      "4 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 0, 2, 2):\n",
      "2 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [0.38313782 0.43103005 0.18583214 0.        ]\n",
      "  Agent 2 strategy: [0.33333333 0.03242871 0.63423796 0.        ]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [0.38313782 0.43103005 0.18583214 0.        ]\n",
      "  Agent 2 strategy: [0.33333333 0.         0.63423796 0.03242871]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lents\\anaconda3\\lib\\site-packages\\nashpy\\algorithms\\support_enumeration.py:260: RuntimeWarning: \n",
      "An even number of (2) equilibria was returned. This\n",
      "indicates that the game is degenerate. Consider using another algorithm\n",
      "to investigate.\n",
      "                  \n",
      "  warnings.warn(warning, RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Multiple equilibria found at state (0, 0, 2, 2):\n",
      "2 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [0.42024614 0.47277691 0.10697695 0.        ]\n",
      "  Agent 2 strategy: [0.33333333 0.33247686 0.33418981 0.        ]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [0.42024614 0.47277691 0.10697695 0.        ]\n",
      "  Agent 2 strategy: [0.33333333 0.         0.33418981 0.33247686]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (1, 0, 2, 2):\n",
      "12 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 12:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (1, 0, 2, 1):\n",
      "8 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (1, 1, 2, 1):\n",
      "12 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 12:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Episode 050 | Avg Reward: A1: 9.20, A2: -20.80\n",
      "\n",
      " Multiple equilibria found at state (1, 1, 0, 2):\n",
      "3 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Episode 060 | Avg Reward: A1: 6.70, A2: -12.80\n",
      "Episode 070 | Avg Reward: A1: 10.30, A2: -12.10\n",
      "Episode 080 | Avg Reward: A1: 7.00, A2: -9.00\n",
      "Episode 090 | Avg Reward: A1: 5.60, A2: -7.50\n",
      "Episode 100 | Avg Reward: A1: 7.00, A2: -7.20\n",
      "\n",
      " Multiple equilibria found at state (1, 1, 1, 2):\n",
      "8 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Episode 110 | Avg Reward: A1: 6.30, A2: -6.70\n",
      "\n",
      " Multiple equilibria found at state (0, 2, 1, 1):\n",
      "4 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Episode 120 | Avg Reward: A1: 9.10, A2: -11.70\n",
      "Episode 130 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 140 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 150 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "\n",
      " Multiple equilibria found at state (1, 0, 1, 1):\n",
      "4 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Episode 160 | Avg Reward: A1: 5.40, A2: -6.90\n",
      "Episode 170 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 180 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 190 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 200 | Avg Reward: A1: 5.20, A2: -5.40\n",
      "Episode 210 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 220 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 230 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 240 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 250 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 260 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 270 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 280 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 290 | Avg Reward: A1: 5.30, A2: -6.10\n",
      "Episode 300 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 310 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 320 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 330 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 340 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 350 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 360 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 370 | Avg Reward: A1: 5.10, A2: -5.70\n",
      "Episode 380 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 390 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 400 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 410 | Avg Reward: A1: 5.10, A2: -5.70\n",
      "Episode 420 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 430 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 440 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 450 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 460 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 470 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 480 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 490 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 500 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 510 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 520 | Avg Reward: A1: 6.00, A2: -9.20\n",
      "Episode 530 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 540 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 550 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 560 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 570 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 580 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 590 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 600 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 610 | Avg Reward: A1: 4.80, A2: -5.00\n",
      "Episode 620 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 630 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 640 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 650 | Avg Reward: A1: 5.20, A2: -5.40\n",
      "Episode 660 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 670 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 680 | Avg Reward: A1: 4.90, A2: -5.10\n",
      "Episode 690 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 700 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 710 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 720 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 730 | Avg Reward: A1: 7.00, A2: -6.10\n",
      "Episode 740 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 750 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 760 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 770 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 780 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 790 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 800 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 810 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 820 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 830 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 840 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 850 | Avg Reward: A1: 5.30, A2: -7.20\n",
      "Episode 860 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 870 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 880 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 890 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 900 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 910 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 920 | Avg Reward: A1: 4.90, A2: -5.10\n",
      "Episode 930 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 940 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 950 | Avg Reward: A1: 6.00, A2: -5.70\n",
      "Episode 960 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 970 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 980 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 990 | Avg Reward: A1: 5.00, A2: -5.00\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = GridGame1()\n",
    "agent = NashQLearner(env, alpha=0.1, gamma=0.99, epsilon=0.5)\n",
    "\n",
    "all_episode_rewards = []\n",
    "epsilon_decay = []\n",
    "n_episodes = 1000\n",
    "\n",
    "for episode in range(n_episodes):\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    total_reward = [0, 0]\n",
    "\n",
    "    while not done:\n",
    "        actions = agent.get_action(state)\n",
    "        next_state, rewards, terminated, truncated, _ = env.step(actions)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        agent.update(state, actions, rewards, next_state)\n",
    "        state = next_state\n",
    "\n",
    "        total_reward[0] += rewards[0]\n",
    "        total_reward[1] += rewards[1]\n",
    "\n",
    "    all_episode_rewards.append(total_reward)\n",
    "    agent.decay_epsilon(episode)\n",
    "    epsilon_decay.append(agent.epsilon)\n",
    "\n",
    "    if episode % 10 == 0:\n",
    "        avg_rewards = np.mean(all_episode_rewards[-10:], axis=0)\n",
    "        print(f\"Episode {episode:03d} | Avg Reward: A1: {avg_rewards[0]:.2f}, A2: {avg_rewards[1]:.2f}\")\n",
    "\n",
    "# Plot learning curve\n",
    "agent.plot_learning_curve(all_episode_rewards)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nashpy import Game\n",
    "\n",
    "def print_equilibria(Q1_matrix, Q2_matrix):\n",
    "    game = Game(Q1_matrix, Q2_matrix)\n",
    "    equilibria = list(game.support_enumeration())\n",
    "\n",
    "    if len(equilibria) == 0:\n",
    "        print(\"No Nash equilibrium found.\")\n",
    "    elif len(equilibria) == 1:\n",
    "        print(\"One Nash equilibrium found:\")\n",
    "    else:\n",
    "        print(f\"{len(equilibria)} Nash equilibria found:\")\n",
    "\n",
    "    for i, (pi1, pi2) in enumerate(equilibria):\n",
    "        print(f\"Equilibrium {i+1}:\")\n",
    "        print(f\"  Agent 1 strategy: {pi1}\")\n",
    "        print(f\"  Agent 2 strategy: {pi2}\")\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_q_values(agent, agent_id=1, state=(1, 1, 1, 1)):\n",
    "    q_table = agent.Q1[state] if agent_id == 1 else agent.Q2[state]\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(q_table, annot=True, fmt=\".1f\", cmap=\"viridis\")\n",
    "\n",
    "    action_names = [\"UP\", \"DOWN\", \"LEFT\", \"RIGHT\"]\n",
    "    plt.xticks(ticks=[0.5, 1.5, 2.5, 3.5], labels=action_names)\n",
    "    plt.yticks(ticks=[0.5, 1.5, 2.5, 3.5], labels=action_names)\n",
    "\n",
    "    plt.xlabel(\"Agent 2 Actions\")\n",
    "    plt.ylabel(\"Agent 1 Actions\")\n",
    "    plt.title(f\"Q-Values for Agent {agent_id} in State {state}\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def visualize_trajectory(agent, env):\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    trajectory1 = [state[\"agent1\"]]\n",
    "    trajectory2 = [state[\"agent2\"]]\n",
    "\n",
    "    while not done:\n",
    "        actions = agent.get_action(state, explore=False)\n",
    "        next_state, _, terminated, truncated, _ = env.step(actions)\n",
    "        done = terminated or truncated\n",
    "        trajectory1.append(next_state[\"agent1\"])\n",
    "        trajectory2.append(next_state[\"agent2\"])\n",
    "        state = next_state\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "\n",
    "    def convert_2darray_coords(pos):\n",
    "        return (pos[1], 2 - pos[0])  # Flip row index for display\n",
    "\n",
    "    # Plot agent trajectories\n",
    "    plt.plot(*zip(*[convert_2darray_coords(p) for p in trajectory1]),\n",
    "             marker='o', label=\"Agent 1\", color='blue')\n",
    "    plt.plot(*zip(*[convert_2darray_coords(p) for p in trajectory2]),\n",
    "             marker='s', label=\"Agent 2\", color='red')\n",
    "\n",
    "    # Plot agent initial positions\n",
    "    init1 = convert_2darray_coords(env.init_positions[0])\n",
    "    init2 = convert_2darray_coords(env.init_positions[1])\n",
    "    plt.text(*init1, \"A1\", fontsize=12, ha='center', va='center', color='blue', fontweight='bold')\n",
    "    plt.text(*init2, \"A2\", fontsize=12, ha='center', va='center', color='red', fontweight='bold')\n",
    "\n",
    "    # Axis config\n",
    "    plt.xticks([0, 1, 2], ['0', '1', '2'])\n",
    "    plt.yticks([0, 1, 2], ['2', '1', '0'])\n",
    "    plt.grid(True)\n",
    "    plt.xlim(-0.5, 2.5)\n",
    "    plt.ylim(-0.5, 2.5)\n",
    "\n",
    "    plt.gca().xaxis.set_ticks_position('top')\n",
    "    plt.gca().xaxis.set_label_position('top')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.title(\"Agents Trajectories\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_exploration_decay(epsilon_decay):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(np.arange(len(epsilon_decay)), epsilon_decay)\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Epsilon\")\n",
    "    plt.title(\"Exploration Rate Decay\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_trajectory(agent, env)\n",
    "plot_exploration_decay(epsilon_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trajectories Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_expert_trajectories(agent, env, n_episodes=100):\n",
    "    expert_trajectories = []\n",
    "\n",
    "    for _ in range(n_episodes):\n",
    "        episode = []\n",
    "        obs, _ = env.reset()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            actions = agent.get_action(obs, explore=False)\n",
    "            next_obs, rewards, terminated, truncated, _ = env.step(actions)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            step = {\n",
    "                \"obs\": {\n",
    "                    \"agent1\": obs[\"agent1\"].copy(),\n",
    "                    \"agent2\": obs[\"agent2\"].copy()\n",
    "                },\n",
    "                \"actions\": actions,\n",
    "                \"next_obs\": {\n",
    "                    \"agent1\": next_obs[\"agent1\"].copy(),\n",
    "                    \"agent2\": next_obs[\"agent2\"].copy()\n",
    "                },\n",
    "                \"rewards\": rewards,\n",
    "                \"done\": done\n",
    "            }\n",
    "\n",
    "            episode.append(step)\n",
    "            obs = next_obs\n",
    "\n",
    "        expert_trajectories.append(episode)\n",
    "\n",
    "    return expert_trajectories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trajectories(trajectories, max_episodes=10):\n",
    "    for ep_idx, episode in enumerate(trajectories[:max_episodes]):\n",
    "        print(f\"\\n=== Trajectory {ep_idx + 1} ===\")\n",
    "        for t, step in enumerate(episode):\n",
    "            obs1 = tuple(step[\"obs\"][\"agent1\"])\n",
    "            obs2 = tuple(step[\"obs\"][\"agent2\"])\n",
    "            a1, a2 = step[\"actions\"]\n",
    "            r1, r2 = step[\"rewards\"]\n",
    "            next_obs1 = tuple(step[\"next_obs\"][\"agent1\"])\n",
    "            next_obs2 = tuple(step[\"next_obs\"][\"agent2\"])\n",
    "            done = step[\"done\"]\n",
    "\n",
    "            print(f\"Step {t:02d} | Obs: A1{obs1}, A2{obs2} | \"\n",
    "                  f\"Actions: A1={a1}, A2={a2} | \"\n",
    "                  f\"Rewards: A1={r1}, A2={r2} | \"\n",
    "                  f\"Next: A1{next_obs1}, A2{next_obs2} | Done: {done}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Trajectory 1 ===\n",
      "Step 00 | Obs: A1(0, 0), A2(0, 2) | Actions: A1=1, A2=2 | Rewards: A1=2, A2=-4 | Next: A1(1, 0), A2(0, 1) | Done: {}\n",
      "Step 01 | Obs: A1(1, 0), A2(0, 1) | Actions: A1=1, A2=2 | Rewards: A1=3, A2=-1 | Next: A1(2, 0), A2(0, 0) | Done: True\n",
      "\n",
      "=== Trajectory 2 ===\n",
      "Step 00 | Obs: A1(0, 0), A2(0, 2) | Actions: A1=1, A2=2 | Rewards: A1=2, A2=-4 | Next: A1(1, 0), A2(0, 1) | Done: {}\n",
      "Step 01 | Obs: A1(1, 0), A2(0, 1) | Actions: A1=1, A2=2 | Rewards: A1=3, A2=-1 | Next: A1(2, 0), A2(0, 0) | Done: True\n",
      "\n",
      "=== Trajectory 3 ===\n",
      "Step 00 | Obs: A1(0, 0), A2(0, 2) | Actions: A1=1, A2=2 | Rewards: A1=2, A2=-4 | Next: A1(1, 0), A2(0, 1) | Done: {}\n",
      "Step 01 | Obs: A1(1, 0), A2(0, 1) | Actions: A1=1, A2=2 | Rewards: A1=3, A2=-1 | Next: A1(2, 0), A2(0, 0) | Done: True\n",
      "\n",
      "=== Trajectory 4 ===\n",
      "Step 00 | Obs: A1(0, 0), A2(0, 2) | Actions: A1=1, A2=2 | Rewards: A1=2, A2=-4 | Next: A1(1, 0), A2(0, 1) | Done: {}\n",
      "Step 01 | Obs: A1(1, 0), A2(0, 1) | Actions: A1=1, A2=2 | Rewards: A1=3, A2=-1 | Next: A1(2, 0), A2(0, 0) | Done: True\n",
      "\n",
      "=== Trajectory 5 ===\n",
      "Step 00 | Obs: A1(0, 0), A2(0, 2) | Actions: A1=1, A2=2 | Rewards: A1=2, A2=-4 | Next: A1(1, 0), A2(0, 1) | Done: {}\n",
      "Step 01 | Obs: A1(1, 0), A2(0, 1) | Actions: A1=1, A2=2 | Rewards: A1=3, A2=-1 | Next: A1(2, 0), A2(0, 0) | Done: True\n",
      "\n",
      "=== Trajectory 6 ===\n",
      "Step 00 | Obs: A1(0, 0), A2(0, 2) | Actions: A1=1, A2=2 | Rewards: A1=2, A2=-4 | Next: A1(1, 0), A2(0, 1) | Done: {}\n",
      "Step 01 | Obs: A1(1, 0), A2(0, 1) | Actions: A1=1, A2=2 | Rewards: A1=3, A2=-1 | Next: A1(2, 0), A2(0, 0) | Done: True\n",
      "\n",
      "=== Trajectory 7 ===\n",
      "Step 00 | Obs: A1(0, 0), A2(0, 2) | Actions: A1=1, A2=2 | Rewards: A1=2, A2=-4 | Next: A1(1, 0), A2(0, 1) | Done: {}\n",
      "Step 01 | Obs: A1(1, 0), A2(0, 1) | Actions: A1=1, A2=2 | Rewards: A1=3, A2=-1 | Next: A1(2, 0), A2(0, 0) | Done: True\n",
      "\n",
      "=== Trajectory 8 ===\n",
      "Step 00 | Obs: A1(0, 0), A2(0, 2) | Actions: A1=1, A2=2 | Rewards: A1=2, A2=-4 | Next: A1(1, 0), A2(0, 1) | Done: {}\n",
      "Step 01 | Obs: A1(1, 0), A2(0, 1) | Actions: A1=1, A2=2 | Rewards: A1=3, A2=-1 | Next: A1(2, 0), A2(0, 0) | Done: True\n",
      "\n",
      "=== Trajectory 9 ===\n",
      "Step 00 | Obs: A1(0, 0), A2(0, 2) | Actions: A1=1, A2=2 | Rewards: A1=2, A2=-4 | Next: A1(1, 0), A2(0, 1) | Done: {}\n",
      "Step 01 | Obs: A1(1, 0), A2(0, 1) | Actions: A1=1, A2=2 | Rewards: A1=3, A2=-1 | Next: A1(2, 0), A2(0, 0) | Done: True\n",
      "\n",
      "=== Trajectory 10 ===\n",
      "Step 00 | Obs: A1(0, 0), A2(0, 2) | Actions: A1=1, A2=2 | Rewards: A1=2, A2=-4 | Next: A1(1, 0), A2(0, 1) | Done: {}\n",
      "Step 01 | Obs: A1(1, 0), A2(0, 1) | Actions: A1=1, A2=2 | Rewards: A1=3, A2=-1 | Next: A1(2, 0), A2(0, 0) | Done: True\n"
     ]
    }
   ],
   "source": [
    "# printing some trajectories -> they're all identical\n",
    "trajectories = sample_expert_trajectories(agent, env, n_episodes=100)\n",
    "print_trajectories(trajectories, max_episodes=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
