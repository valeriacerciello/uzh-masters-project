{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating expert trajectories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom 2-agent grid world using Gymnasium.\n",
    "\n",
    "*Features*:\n",
    "\n",
    "Grid size: 3x3\n",
    "\n",
    "A1 starts at (0,0), A2 starts at (0,2).\n",
    "\n",
    "Goals: A1 reach row 2, A2 reach column 0\n",
    "\n",
    "Actions: [UP, DOWN, LEFT, RIGHT] (4 actions per agent).\n",
    "\n",
    "Observations: Positions of both agents.\n",
    "\n",
    "Rewards:\n",
    "\n",
    "A1:   \n",
    "|   |   |   |       \n",
    "| - | - | - |       \n",
    "| 1 | 4 | 7 |       \n",
    "| 2 | 5 | 8 |      \n",
    "| 3 | 6 | 9 |\n",
    "\n",
    "A2:   \n",
    "|    |    |    |       \n",
    "| -- | -- | -- |       \n",
    "| -1 | -4 | -7 |       \n",
    "| -2 | -5 | -8 |      \n",
    "| -3 | -6 | -9 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "\n",
    "grid_size = 3\n",
    "max_steps = 100000\n",
    "\n",
    "class GridGame1(gym.Env):\n",
    "    metadata = {'render_modes': ['human'], 'render_fps': 4}\n",
    "\n",
    "    def __init__(self):\n",
    "        self.grid_size = grid_size\n",
    "        self.n_agents = 2\n",
    "        self.max_steps = max_steps\n",
    "\n",
    "        # Updated start positions\n",
    "        self.init_positions = [(0, 0), (0, 2)]  # A1 at (0,0), A2 at (0,2)\n",
    "\n",
    "        # Goals\n",
    "        self.goal_row_a1 = 2  # A1 needs to reach row 2\n",
    "        self.goal_col_a2 = 0  # A2 needs to reach column 0\n",
    "\n",
    "        # Action space: 4 actions per agent\n",
    "        self.action_space = spaces.MultiDiscrete([4, 4])\n",
    "\n",
    "        # Observation space: positions of both agents\n",
    "        self.observation_space = spaces.Dict({\n",
    "            \"agent1\": spaces.Box(0, self.grid_size - 1, shape=(2,), dtype=int),\n",
    "            \"agent2\": spaces.Box(0, self.grid_size - 1, shape=(2,), dtype=int)\n",
    "        })\n",
    "\n",
    "        # Reset environment\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.agent1_pos, self.agent2_pos = self.init_positions\n",
    "        self.steps = 0\n",
    "        return self._get_obs(), {}\n",
    "\n",
    "    def _get_obs(self):\n",
    "        return {\n",
    "            \"agent1\": np.array(self.agent1_pos),\n",
    "            \"agent2\": np.array(self.agent2_pos)\n",
    "        }\n",
    "\n",
    "    def step(self, actions):\n",
    "        self.steps += 1\n",
    "\n",
    "        # Move agents\n",
    "        new_pos1 = self._move(self.agent1_pos, actions[0])\n",
    "        new_pos2 = self._move(self.agent2_pos, actions[1])\n",
    "\n",
    "        # Check for collision\n",
    "        if new_pos1 == new_pos2:\n",
    "            new_pos1 = self.agent1_pos\n",
    "            new_pos2 = self.agent2_pos\n",
    "            reward1 = reward2 = -1  # penalty for collision\n",
    "            done = False\n",
    "        else:\n",
    "            reward1 = self._compute_reward_agent1(new_pos1)\n",
    "            reward2 = self._compute_reward_agent2(new_pos2)\n",
    "\n",
    "            self.agent1_pos = new_pos1\n",
    "            self.agent2_pos = new_pos2\n",
    "\n",
    "            done_a1 = self.agent1_pos[0] == self.goal_row_a1\n",
    "            done_a2 = self.agent2_pos[1] == self.goal_col_a2\n",
    "            done = done_a1 or done_a2\n",
    "\n",
    "        # Force episode end after max_steps\n",
    "        done = done or self.steps >= self.max_steps\n",
    "\n",
    "        return self._get_obs(), (reward1, reward2), done, {}, {}\n",
    "\n",
    "    def _compute_reward_agent1(self, pos):\n",
    "        x, y = pos\n",
    "        # A1 reward matrix values (flipped row-wise)\n",
    "        reward_map = [\n",
    "            [1, 4, 7],\n",
    "            [2, 5, 8],\n",
    "            [3, 6, 9]\n",
    "        ]\n",
    "        return reward_map[x][y]\n",
    "\n",
    "    def _compute_reward_agent2(self, pos):\n",
    "        x, y = pos\n",
    "        # A2 reward matrix values (flipped and negative)\n",
    "        reward_map = [\n",
    "            [-1, -4, -7],\n",
    "            [-2, -5, -8],\n",
    "            [-3, -6, -9]\n",
    "        ]\n",
    "        return reward_map[x][y]\n",
    "\n",
    "    def _move(self, pos, action):\n",
    "        x, y = pos\n",
    "        if action == 0:  # UP\n",
    "            x = max(0, x - 1)\n",
    "        elif action == 1:  # DOWN\n",
    "            x = min(self.grid_size - 1, x + 1)\n",
    "        elif action == 2:  # LEFT\n",
    "            y = max(0, y - 1)\n",
    "        elif action == 3:  # RIGHT\n",
    "            y = min(self.grid_size - 1, y + 1)\n",
    "        return (x, y)\n",
    "\n",
    "    def render(self):\n",
    "        grid = np.full((3, 3), '路', dtype='<U3')\n",
    "\n",
    "        # Mark goals\n",
    "        for col in range(self.grid_size):\n",
    "            grid[self.goal_row_a1, col] = 'G1'\n",
    "        for row in range(self.grid_size):\n",
    "            if grid[row, self.goal_col_a2] == 'G1':\n",
    "                grid[row, self.goal_col_a2] = 'G1/2'\n",
    "            else:\n",
    "                grid[row, self.goal_col_a2] = 'G2'\n",
    "\n",
    "        # Mark agents\n",
    "        ax1, ay1 = self.agent1_pos\n",
    "        ax2, ay2 = self.agent2_pos\n",
    "        if (ax1, ay1) == (ax2, ay2):\n",
    "            grid[ax1, ay1] = 'A*'\n",
    "        else:\n",
    "            grid[ax1, ay1] = 'A1'\n",
    "            grid[ax2, ay2] = 'A2'\n",
    "\n",
    "        print(\"Current State:\")\n",
    "        for row in grid:\n",
    "            print(\" \".join(f\"{cell:^5}\" for cell in row))\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current State:\n",
      " A1     路    A2  \n",
      " G2     路     路  \n",
      " G1/   G1    G1  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = GridGame1()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "def plot_env_state(env):\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "    # 1) Draw grid lines\n",
    "    for x in range(env.grid_size + 1):\n",
    "        ax.axvline(x, color='gray', linestyle='-', linewidth=1)\n",
    "    for y in range(env.grid_size + 1):\n",
    "        ax.axhline(y, color='gray', linestyle='-', linewidth=1)\n",
    "\n",
    "    # 2) Set axis limits and ticks\n",
    "    ax.set_xlim(0, env.grid_size)\n",
    "    ax.set_ylim(0, env.grid_size)\n",
    "    ax.set_xticks([i + 0.5 for i in range(env.grid_size)])\n",
    "    ax.set_yticks([i + 0.5 for i in range(env.grid_size)])\n",
    "    ax.set_xticklabels([str(i) for i in range(env.grid_size)])\n",
    "    ax.set_yticklabels([str(i) for i in range(env.grid_size)])\n",
    "\n",
    "    ax.xaxis.set_ticks_position('top')\n",
    "    ax.xaxis.set_label_position('top')\n",
    "    ax.invert_yaxis()\n",
    "    ax.set_title(\"Grid Game 1\")\n",
    "\n",
    "    # Helper for plotting cells\n",
    "    def cell_coords(row, col):\n",
    "        return (col, row)\n",
    "\n",
    "    def center_coords(row, col):\n",
    "        return (col + 0.5, row + 0.5)\n",
    "\n",
    "    # 3) Highlight A1's goal row (row 2)\n",
    "    for col in range(env.grid_size):\n",
    "        current_label = \"G1\"\n",
    "        if col == env.goal_col_a2 and 0 < env.grid_size:\n",
    "            current_label = \"G1 & G2\"\n",
    "        ax.add_patch(patches.Rectangle(cell_coords(env.goal_row_a1, col), 1, 1, facecolor='red', alpha=0.5))\n",
    "        ax.text(*center_coords(env.goal_row_a1, col), current_label, ha='center', va='center', fontsize=10, color='red')\n",
    "\n",
    "    # 4) Highlight A2's goal column (col 0)\n",
    "    for row in range(env.grid_size):\n",
    "        current_label = \"G2\"\n",
    "        # If G1 already in the cell, merge label\n",
    "        if row == env.goal_row_a1 and 0 < env.grid_size:\n",
    "            current_label = \"G1 & G2\"\n",
    "        ax.add_patch(patches.Rectangle(cell_coords(row, env.goal_col_a2), 1, 1, facecolor='lightblue', alpha=0.5))\n",
    "        ax.text(*center_coords(row, env.goal_col_a2), current_label, ha='center', va='center', fontsize=10, color='blue')\n",
    "\n",
    "    # 5) Plot agents\n",
    "    a1_row, a1_col = env.agent1_pos\n",
    "    a2_row, a2_col = env.agent2_pos\n",
    "\n",
    "    if (a1_row, a1_col) == (a2_row, a2_col):\n",
    "        ax.plot(*center_coords(a1_row, a1_col), 'purple', marker='o', markersize=20)\n",
    "        ax.text(*center_coords(a1_row, a1_col), \"A1&A2\", fontsize=10, ha='center', va='center', color='white', fontweight='bold')\n",
    "    else:\n",
    "        ax.plot(*center_coords(a1_row, a1_col), 'ro', markersize=20)\n",
    "        ax.text(*center_coords(a1_row, a1_col), \"A1\", fontsize=12, ha='center', va='center', color='white', fontweight='bold')\n",
    "\n",
    "        ax.plot(*center_coords(a2_row, a2_col), 'bo', markersize=20)\n",
    "        ax.text(*center_coords(a2_row, a2_col), \"A2\", fontsize=12, ha='center', va='center', color='white', fontweight='bold')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Test with updated env\n",
    "env = GridGame1()\n",
    "env.reset()\n",
    "plot_env_state(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nash Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nashpy import Game\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class NashQLearner:\n",
    "    def __init__(self, env, alpha=0.1, gamma=0.99, epsilon=0.5, min_epsilon=0.01, decay_rate=0.0005):\n",
    "        self.env = env\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.min_epsilon = min_epsilon\n",
    "        self.decay_rate = decay_rate\n",
    "        self.epsilon_history = []\n",
    "\n",
    "        self.grid_size = env.grid_size\n",
    "        self.Q1 = np.zeros((self.grid_size, self.grid_size, self.grid_size, self.grid_size, 4, 4))\n",
    "        self.Q2 = np.zeros((self.grid_size, self.grid_size, self.grid_size, self.grid_size, 4, 4))\n",
    "\n",
    "    def decay_epsilon(self, episode):\n",
    "        self.epsilon = max(self.min_epsilon, self.epsilon * np.exp(-self.decay_rate * episode))\n",
    "        self.epsilon_history.append(self.epsilon)\n",
    "\n",
    "    def get_action(self, state, explore=True):\n",
    "        s = self._state_to_index(state)\n",
    "    \n",
    "        game = Game(self.Q1[s], self.Q2[s])\n",
    "        equilibria = list(game.support_enumeration())\n",
    "    \n",
    "        if len(equilibria) > 1:\n",
    "            print(f\"\\n Multiple equilibria found at state {s}:\")\n",
    "            print_equilibria(self.Q1[s], self.Q2[s])\n",
    "    \n",
    "        if not equilibria or (explore and np.random.rand() < self.epsilon):\n",
    "            return np.random.randint(0, 4), np.random.randint(0, 4)\n",
    "    \n",
    "        pi1, pi2 = equilibria[0]\n",
    "        a1 = np.random.choice(4, p=pi1)\n",
    "        a2 = np.random.choice(4, p=pi2)\n",
    "        return a1, a2\n",
    "\n",
    "    def update(self, state, actions, rewards, next_state):\n",
    "        s = self._state_to_index(state)\n",
    "        s_next = self._state_to_index(next_state)\n",
    "        a1, a2 = actions\n",
    "        r1, r2 = rewards\n",
    "\n",
    "        game_next = Game(self.Q1[s_next], self.Q2[s_next])\n",
    "        equilibria_next = list(game_next.support_enumeration())\n",
    "\n",
    "        if not equilibria_next:\n",
    "            nash_q1 = 0\n",
    "            nash_q2 = 0\n",
    "        else:\n",
    "            pi1_next, pi2_next = equilibria_next[0]  # Again, just take the first\n",
    "            nash_q1 = np.sum(np.outer(pi1_next, pi2_next) * self.Q1[s_next])\n",
    "            nash_q2 = np.sum(np.outer(pi1_next, pi2_next) * self.Q2[s_next])\n",
    "\n",
    "        self.Q1[s][a1][a2] += self.alpha * (r1 + self.gamma * nash_q1 - self.Q1[s][a1][a2])\n",
    "        self.Q2[s][a1][a2] += self.alpha * (r2 + self.gamma * nash_q2 - self.Q2[s][a1][a2])\n",
    "\n",
    "    def _state_to_index(self, state):\n",
    "        return (\n",
    "            state[\"agent1\"][0], state[\"agent1\"][1],\n",
    "            state[\"agent2\"][0], state[\"agent2\"][1]\n",
    "        )\n",
    "\n",
    "    def plot_learning_curve(self, episode_rewards):\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(np.arange(len(episode_rewards)), [r[0] for r in episode_rewards], label=\"Agent 1\")\n",
    "        plt.plot(np.arange(len(episode_rewards)), [r[1] for r in episode_rewards], label=\"Agent 2\")\n",
    "        plt.xlabel(\"Episode\")\n",
    "        plt.ylabel(\"Total Reward\")\n",
    "        plt.title(\"Learning Curve\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lents\\anaconda3\\lib\\site-packages\\nashpy\\algorithms\\support_enumeration.py:260: RuntimeWarning: \n",
      "An even number of (16) equilibria was returned. This\n",
      "indicates that the game is degenerate. Consider using another algorithm\n",
      "to investigate.\n",
      "                  \n",
      "  warnings.warn(warning, RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Multiple equilibria found at state (0, 0, 0, 2):\n",
      "16 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 12:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 13:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 14:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 15:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 16:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 1, 0, 2):\n",
      "16 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 12:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 13:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 14:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 15:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 16:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 1, 1, 2):\n",
      "16 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 12:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 13:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 14:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 15:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 16:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 1, 0, 2):\n",
      "12 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 12:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lents\\anaconda3\\lib\\site-packages\\nashpy\\algorithms\\support_enumeration.py:260: RuntimeWarning: \n",
      "An even number of (12) equilibria was returned. This\n",
      "indicates that the game is degenerate. Consider using another algorithm\n",
      "to investigate.\n",
      "                  \n",
      "  warnings.warn(warning, RuntimeWarning)\n",
      "c:\\Users\\lents\\anaconda3\\lib\\site-packages\\nashpy\\algorithms\\support_enumeration.py:260: RuntimeWarning: \n",
      "An even number of (8) equilibria was returned. This\n",
      "indicates that the game is degenerate. Consider using another algorithm\n",
      "to investigate.\n",
      "                  \n",
      "  warnings.warn(warning, RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Multiple equilibria found at state (0, 1, 0, 2):\n",
      "8 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 0, 0, 2):\n",
      "12 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 12:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 0, 0, 2):\n",
      "11 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 0, 0, 2):\n",
      "7 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lents\\anaconda3\\lib\\site-packages\\nashpy\\algorithms\\support_enumeration.py:260: RuntimeWarning: \n",
      "An even number of (4) equilibria was returned. This\n",
      "indicates that the game is degenerate. Consider using another algorithm\n",
      "to investigate.\n",
      "                  \n",
      "  warnings.warn(warning, RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Multiple equilibria found at state (0, 1, 0, 2):\n",
      "4 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 1, 0, 2):\n",
      "3 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (1, 1, 0, 1):\n",
      "16 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 12:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 13:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 14:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 15:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 16:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (1, 2, 0, 1):\n",
      "16 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 12:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 13:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 14:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 15:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 16:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 2, 0, 1):\n",
      "16 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 12:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 13:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 14:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 15:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 16:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Episode 000 | Avg Reward: A1: 47.00, A2: -65.00\n",
      "\n",
      " Multiple equilibria found at state (0, 0, 0, 2):\n",
      "7 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 0, 0, 2):\n",
      "7 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (1, 0, 1, 2):\n",
      "16 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 12:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 13:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 14:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 15:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 16:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (1, 1, 0, 2):\n",
      "16 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 12:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 13:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 14:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 15:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 16:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 0, 0, 2):\n",
      "3 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 2, 0, 1):\n",
      "12 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 12:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (1, 2, 0, 1):\n",
      "12 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 12:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 2, 1, 1):\n",
      "16 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 12:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 13:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 14:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 15:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 16:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 2, 0, 1):\n",
      "8 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 2, 1, 1):\n",
      "12 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 12:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 2, 2, 1):\n",
      "16 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 12:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 13:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 14:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 15:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 16:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 2, 1, 1):\n",
      "8 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 1, 1, 2):\n",
      "12 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 12:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 1, 2, 2):\n",
      "16 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 12:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 13:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 14:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 15:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 16:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 0, 2, 2):\n",
      "16 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 12:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 13:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 14:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 15:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 16:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (1, 0, 1, 2):\n",
      "12 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 12:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 0, 1, 1):\n",
      "16 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 12:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 13:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 14:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 15:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 16:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 0, 0, 1):\n",
      "16 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 12:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 13:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 14:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 15:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 16:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 0, 0, 1):\n",
      "12 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 12:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 0, 0, 2):\n",
      "3 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 0, 0, 1):\n",
      "8 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (1, 0, 0, 1):\n",
      "16 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 12:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 13:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 14:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 15:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 16:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 0, 0, 1):\n",
      "8 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 0, 1, 1):\n",
      "12 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 12:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 0, 2, 1):\n",
      "16 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 12:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 13:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 14:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 15:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 16:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (1, 0, 2, 1):\n",
      "16 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 12:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 13:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 14:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 15:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 16:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 0, 1, 1):\n",
      "8 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 1, 2, 1):\n",
      "16 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 12:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 13:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 14:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 15:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 16:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 1, 2, 2):\n",
      "12 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 12:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 1, 1, 2):\n",
      "8 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 1, 1, 1):\n",
      "16 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 12:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 13:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 14:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 15:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 16:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 2, 1, 2):\n",
      "16 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 12:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 13:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 14:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 15:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 16:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (1, 2, 1, 1):\n",
      "16 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 12:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 13:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 14:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 15:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 16:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (1, 2, 2, 1):\n",
      "16 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 12:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 13:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 14:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 15:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 16:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (1, 0, 0, 2):\n",
      "16 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 12:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 13:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 14:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 15:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 16:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 0, 0, 1):\n",
      "4 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lents\\anaconda3\\lib\\site-packages\\nashpy\\algorithms\\support_enumeration.py:260: RuntimeWarning: \n",
      "An even number of (0) equilibria was returned. This\n",
      "indicates that the game is degenerate. Consider using another algorithm\n",
      "to investigate.\n",
      "                  \n",
      "  warnings.warn(warning, RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Multiple equilibria found at state (0, 0, 1, 2):\n",
      "16 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 12:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 13:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 14:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 15:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 16:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (1, 0, 0, 1):\n",
      "12 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 12:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 0, 1, 1):\n",
      "8 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 0, 1, 1):\n",
      "4 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 0, 1, 2):\n",
      "12 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 12:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 0, 2, 2):\n",
      "12 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 12:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 0, 2, 1):\n",
      "12 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 12:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (1, 0, 0, 1):\n",
      "8 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (1, 0, 0, 1):\n",
      "7 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (1, 0, 1, 2):\n",
      "8 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (1, 1, 2, 2):\n",
      "16 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 12:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 13:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 14:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 15:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 16:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (1, 0, 2, 1):\n",
      "12 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 12:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 0, 2, 1):\n",
      "8 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 0, 1, 2):\n",
      "8 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (1, 0, 1, 2):\n",
      "4 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (1, 1, 0, 2):\n",
      "12 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 12:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 1, 1, 2):\n",
      "4 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 1, 1, 1):\n",
      "12 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 12:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 1, 1, 1):\n",
      "11 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 1, 2, 1):\n",
      "12 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 12:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (1, 0, 0, 2):\n",
      "12 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 12:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (1, 0, 0, 2):\n",
      "8 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 0, 1, 2):\n",
      "8 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 1, 2, 1):\n",
      "8 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (1, 1, 2, 1):\n",
      "16 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 12:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 13:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 14:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 15:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 16:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 1, 1, 1):\n",
      "7 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (1, 0, 0, 1):\n",
      "3 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 0, 1, 2):\n",
      "4 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (1, 0, 2, 2):\n",
      "16 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 12:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 13:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 14:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 15:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 16:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (1, 0, 1, 2):\n",
      "4 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (1, 1, 1, 2):\n",
      "16 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 12:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 13:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 14:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 15:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 16:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (1, 1, 0, 2):\n",
      "8 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (1, 1, 0, 2):\n",
      "7 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 1, 0, 2):\n",
      "3 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [0.44203136 0.55796864 0.         0.        ]\n",
      "  Agent 2 strategy: [0.         0.         0.38107382 0.61892618]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [0.40732615 0.         0.         0.59267385]\n",
      "  Agent 2 strategy: [0.         0.68417003 0.31582997 0.        ]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0.40732615 0.4674196  0.         0.12525425]\n",
      "  Agent 2 strategy: [0.         0.10661155 0.3800474  0.51334105]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 1, 0, 2):\n",
      "3 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [0.44203136 0.55796864 0.         0.        ]\n",
      "  Agent 2 strategy: [0.         0.         0.38107382 0.61892618]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [0.40732615 0.         0.         0.59267385]\n",
      "  Agent 2 strategy: [0.         0.68417003 0.31582997 0.        ]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0.40732615 0.4674196  0.         0.12525425]\n",
      "  Agent 2 strategy: [0.         0.10661155 0.3800474  0.51334105]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 2, 0, 1):\n",
      "4 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 2, 0, 1):\n",
      "4 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0.8 0.2 0.  0. ]\n",
      "  Agent 2 strategy: [0.46666667 0.         0.53333333 0.        ]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (1, 0, 0, 2):\n",
      "4 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (1, 0, 0, 2):\n",
      "4 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (1, 0, 1, 1):\n",
      "16 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 12:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 13:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 14:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 15:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 16:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Episode 010 | Avg Reward: A1: 28.80, A2: -62.00\n",
      "\n",
      " Multiple equilibria found at state (1, 0, 1, 1):\n",
      "12 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 12:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 0, 2, 1):\n",
      "4 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 1, 2, 2):\n",
      "8 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (1, 1, 2, 2):\n",
      "12 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 12:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (1, 2, 2, 2):\n",
      "16 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 12:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 13:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 14:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 15:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 16:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (1, 2, 2, 2):\n",
      "12 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 12:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 2, 1, 2):\n",
      "12 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 12:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 2, 1, 2):\n",
      "8 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 2, 1, 2):\n",
      "7 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 1, 1, 1):\n",
      "3 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 0, 2, 1):\n",
      "4 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 0, 2, 2):\n",
      "8 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 0, 2, 2):\n",
      "4 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Episode 020 | Avg Reward: A1: 11.70, A2: -27.50\n",
      "\n",
      " Multiple equilibria found at state (1, 1, 0, 1):\n",
      "12 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 12:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 1, 1, 1):\n",
      "3 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (1, 1, 0, 1):\n",
      "8 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (1, 1, 0, 2):\n",
      "7 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 2, 0, 1):\n",
      "4 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0.8 0.2 0.  0. ]\n",
      "  Agent 2 strategy: [0.46666667 0.         0.53333333 0.        ]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (1, 2, 0, 2):\n",
      "16 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 12:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 13:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 14:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 15:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 16:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (1, 2, 0, 2):\n",
      "15 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 12:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 13:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 14:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 15:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 2, 1, 2):\n",
      "7 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 2, 2, 2):\n",
      "16 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 12:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 13:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 14:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 15:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 16:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 2, 1, 2):\n",
      "3 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (1, 2, 2, 2):\n",
      "8 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 2, 2, 1):\n",
      "12 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 12:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 2, 1, 1):\n",
      "4 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 2, 1, 2):\n",
      "3 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (1, 2, 0, 2):\n",
      "11 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (1, 2, 0, 1):\n",
      "8 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (1, 0, 1, 1):\n",
      "8 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Episode 030 | Avg Reward: A1: 19.00, A2: -23.80\n",
      "\n",
      " Multiple equilibria found at state (0, 0, 1, 2):\n",
      "4 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 0, 1, 2):\n",
      "4 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (1, 2, 0, 1):\n",
      "4 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (1, 1, 0, 1):\n",
      "4 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (1, 1, 1, 2):\n",
      "12 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 12:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 1, 2, 2):\n",
      "4 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 1, 2, 1):\n",
      "4 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 2, 2, 1):\n",
      "12 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 12:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (1, 2, 1, 1):\n",
      "12 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [1. 0. 0. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 12:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Episode 040 | Avg Reward: A1: 14.70, A2: -23.90\n",
      "\n",
      " Multiple equilibria found at state (0, 0, 2, 2):\n",
      "4 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (0, 0, 2, 2):\n",
      "2 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [0.38313782 0.43103005 0.18583214 0.        ]\n",
      "  Agent 2 strategy: [0.33333333 0.03242871 0.63423796 0.        ]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [0.38313782 0.43103005 0.18583214 0.        ]\n",
      "  Agent 2 strategy: [0.33333333 0.         0.63423796 0.03242871]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lents\\anaconda3\\lib\\site-packages\\nashpy\\algorithms\\support_enumeration.py:260: RuntimeWarning: \n",
      "An even number of (2) equilibria was returned. This\n",
      "indicates that the game is degenerate. Consider using another algorithm\n",
      "to investigate.\n",
      "                  \n",
      "  warnings.warn(warning, RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Multiple equilibria found at state (0, 0, 2, 2):\n",
      "2 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [0.42024614 0.47277691 0.10697695 0.        ]\n",
      "  Agent 2 strategy: [0.33333333 0.33247686 0.33418981 0.        ]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [0.42024614 0.47277691 0.10697695 0.        ]\n",
      "  Agent 2 strategy: [0.33333333 0.         0.33418981 0.33247686]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (1, 0, 2, 2):\n",
      "12 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 12:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (1, 0, 2, 1):\n",
      "8 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      " Multiple equilibria found at state (1, 1, 2, 1):\n",
      "12 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 9:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 10:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 1. 0. 0.]\n",
      "\n",
      "Equilibrium 11:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 12:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Episode 050 | Avg Reward: A1: 9.20, A2: -20.80\n",
      "\n",
      " Multiple equilibria found at state (1, 1, 0, 2):\n",
      "3 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Episode 060 | Avg Reward: A1: 6.70, A2: -12.80\n",
      "Episode 070 | Avg Reward: A1: 10.30, A2: -12.10\n",
      "Episode 080 | Avg Reward: A1: 7.00, A2: -9.00\n",
      "Episode 090 | Avg Reward: A1: 5.60, A2: -7.50\n",
      "Episode 100 | Avg Reward: A1: 7.00, A2: -7.20\n",
      "\n",
      " Multiple equilibria found at state (1, 1, 1, 2):\n",
      "8 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 5:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 6:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 7:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 8:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Episode 110 | Avg Reward: A1: 6.30, A2: -6.70\n",
      "\n",
      " Multiple equilibria found at state (0, 2, 1, 1):\n",
      "4 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 1. 0.]\n",
      "\n",
      "Episode 120 | Avg Reward: A1: 9.10, A2: -11.70\n",
      "Episode 130 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 140 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 150 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "\n",
      " Multiple equilibria found at state (1, 0, 1, 1):\n",
      "4 Nash equilibria found:\n",
      "Equilibrium 1:\n",
      "  Agent 1 strategy: [1. 0. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 2:\n",
      "  Agent 1 strategy: [0. 1. 0. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 3:\n",
      "  Agent 1 strategy: [0. 0. 1. 0.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Equilibrium 4:\n",
      "  Agent 1 strategy: [0. 0. 0. 1.]\n",
      "  Agent 2 strategy: [0. 0. 0. 1.]\n",
      "\n",
      "Episode 160 | Avg Reward: A1: 5.40, A2: -6.90\n",
      "Episode 170 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 180 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 190 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 200 | Avg Reward: A1: 5.20, A2: -5.40\n",
      "Episode 210 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 220 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 230 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 240 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 250 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 260 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 270 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 280 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 290 | Avg Reward: A1: 5.30, A2: -6.10\n",
      "Episode 300 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 310 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 320 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 330 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 340 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 350 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 360 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 370 | Avg Reward: A1: 5.10, A2: -5.70\n",
      "Episode 380 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 390 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 400 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 410 | Avg Reward: A1: 5.10, A2: -5.70\n",
      "Episode 420 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 430 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 440 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 450 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 460 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 470 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 480 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 490 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 500 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 510 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 520 | Avg Reward: A1: 6.00, A2: -9.20\n",
      "Episode 530 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 540 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 550 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 560 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 570 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 580 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 590 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 600 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 610 | Avg Reward: A1: 4.80, A2: -5.00\n",
      "Episode 620 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 630 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 640 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 650 | Avg Reward: A1: 5.20, A2: -5.40\n",
      "Episode 660 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 670 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 680 | Avg Reward: A1: 4.90, A2: -5.10\n",
      "Episode 690 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 700 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 710 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 720 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 730 | Avg Reward: A1: 7.00, A2: -6.10\n",
      "Episode 740 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 750 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 760 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 770 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 780 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 790 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 800 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 810 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 820 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 830 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 840 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 850 | Avg Reward: A1: 5.30, A2: -7.20\n",
      "Episode 860 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 870 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 880 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 890 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 900 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 910 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 920 | Avg Reward: A1: 4.90, A2: -5.10\n",
      "Episode 930 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 940 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 950 | Avg Reward: A1: 6.00, A2: -5.70\n",
      "Episode 960 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 970 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 980 | Avg Reward: A1: 5.00, A2: -5.00\n",
      "Episode 990 | Avg Reward: A1: 5.00, A2: -5.00\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA14AAAHWCAYAAABnpFhuAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAAB1sUlEQVR4nO3dd3iUVd7G8XvSJgmQAqSAtCBIUVGKYlQEBEFxRZTXVRd3QVRARaXY0F1WbFgW7IqrKLoWQFddV1kkKhYEAakiXUBQCEUgIYQkk5nn/SNkmJrMJFMyk+/nuriYcuaZM+EwmXt+55zHZBiGIQAAAABA0MSEuwMAAAAAEO0IXgAAAAAQZAQvAAAAAAgyghcAAAAABBnBCwAAAACCjOAFAAAAAEFG8AIAAACAICN4AQAAAECQEbwAAAAAIMgIXgCAeq1NmzYaMWJEuLsBAIhyBC8AQK3NmjVLJpNJP/zwQ7i7EnFKSkr01FNPqWfPnkpNTVViYqJOOeUUjR07Vps3bw539wAAARIX7g4AABBOmzZtUkxMeL6HPHDggC6++GKtWLFCf/jDH/SnP/1JDRs21KZNmzR79mz985//VFlZWVj6BgAILIIXACBqlJeXy2azKSEhwefHmM3mIPaoaiNGjNCqVav0/vvva+jQoU73PfTQQ7r//vsD8jw1+bkAAAKLqYYAgJD57bffNHLkSGVlZclsNuvUU0/Va6+95tSmrKxMkydPVvfu3ZWamqoGDRqoV69eWrhwoVO7HTt2yGQy6R//+IeefvppnXzyyTKbzVq/fr0eeOABmUwmbd26VSNGjFBaWppSU1N1/fXXq7i42Ok4rmu8KqdNfvfdd5owYYIyMjLUoEEDXXHFFdq/f7/TY202mx544AE1b95cycnJ6tu3r9avX+/TurGlS5fq008/1Q033OAWuqSKQPiPf/zDfr1Pnz7q06ePW7sRI0aoTZs21f5cVq1apbi4OE2ZMsXtGJs2bZLJZNLzzz9vv+3w4cMaN26cWrZsKbPZrHbt2unxxx+XzWar8nUBADyj4gUACIm9e/fqnHPOkclk0tixY5WRkaH//e9/uuGGG1RYWKhx48ZJkgoLC/Xqq6/q2muv1U033aQjR45o5syZGjhwoJYtW6YzzzzT6bivv/66SkpKNGrUKJnNZjVu3Nh+3x//+Efl5ORo6tSpWrlypV599VVlZmbq8ccfr7a/t912m9LT0/X3v/9dO3bs0NNPP62xY8dqzpw59jaTJk3SE088ocsuu0wDBw7UmjVrNHDgQJWUlFR7/I8//liS9Oc//9mHn57/XH8uzZo1U+/evTV37lz9/e9/d2o7Z84cxcbG6qqrrpIkFRcXq3fv3vrtt980evRotWrVSosXL9akSZO0Z88ePf3000HpMwBEM4IXACAk7r//flmtVv34449q0qSJJGnMmDG69tpr9cADD2j06NFKSkpSenq6duzY4TQt7qabblLHjh313HPPaebMmU7H/fXXX7V161ZlZGS4PWfXrl2d2v/++++aOXOmT8GrSZMmWrBggUwmk6SK6tazzz6rgoICpaamau/evZo+fbqGDBmiDz/80P64KVOm6IEHHqj2+Bs2bJAknX766dW2rQlPP5err75ao0eP1rp163TaaafZb58zZ4569+6trKwsSdL06dP1888/a9WqVWrfvr0kafTo0WrevLmefPJJTZw4US1btgxKvwEgWjHVEAAQdIZh6N///rcuu+wyGYahAwcO2P8MHDhQBQUFWrlypSQpNjbWHrpsNpsOHjyo8vJy9ejRw97G0dChQz2GLqki2Dnq1auXfv/9dxUWFlbb51GjRtlDV+VjrVarfvnlF0nSF198ofLyct1yyy1Oj7vtttuqPbYkex8aNWrkU3t/efq5XHnllYqLi3Oq2q1bt07r16/X1Vdfbb/tvffeU69evZSenu70b9W/f39ZrVZ98803QekzAEQzKl4AgKDbv3+/Dh8+rH/+85/65z//6bHNvn377JffeOMNTZs2TRs3bpTFYrHfnpOT4/Y4T7dVatWqldP19PR0SdKhQ4eUkpJSZZ+reqwkewBr166dU7vGjRvb21al8vmPHDmitLS0atv7y9PPpWnTpurXr5/mzp2rhx56SFJFtSsuLk5XXnmlvd2WLVu0du1ar4HW8d8KAOAbghcAIOgqN2S47rrrNHz4cI9tunTpIkl66623NGLECA0ZMkR33XWXMjMzFRsbq6lTp+rnn392e1xSUpLX542NjfV4u2EY1fa5No/1RceOHSVJP/74o3r16lVte5PJ5PG5rVarx/befi7XXHONrr/+eq1evVpnnnmm5s6dq379+qlp06b2NjabTRdddJHuvvtuj8c45ZRTqu0vAMAZwQsAEHQZGRlq1KiRrFar+vfvX2Xb999/X23bttUHH3zgNNXPdUOIcGvdurUkaevWrU7Vpd9//91eFavKZZddpqlTp+qtt97yKXilp6dr27ZtbrdXVt58NWTIEI0ePdo+3XDz5s2aNGmSU5uTTz5ZRUVF1f5bAQB8xxovAEDQxcbGaujQofr3v/+tdevWud3vuE17ZaXJsbqzdOlSLVmyJPgd9UO/fv0UFxenl156yel2xy3Zq5Kbm6uLL75Yr776qj766CO3+8vKynTnnXfar5988snauHGj089qzZo1+u677/zqd1pamgYOHKi5c+dq9uzZSkhI0JAhQ5za/PGPf9SSJUv02WefuT3+8OHDKi8v9+s5AQBUvAAAAfTaa69p/vz5brffcccdeuyxx7Rw4UL17NlTN910kzp37qyDBw9q5cqV+vzzz3Xw4EFJ0h/+8Ad98MEHuuKKK3TppZdq+/btmjFjhjp37qyioqJQvySvsrKydMcdd2jatGkaPHiwLr74Yq1Zs0b/+9//1LRpU6dqnTdvvvmmBgwYoCuvvFKXXXaZ+vXrpwYNGmjLli2aPXu29uzZYz+X18iRIzV9+nQNHDhQN9xwg/bt26cZM2bo1FNP9WmzEEdXX321rrvuOr344osaOHCg2xqzu+66Sx9//LH+8Ic/aMSIEerevbuOHj2qH3/8Ue+//7527NjhNDURAFA9ghcAIGBcqz+VRowYoRYtWmjZsmV68MEH9cEHH+jFF19UkyZNdOqppzpt7z5ixAjl5+fr5Zdf1meffabOnTvrrbfe0nvvvaevvvoqRK/EN48//riSk5P1yiuv6PPPP1dubq4WLFig888/X4mJidU+PiMjQ4sXL9aLL76oOXPm6P7771dZWZlat26twYMH64477rC37dSpk958801NnjxZEyZMUOfOnfWvf/1L77zzjt8/l8GDByspKUlHjhxx2s2wUnJysr7++ms9+uijeu+99/Tmm28qJSVFp5xyiqZMmaLU1FS/ng8AIJmMQK0SBgAAOnz4sNLT0/Xwww/r/vvvD3d3AAB1BGu8AACooWPHjrnd9vTTT0uS+vTpE9rOAADqNKYaAgBQQ3PmzNGsWbM0aNAgNWzYUIsWLdK7776rAQMG6Lzzzgt39wAAdQjBCwCAGurSpYvi4uL0xBNPqLCw0L7hxsMPPxzurgEA6hjWeAEAAABAkLHGCwAAAACCjOAFAAAAAEHGGi8/2Ww27d69W40aNfLp5JgAAAAAopNhGDpy5IiaN2+umJiqa1oELz/t3r1bLVu2DHc3AAAAANQRu3btUosWLapsQ/DyU6NGjSRV/HBTUlLC3BvJYrFowYIFGjBggOLj48PdHUQAxgz8xZiBvxgz8BdjBv6qK2OmsLBQLVu2tGeEqhC8/FQ5vTAlJaXOBK/k5GSlpKTwRgWfMGbgL8YM/MWYgb8YM/BXXRszvixBYnMNAAAAAAgyghcAAAAABBnBCwAAAACCjDVeAAAAQB1gGIbKy8tltVrD3ZU6z2KxKC4uTiUlJUH/ecXHxys2NrbWxyF4AQAAAGFWVlamPXv2qLi4ONxdiQiGYSg7O1u7du0K+rl1TSaTWrRooYYNG9bqOAQvAAAAIIxsNpu2b9+u2NhYNW/eXAkJCUEPE5HOZrOpqKhIDRs2rPbExbVhGIb279+vX3/9Ve3bt69V5YvgBQAAAIRRWVmZbDabWrZsqeTk5HB3JyLYbDaVlZUpMTExqMFLkjIyMrRjxw5ZLJZaBS821wAAAADqgGAHCNRMoKqP/OsCAAAAQJARvAAAAAAgyCIqeH3zzTe67LLL1Lx5c5lMJn300UdO9xuGocmTJ6tZs2ZKSkpS//79tWXLFqc2Bw8e1LBhw5SSkqK0tDTdcMMNKioqCuGrAAAAAFDfRFTwOnr0qM444wy98MILHu9/4okn9Oyzz2rGjBlaunSpGjRooIEDB6qkpMTeZtiwYfrpp5+Ul5enTz75RN98841GjRoVqpcAAAAARJUlS5YoNjZWl156adj6sGPHDplMJq1evbratrfffru6d+8us9msM888M+h9qxRRuxpecskluuSSSzzeZxiGnn76af31r3/V5ZdfLkl68803lZWVpY8++kjXXHONNmzYoPnz52v58uXq0aOHJOm5557ToEGD9I9//EPNmzcP2WsBAAAAosHMmTN12223aebMmdq9e3dEfKYeOXKkli5dqrVr14bsOSMqeFVl+/btys/PV//+/e23paamqmfPnlqyZImuueYaLVmyRGlpafbQJUn9+/dXTEyMli5dqiuuuMLtuKWlpSotLbVfLywslFRxtmyLxRLEV+Sbyj4Eoi//WbNH837M17SrTldDc9QMDbgI5JhB/cCYgb8YM/BXfR8zFotFhmHIZrPJZrNJqigqHLNYQ96XpPhYv3bxKyoq0pw5c7Rs2TLt2bNHr7/+uiZNmuTU5uOPP9Zdd92lXbt2KTc3V3/5y180cuRI/f7770pLS5MkLVq0SPfff79++OEHNW3aVEOGDNGjjz6qBg0aSJLatm2rm266SVu3btX777+v9PR0TZgwQbfddptsNptycnIkSV27dpUk9e7dW19++aXHPj/99NOSpH379mnt2rX2n7k3NptNhmF43E7enzEbNZ+u8/PzJUlZWVlOt2dlZdnvy8/PV2ZmptP9cXFxaty4sb2Nq6lTp2rKlCluty9YsKBOnWchLy+v1se4c0nFcLj39c81qFXVAxCRLxBjBvULYwb+YszAX/V1zMTFxSk7O1tFRUUqKyuTJB0rsyp3+vch78uSCecoKcH3c1W99dZbat++vZo1a6YrrrhC9913n2655RZ7ePvll1/0xz/+UaNHj9Zf/vIXrV27Vn/9618lSUeOHFFMTIy2b9+uQYMG6f7779fTTz+tAwcO6O6779aYMWPsS4xsNpumTZum++67T7fddpv+85//aOLEiTrvvPPUvn17ffHFF+rXr58++ugjdezYUQkJCfaCiTelpaWyWq3VtisrK9OxY8f0zTffqLy83Om+4uJin39WURO8gmXSpEmaMGGC/XphYaFatmypAQMGKCUlJYw9q2CxWJSXl6eLLrpI8fHxtTrWHUsWSJKantRagwZ1CkT3UAcFcsygfmDMwF+MGfirvo+ZkpIS7dq1Sw0bNlRiYqIkKa6svJpHBUejlEZKTvA9Irz77rv6y1/+opSUFF155ZW67bbbtGrVKvXp00eS9M4776hDhw565plnJEndu3fXtm3b9Oijj6pRo0ZKSUnR888/rz/96U+655577Md97rnn1LdvX73yyiv2kyQPGjTI/rm8S5cueumll7R8+XJ1795dbdq0kSS1bNlS7du396nvZrNZsbGx1X6mLykpUVJSki644AL7v0+l6kKbo6gJXtnZ2ZKkvXv3qlmzZvbb9+7da180l52drX379jk9rry8XAcPHrQ/3pXZbJbZbHa7PT4+vk69MQSyPzExMXXqtSE46toYRt3HmIG/GDPwV30dM1arVSaTSTExMfaTKDcwx2v9gwND3hd/phpu2rRJy5Yt04cffqiYmBglJCTo6quv1uuvv64LL7xQkrR582adddZZTieH7tmzpyTZX+/atWu1du1avfPOO/Y2lVMvf/nlF3XqVFEQOOOMM5yOk5mZqf379zv93BwvV6fydVbXPiYmRiaTyeP49Ge8Rk3wysnJUXZ2tr744gt70CosLNTSpUt18803S5Jyc3N1+PBhrVixQt27d5ckffnll7LZbPYBAMmQEe4uAAAA1Gsmk8mvylM4zJw5U+Xl5U6baRiGIbPZrOeff16pqak+HaeoqEijR4/W7bff7nZfq1at7JddQ47JZKp2fVZdUrf/NV0UFRVp69at9uvbt2/X6tWr1bhxY7Vq1Urjxo3Tww8/rPbt2ysnJ0d/+9vf1Lx5cw0ZMkSS1KlTJ1188cW66aabNGPGDFksFo0dO1bXXHNNROy+AgAAANQF5eXlevPNNzVt2jQNGDDA6b4hQ4bo3Xff1ZgxY9ShQwfNmzfP6f7ly5c7Xe/WrZvWr1+vdu3a1bg/CQkJkiqqh3VVRAWvH374QX379rVfr5zjOXz4cM2aNUt33323jh49qlGjRunw4cM6//zzNX/+fKe5mG+//bbGjh2rfv36KSYmRkOHDtWzzz4b8tdSl5nk+042AAAAqH8++eQTHTp0SDfccINbZWvo0KGaOXOmxowZo9GjR2v69Om65557dMMNN2j16tWaNWuWpBNT/e655x6dc845Gjt2rG688UY1aNBA69evV15enp5//nmf+pOZmamkpCTNnz9fLVq0UGJioteK29atW1VUVKT8/HwdO3bMfu6vzp072wNcMETUCZT79OkjwzDc/jj+4z344IPKz89XSUmJPv/8c51yyilOx2jcuLHeeecdHTlyRAUFBXrttdfUsGHDMLyauouphgAAAKjKzJkz1b9/f4/hZujQofrhhx+0du1a5eTk6P3339cHH3xg3xDj/vvvlyT7PgpdunTR119/rc2bN6tXr17q2rWrJk+e7NeMtLi4OD377LN6+eWX1bx5c/t5fT258cYb1bVrV7388svavHmzunbtqq5du2r37t1+/hT8E1EVLwAAAADh99///tfrfWeffbYM48QX+YMHD9bgwYPt1x955BF7VarSWWedpQULFng95o4dO9xu+/bbb512JLzxxht14403Vtv3r776qto2wUDwghumGgIAACBQXnzxRZ111llq0qSJvvvuOz355JMaO3ZsuLsVcgQvAAAAAEGzZcsWPfzwwzp48KBatWqliRMnatKkSeHuVsgRvOCGNV4AAAAIlKeeekpPPfVUuLsRdhG1uQYAAAAARCKCF9ywxgsAAAAILIIX3DDVEAAAAAgsghcAAAAABBnBC26YaggAAAAEFsELbphqCAAAAAQWwQsAAAAAgozgBQAAAKDGlixZotjYWF166aVh68OOHTtkMpm0evXqKtutWbNG1157rVq2bKmkpCR16tRJzzzzTEj6yAmUAQAAANTYzJkzddttt2nmzJnavXu3mjdvHu4uebVixQplZmbqrbfeUsuWLbV48WKNGjVKsbGxGjt2bFCfm4oXAAAAUNcYhlR2NPR/DP/W+hcVFWnOnDm6+eabdemll2rWrFlubT7++GO1b99eiYmJ6tu3r9544w2ZTCYdPnzY3mbRokXq1auXkpKS1LJlS91+++06evSo/f42bdro0Ucf1ciRI9WoUSO1adPG6blycnIkSV27dpXJZFKfPn089nfkyJF65pln1Lt3b7Vt21bXXXedrr/+en3wwQd+ve6aoOIFAAAA1DWWYunRMFSO7tstJTTwufncuXPVsWNHdejQQdddd53GjRunSZMmyWSq2CV7+/bt+r//+z/dcccduvHGG7Vq1SrdeeedTsf4+eefdfHFF+vhhx/Wa6+9pv3792vs2LEaO3asXn/9dXu7adOm6aGHHtJ9992n9957TxMnTtTAgQPVqVMnLVu2TGeffbY+//xznXrqqUpISPD5NRQUFKhx48Y+t68pKl4AAAAAamTmzJm67rrrJEkXX3yxCgoK9PXXX9vvf/nll9WhQwc9+eST6tChg6655hqNGDHC6RhTp07VsGHDNG7cOLVv317nnnuunn32Wb355psqKSmxtxs0aJBuueUWtWvXTnfffbeaNGmihQsXSpIyMjIkSU2aNFF2drbPQWrx4sWaM2eORo0aVZsfg0+oeAEAAAB1TXxyRfUpHM/ro02bNmnZsmX68MMPJUlxcXG6+uqrNXPmTPtUv02bNumss85yetzZZ5/tdH3NmjVau3at3n77bftthmHIZrNp+/bt6tSpkySpS5cu9vtNJpMyMzO1f/9+v16eo3Xr1unyyy/X3//+dw0YMKDGx/EVwQsAAACoa0wmv6b8hcPMmTNVXl7utJmGYRgym816/vnnlZqa6tNxioqKNHr0aN1+++1u97Vq1cp+OT4+3uk+k8kkm81Wo76vX79e/fr106hRo/TXv/61RsfwF8ELAAAAgF/Ky8v15ptvatq0aW7VoiFDhujdd9/VmDFj1KFDB82bN8/p/uXLlztd79atm9avX6927drVuD+Va7qsVmu1bX/66SddeOGFGj58uB555JEaP6e/WOMFAAAAwC+ffPKJDh06pBtuuEGnnXaa05+hQ4dq5syZkqTRo0dr48aNuueee7R582bNnTvXvhth5QYc99xzjxYvXqyxY8dq9erV2rJli/7zn//4tb17ZmamkpKSNH/+fO3du1cFBQUe261bt059+/bVgAEDNGHCBOXn5ys/P79WUxZ9RfCKcP9YsEUPrYxVwTFLuLsCAACAemLmzJnq37+/x+mEQ4cO1Q8//KC1a9cqJydH77//vj744AN16dJFL730ku6//35JktlsllSxduvrr7/W5s2b1atXL3Xt2lWTJ0/263xgcXFxevbZZ/Xyyy+refPmuvzyyz22e//997V//3699dZbatasmf2P6zq0YGCqYYR7+dvtkkz61/c7NX5Ax3B3BwAAAPXAf//7X6/3nX322TIczgc2ePBgDR482H79kUceUYsWLZSYmGi/7ayzztKCBQu8HnPHjh1ut3377bdKSUmxX7/xxht14403VtnvBx54QA888ECVbYKF4BUlym3+newOAAAACIUXX3xRZ511lpo0aaLvvvtOTz75pF/TCKMFwStK+HmScQAAACAktmzZoocfflgHDx5Uq1atNHHiRE2aNCnc3Qo5gleUMEheAAAAqIOeeuopPfXUU+HuRtixuUaUIHYBAAAAdRfBK0rYqHgBAABENGYw1U2B+ncheEWJQP4/5f88AABA6MTHx0uSiouLw9wTeFJWViZJio2NrdVxWOMVJah4AQAARKbY2FilpaVp3759kqTk5GT7yYXhmc1mU1lZmUpKShQTE7xaks1m0/79+5WcnKy4uNpFJ4JXlAhk7uL/OQAAQGhlZ2dLkj18oWqGYejYsWNKSkoKekiNiYlRq1atav08BK8oEch6F8UzAACA0DKZTGrWrJkyMzNlsVjC3Z06z2Kx6JtvvtEFF1xgn6oZLAkJCQGpqhG8ogRTDQEAACJfbGxsrdcS1QexsbEqLy9XYmJi0INXoLC5RgRz3GGFqYYAAABA3UXwimA2h7DFVEMAAACg7iJ4RTCrzbHiVbu0xHkjAAAAgOAheEUw5+BVu2M5Pp6phgAAAEBgEbwiWLnNZr9c2801HB9N8QsAAAAILIJXBHPIXU7rvWqCqYYAAABA8BC8Iphjxau2wam2wQ0AAACAdwSvCOa4xqu8lsnJCOi+iAAAAAAcEbwimNWhylVure2uhrXtDQAAAABvCF4RzDFsOU47rAmCFwAAABA8BK8I5riToaW2FS+mGgIAAABBQ/CKYI7ruixWKl4AAABAXUXwimD+bK5hsxlVhrPangcMAAAAgHcErwjmFLyqqXhd+dJinfXI5yqxWD3eT+wCAAAAgofgFcH8qXit3nVYh4stWr3rsMf7KXgBAAAAwUPwimDOa7xqmZwIXgAAAEDQELwimHPFq3aba7DGCwAAAAgeglcEcwxelvLabicPAAAAIFgIXhEskBUvg4oXAAAAEDQErwhm9XGNly+hynFvDiIYAAAAEFgErwjmWOWqaldDazU7HkqS4RC3qH4BAAAAgUXwimCOG2JUdR4vH3KXU5mL3AUAAAAEFsErgpVbfTuPl2NAM3ltc+IywQsAAAAILIJXBHMMVFWv8XK47K2Nwz1sLQ8AAAAEFsErgpX7uKuhL0HKl3AGAAAAoGYIXhHMcdOMqjbQ8Cl4OV4meQEAAAABRfCKYI5hq6qw5MvmGjYbuxoCAAAAwULwimCOUw2tVYQlf4MUsQsAAAAILIJXBHOteHkLWD6dx8tpV0OiFwAAABBIBK8I5hqovAUsX7aKd1wH5tN5vwAAAAD4jOAVwdyCl5dU5VjB8lbNMrxcBgAAAFB7URW8HnjgAZlMJqc/HTt2tN9fUlKiW2+9VU2aNFHDhg01dOhQ7d27N4w9rh3X4OW9muX5svNj2VwDAAAACJaoCl6SdOqpp2rPnj32P4sWLbLfN378eP33v//Ve++9p6+//lq7d+/WlVdeGcbe1o7vUw2rPzmyL9MRAQAAANRMXLg7EGhxcXHKzs52u72goEAzZ87UO++8owsvvFCS9Prrr6tTp076/vvvdc4553g8XmlpqUpLS+3XCwsLJUkWi0UWiyUIr8B3pZZy5+tlZUqIcU9NZQ79LPPSb8fbrDabW5v1ewq1cNMB3Xhea5njY2vbdYRR5b9tuMcvIgdjBv5izMBfjBn4q66MGX+eP+qC15YtW9S8eXMlJiYqNzdXU6dOVatWrbRixQpZLBb179/f3rZjx45q1aqVlixZ4jV4TZ06VVOmTHG7fcGCBUpOTg7a6/DFhl9Nkk6EoPmf5alBvHu730ukyn/qZct/0NGt7uEsv/hEmz179mjevN+c7r9jScV9Gzdt0iUtKYlFg7y8vHB3ARGGMQN/MWbgL8YM/BXuMVNcXOxz26gKXj179tSsWbPUoUMH7dmzR1OmTFGvXr20bt065efnKyEhQWlpaU6PycrKUn5+vtdjTpo0SRMmTLBfLywsVMuWLTVgwAClpKQE66X45OeFP0u7frZfv7B/fzVpkODW7peDxdKqiimX3bp3V7+OmW5ttuwt0tQ1iyVJmVnZGjToTKf771iyQJJkaZitQYO6BuolIAwsFovy8vJ00UUXKT7eQ1IHXDBm4C/GDPzFmIG/6sqYqZwN54uoCl6XXHKJ/XKXLl3Us2dPtW7dWnPnzlVSUlKNjmk2m2U2m91uj4+PrwNvDCana7GxcR77FBMT63TZY5u4E21MJpPX11bVfYgsdWMMI5IwZuAvxgz8xZiBv8I9Zvx57qjbXMNRWlqaTjnlFG3dulXZ2dkqKyvT4cOHndrs3bvX45qwSOC6fbzjxhkHj5aptNx6/HbHNp6P5XQC5YD1EAAAAIAU5cGrqKhIP//8s5o1a6bu3bsrPj5eX3zxhf3+TZs2aefOncrNzQ1jL2uu3MuuhvkFJer2UJ4u/MfXknw8j5fTroZVRS9TFfcBAAAA8CSqgtedd96pr7/+Wjt27NDixYt1xRVXKDY2Vtdee61SU1N1ww03aMKECVq4cKFWrFih66+/Xrm5uV431qjrUhLj1Tw10X69Mnh9s2W/JOm3w8ck+VbxsjmFswB3tI77busBTZy7RgXH2EkJAAAAwRFVa7x+/fVXXXvttfr999+VkZGh888/X99//70yMjIkSU899ZRiYmI0dOhQlZaWauDAgXrxxRfD3Ouau7VvO406v7VO+/tnKrWZ7OHJtSbly3m8HNWz3KVhry6VJCUnxOqhIaeFuTcAAACIRlEVvGbPnl3l/YmJiXrhhRf0wgsvhKhHoWE6nrR8qWZ5C16GU1WsvkWvCr8e8n07UAAAAMAfUTXVsL6q/EesnGpoMjnXvJzXb3k+Rn2eaggAAAAEG8ErCpyoeHlOTD5VvLxcBgAAAFB7BK8oEHM8eNkrXi73Wx3mIFq9zEf0ZedDAAAAADVD8IoClf+I3iteJy57n2pYfRsAAAAANUPwigL2qYY25+tSRfXK8GlXQ4eKF5MNAQAAgIAieEUB+1RDD6HKZvh2Hi+nXQ1tAewcAAAAAIJXNKgscJ3Y1fDEfeU2Ww0216DiBQAAAAQSwSsKxFSxq6HN5rpVvJd1YDbHNoHtHwAAAFDfEbyigD142Xc1PFHyKrfZXE6O7PkYbCcPAAAABA/BKwrYpxr6UPHyOtXQaVdDohcAAAAQSASvKOC6q6GjcpvN6dxd3jfXYKohAAAAECwEryhQ+Y9YWfFyrGpZDcOnahZTDQEAAIDgIXhFAdc1Xo4VLqvNcA5iXkpezuvAiF4AAABAIBG8okDlGi+bp4qXzfDpPF42phoCAAAAQUPwigL2EyjbK14n7nOtePl2Hi8AAAAAgUTwigIml/N4WV0qXoYP5/HypQ0AAACAmiF4RQH75hrHK102tzVeJ9r6dB4vlzYEMQAAAKB2CF5RIMbkXOly2lzD8HGqocPtOw8Wq9xhvmIwc9exMqvyC0qC9wQAAABAHUDwigKVUw0ND5trlFsNH8/jdeJywTGLxry10n7d04mZA2XkrOU6//Evta+Q8AUAAIDoRfCKAiemGrpXvGy+nsfL5ebPN+y1X/a2BX0gbNlXpHKboV8PHwvacwAAAADhRvCKAibXXQ0dK14+7mpY1bm7gnler1KLVVJFZQ4AAACIVgSvKBDjsquh4+YathpsruEqiAUvlZQfD142WzUtg4ONQwAAABAKBK8o4LqroeN5vHyteHnLHzaboVveXun5TgdPzN+oiXPX+BVkrDZDluOVrnBVvII5jRIAAACoRPCKAlWdx8vmch4vm5eg4S0wfbNlv77ZvL/K5zcMQy9+9bP+vfJXbd5b5HO/S45PM5TCF4CCuXEIAAAAUIngFQUq/xE9TTUsr+VUw9+Lyqp9/tLyEyU2fwKUY/CyWMMz1TBMMxwBAABQz8SFuwOovcqK1/4jpbrsuUXatv9E1clay6mGxWXlLu3cGzoGqLhYk6/dVolDYCun4gUAAIAoRvCKApWba7z67XYdcwhB0vHg5RBqvOUMw0vNq6jU5Xgeg1fNTrZ8rOzEscMWvFjjBQAAgBBgqmEUcD2PlyP3qYbetpP3fGzXipen5zhWwymDjpWy8rBNNXQIpWHpAQAAAOoDglcUqJxq2CjRvYBpM3ydauit4uU61dC9jVOA8qOCVFpe+4rXByt/1blTv9BPuwtq9HjHCh6zDgEAdZFhGPrzzKW6/vVlnAYFiGAEryhQuaqqzEPVyNfNNbw5WupfxcufypXjFMWabic/Ye4a7S4o0YQ5a2r0eKdznvHLDABQB+07UqpvtxzQwk37dcTl9zKAyEHwigKVa7xcq1OS+3by3r4p8xY6jpb5ssbLcaphzXY1tNZye8GScmv1jTyw+lANBAAgnBx/PxnsxgtELIJXFKgMXp5yg9sJlL28YXvLHIXHLE7XPZ0HrKbbwjtWvPwJbIHkWMFjow0AQF3k+OvJ0+wWAJGB4BUFqvpHtPm4uYa34FXgGryq2dWw3I/KlfPasNr9IvF9E3tnjk/LOb0AAHWR4zT+cJ33EkDtEbyigKmK1OFa8fJ23ipvgcy14uWpMOW4LbxfUw0DsLlGbVl9+NkAABBOFoIXEBUIXlGgquBlNXw9j5dnrmu8PE41dAxQfgQvp/N4MdUQAACPyspP/H4ieAGRi+AVBar6R7RabT5NNfSUvAzDcApHkudwUtOphqXljo8LT+jxZat9AADCyTFsOf7uBBBZCF5RIKbKipdruPDczvCQvMptho66nEDZ8xqv2u9qWNsTKNc0MlHxAgDUdWXW8G9GBaD2CF5RoMqphjbfKl6eMsfR0nK3qYnVBS//zuPluJ08Uw0BAPDEUs4aLyAaELyiQJVTDW2q8jxe634r0Pg5q/XboWNujz1S4n5eMI8nUHbcXKOa8PLkZxs167vtkqrfTt4wDD0+f6PeXLKjymNK3nc1tNkMPfDxT/r3il89389UQwBAHedU8WKqIRCx4sLdAdRe9RUv7+fx+sNzi7w+1nUrecnz5hyOm2tU9Qthy94jemHhz5KkEeflVLud/Po9hXrpq4r2f8lt4/W4Vfli4z7NWrxDkjS0ewu3+6l4AQDqOscvJzmPFxC5qHhFgZgqVjhZbfJtcw0PCkvcg5enLdePlfm2ucaR0hMVNIvVppJqNtcoKD7x/DUNRb8XlVZ5vy/r3wAACCfH6YVlVLyAiEXwigJVbq7hWvHyI1wUHvNtqqFTxcvHRb/HLNZq14Y5PpVj20ByfFoqXgCAusjC5hpAVCB4RQH/zuNVu4qX63m8SixWLdt+0H69qvNxOT62xDV4eQg9jtWzQAQvT8GKqYYAQmVPwTGt310Y7m4gApWxuQbC4HBxmVb8csivz46oGsErClT1j1huM2o81dDT5hqu2eSLDfu0/8iJ6XxVTTV03EyjpMzmUvGqetOOYx6Cl6eTOVfFU3hjcw0AoZI79UsNevZb7TpYHO6uIMI4rutijRdC5dJnF2noS4v19eb94e5K1CB4RYGqKl42m+EUKPyZoVDoYXMN1zVe+46UOF2vnALx/opf9X8vLdb7K37VFS9+p637jjgFn4qphlWvDTvqELwc29qfy4eTNTv21lPwouIFINR+2l0Q7i4gwrCdPMLht8MVO15/9lN+mHsSPdjVMApUtcar3GY47UQY6KmGR0udq2KVa7XufG+NJOmHXw5JksbPWaNRF7S1t3ObaughERY7nLzZU2jyZZ57qaXqqpmViheAEKjtSeJRvzntasjmGggxPh4FDsErClRVtrTZDKeqj1+7GnraXMPl8Y5VKcnzWi1JOlBU6l7xKq96jVdRadXBy/GXj7dX5bhzoqeqmY2KF4AQcH2vBPzhdB4vQjwQsZhqGAUSHeJzYrzzP2m5y1RDH2bn2flS8Sp2qXh5m3seYzI5hacSt6mGHipepVVXqxx/+XgLTcfKnJ/TFVMNAYSCYwW/jF3p4Cd2NQSiA8ErCiQ7BK/05ASn+45ZrE4bYizZ9rvHEyN7krd+r9ttrtmk8lvcRsfTn7fpNCaTy+YaPmwnf9RpqqHz/TsOHNXsZbvs1719A+hYVat+cw2PhwCAWjvq8EWS6xdWQHU4jxdCjenRwUHwigLJsScSQ2pSvNN9hccsbuu67v33Wkn+rfeq5FoVqlzjVfm85VbD43FjY0xOVasSi63a7eSrqnj1+cdXeurzzfbr3n4RlVSzMyLn8QIQCo7rYYsIXvAT28kj1JgeHRwEryjgWPFq3MC54nW42OIWKP63rmJ3mtIafGvmukas8j9mWnJF8LLYDI9rqVynGhaVljtNl/D0zUpRNZtrOPI29cK5yub+HFanHR8JXgCCw7GCX8wHGvjJ8XccwQuh4PhlEVXWwCF4RQHH4OVa8So4ZvE6ha7UQxCpjmvwqpwyk5ZUEfjKrTaP3+a6TjV0ne740+5CvfrtNqcAVlzN5hqOvK0tO1bNroaOa9b8PS9YTSzeekBzf9hVfUMAUcWxgu8YwgBflDHVECHmuC6V96zA8WlXwwkTJvh8wOnTp9e4M6gZx+AV67K3/OFjFq87GXoKItVxrZ4VuUw1tFgNj8Er1uQ81fBwcZnT/aXlNj386QbFx8Zo+LltJLmex+vEZU/f9lmsNhmGIZPLSc1cN/Rw5TjF0duOjIH0p1eXSpI6N0vRaSelBv354Mxqq5gKGxfLd04ILccPLq6n4QCq43geLzZnQSg4rUulSh8wPgWvVatWOV1fuXKlysvL1aFDB0nS5s2bFRsbq+7duwe+h6hWnMNnSNeMVVBs8VrJqa6K5InNkFPAqfzPmFK5xstmU1GJ+4eKGJPJ6Zxah4o9b/CxbMfBE8HL4cPJsbITv3QOuYQ2qeJ1W22G4mKdg9exaoKX68/GZjMUU9WJ0WrBscq362AxwSvEDMPQ4OcX6ZjFqs/GXaB4whdCyHlzDT7EwD8WtpNHiLEuNTh8+uSxcOFC+5/LLrtMvXv31q+//qqVK1dq5cqV2rVrl/r27atLL7002P1FNVzfkMusNq/fVLy/4tcaPcdXm/frre9/kXSiFF25xqvcauhIqXuoKim3ulS8qt5Z0TAM+xnTKx9f6dBRz4/1tM6r1GUnRVeu67qCuc7rt0MnXs/vR93DYzD9eqhYT8zf6POOltFod0GJftpdqG37j2rrvqJwdwf1DNN2Qu9AUaken79R+46UhLsrtcYar9AoOGbRE/M3On3+qK8cZx3xZVHg+H0C5WnTpmnBggVKT0+335aenq6HH35YAwYM0MSJEwPaQfjH03Q5TxWiTflH9PzCrTV6jutfXy5JOqNFmttUw+Kycu0tdP8ld7TUeft416mGlSp3Ifz9aJlTOHM8H9fvR0s9PrbMalOSYp1uc1rjVeZhcw2Xn5fVZig+1q1ZQOx2eCPfHeI39ZGzlmvz3iL98nuxnv7j6SF97rpiy94j9svb9h9Vp2YpYexNYB06WqbUpPigVWslqcxa8f87NT6++sZww7Sd0Ltj9ip9t/V3rfjlkOaOzg13d2qFEygHV8ExixokxGrSB2s178d8fblxn+aPuyDc3QorviwKDr/n2hQWFmr//v1ut+/fv19Hjhzx8AiEUlKCe2rI9xCEBj79jcfH5zRt4HZbsodjStJvh4vtG2ZUBq+VOw9r/Jw1bm2PlZU7hSBPYbDimBWBZMte54pEqQ8VL08Ljp3WeJV7mGroUuHyth4uEBy/QQvlt2mGYWjz8Z/npz/uCdnz1jWOVa5oqnjlrd+rbg/naer/NgTtOYrLyvXYmlhd8uxiHfFwYnVUz/GDC9N2QuO7rb9LkpZtPxjmntSe4+83NtcIrJU7D+mshz/XuDmrNe/Hil2fN+YfqdEpd6KJ4/sU61IDx2T4ObL+8pe/6Ntvv9W0adN09tlnS5KWLl2qu+66S7169dIbb7wRlI7WFYWFhUpNTVVBQYFSUsL/jbnFYtG8efN0LPsMvfH9Lr3yl+6avWyXlu04qD0Fx7TroH8f8Fs3SdYvvxc73ZbZyKx9R9yrTHcOOEX/WFBxLq2XhnXTzW+vrPkLOS4hLka92jXVnoISrd9TaL89OyVRpzav+Hn/dviYNua7h/zz2jVRYpxzSFz88+/2wNeycZJOyWzkdP+uQ8X2UCJJvU/JUFyQqgbbDhzV9gNHJVVs+9+1ZVpQnsdVuc3Q15tPfFnSt0NT7du3T5mZmYox1Z91Tlv2FWnnwYqxfVJakjpmN6rmEZHhi4377Jf7dcwMynMcKCrVml8LJEldWqQqo6E5KM8TzTbmH7F/4dLQHKeeOY3D3KPgshk2j+8zice/yCsJctXPkPSlw/+NCztmKlDv7PGxMTJkqDyEm1ys3HnIvja6SYMEnRmi3x+h5G3MBNuaXw/rQJH7l8HB/DwQSJVr2wM9HnceLNaW419Sxpikvh2C8/ulNmyGTQNS8vV/gwcpPoyzMfzJBn4Hr+LiYt1555167bXXZLFUvAnExcXphhtu0JNPPqkGDdwrJtGkrgavQYPcB93IWcvtv3jiYkw+7do34aJT9PXm/VrxyyH7bbf3a69nv9ji1rZ9ZkNt2VekRuY4vTvqHP3huUV+979tRgNt23/U6/0dshpp014qqQAAAHD2UPdyXTMkcoKXX2u8rFarfvjhBz3yyCN68skn9fPPP0uSTj755KgPXJHo8aFd9NWmfTKZTOrbIUNb9hXJHBejLfuKdHJGQxWXlcswTkwTLCyxqHvrdA0/t41ufmuFFv9cMU3jtgvbqVurNDVKjNfWfUf03g+/6odfDtm/Cbnm7JY67aRUvXVDT/1+tFSNGyTo96IytUhP0rb9R3X3v9dKkjo1S9G9l3TU4q0H9PI32yRJt1/YXi0bJ2nXwWM6KT1J2/afqD4lJcTpwo6ZWrz1gNvUxMT4WDVLTdKug8Vqm9FAm6sIZy3Tk1VwzKJCL1OkzHGxatk4WVv3BT/gpSbFq4E5LuRrvGJMJrXNaKCf9x1VubVca9f+qC5dTldsbJAWtNVRjRuYFRsj7fdQwY1krZs00K+HjslqC84UJKvVqp83/KizundXAYusayw1KV4NzfH67XBx9Y0jnNVqdXuf+WbzAft050GnZ6v3KRlB7YNJJp2c2UA/7z8asGljby/dqbXHq7+PXXm6TCEsiGQ2SpQhI+revyp5GjOh0jI9WXuPlMhSbujkzIovhIO59CBQdvxerJe+qvgsftuF7dQiPSmgx0+Mj1XzNOfPZnWJ1WpV/O614e6GX/wKXrGxsRowYIA2bNignJwcdenSJVj9QgBkNDLrqh4t7debHJ8e1LVVureH2L3ylx66+e2VGnhqluJjY9TneIm5e+t0pSYl6IdfVkiqmBp4U6+2kqTz2zd1O06PNo3twWvgqVnqfUqGLmjfVGVWmwqOWfSHLs0UFxuj7q0r2p/Vxn36zYBTs7328+zj03V8eU3V6d669seo67q3biyLxaIGe9dqUPcWYf2GCJHDYrFo3t616tcpkzEDn3h6n2mRnmwPXsN6ttZ57dx/ZwRD99aBm9Z57slNdff7a3Vtz1YafEbzgB0XnsdMuARyzASTzWbo0NEyxcaYNOGiU9zOZRoonj6b1QWVv5siid+7Gp522mnatm2bcnJygtEf1BENzHF6c+TZHu/r3CxFMaaKc3r96exWykxJrPJYWSlm7S0s1eVnniRJMplM+vtlpwa8zwCAuqtbq3Q1SoyTDKlrq7Rwd6dGWjZO1rujzgl3NwBJUkyMSY8NpQgSSfwOXg8//LDuvPNOPfTQQ+revbvbFMO6sO7JFy+88IKefPJJ5efn64wzztBzzz1n3ywEVWvVJFmf3t5L+YUlOvfkJtW2/+9t56ug2OJxx0QAQP2QlBCrz45v0Z2c4PfHDwCIeH6/8w0aNEiSNHjwYKeSpmEYMplMslrr/vz/OXPmaMKECZoxY4Z69uypp59+WgMHDtSmTZuUmVn3dm2pizo1S/H5PEiZjRKV2ajqqhgAIPo1TwvsGhQAiCR+B6+FCxcGox8hNX36dN100026/vrrJUkzZszQp59+qtdee0333ntvmHsHAAAAINr4Hbx69+4djH6ETFlZmVasWKFJkybZb4uJiVH//v21ZMkSt/alpaUqLT2xg1BhYcW5pSwWi307/XCq7ENd6AsiA2MG/mLMwF+MGfiLMQN/1ZUx48/z13iSdXFxsXbu3KmyMudtvuv6TocHDhyQ1WpVVlaW0+1ZWVnauHGjW/upU6dqypQpbrcvWLBAycnJQeunv/Ly8sLdBUQYxgz8xZiBvxgz8BdjBv4K95gpLvb9FCF+B6/9+/fr+uuv1//+9z+P90fCGi9/TJo0SRMmTLBfLywsVMuWLTVgwIA6sZGIxWJRXl6eLrroorBvv4rIwJiBvxgz8BdjBv5izMBfdWXMVM6G84XfwWvcuHE6fPiwli5dqj59+ujDDz/U3r179fDDD2vatGn+Hi7kmjZtqtjYWO3du9fp9r179yo72/18UWazWWaz2e32+Pj4OvXGUNf6g7qPMQN/MWbgL8YM/MWYgb/CPWb8ee4Yfw/+5Zdfavr06erRo4diYmLUunVrXXfddXriiSc0depUfw8XcgkJCerevbu++OIL+202m01ffPGFcnNzw9gzAAAAANHK7+B19OhR+5br6enp2r9/vyTp9NNP18qVKwPbuyCZMGGCXnnlFb3xxhvasGGDbr75Zh09etS+yyEAAAAABJLfUw07dOigTZs2qU2bNjrjjDP08ssvq02bNpoxY4aaNWsWjD4G3NVXX639+/dr8uTJys/P15lnnqn58+e7bbgRtWw2KSbmxGWTSTKME7dV9ZjyMkmGZIqVbBYpPsm9jc0qlZdKCcknbrOWS5biivaGUdE+Js65fVyiVFZ04nix8RXty0sr/jiKjZdizVL5MSk+WSo94r3vCQ0kq0Wylnm+Pyau4nmqOkagxJkrfnYW3xdiBoQpRjI3rHiNFovirMcqLlvr2UlM4xIrxrvlWLh7EljmRhVjyhakNbbl5TLZyo//P4qyn10ohev/fziUl4f/fcZkqvi/UXrkxO+dQAj2/zdvKn9/lpeE9nlDJZxjJqGhZC2VbOXHx0yRZNhC24eaijt+ntRgjIvKz1qOn83qkvLyyPl3Os5kGP69G7311lsqLy/XiBEjtGLFCl188cU6ePCgEhISNGvWLF199dXB6mudUFhYqNTUVBUUFNSZzTXmzZunQYMG+TbH9JMJ0ob/SrcsqfiP9HJvqeSwlNpSuuX7ig/nrnYuld4aKvW4XloxSyqtXERoki57Wuo+QircI804T+p0mbTtK+nwTunc26UVr0tnDpPWfSAV5bsf+6wbpS0LpMO7JLkMxZh46dzbpKUvS5ajLg80ubevKVNMxP3HBQAAqO/mn/as+l3+p7BvruFrNvD7K4XrrrvOfrl79+765ZdftHHjRrVq1UpNmzb1v7cIrR9mVvy9dIZ0aEdF6JKkgl3S+v9IXYdVXLfZpCXPSa1ypfeGS2VHpMXPuhzMkL58ROpytbTmHan494pgVum7pyv+/v5F7/1Z/qr3+2wWadF0L3cG8NtLQhcAAACCzO/gtW3bNrVt29Z+PTk5Wd26dQtopxACpUXuU+8cp7/8OFfKm1xxuUGm9+Mc3Sf9slhKbhKYfiWmSRM3SYuekr5+7MTtp18lDX6+4vLSl6TPH3B/7O2rpUYu011fPEc6tL3i8tmjpYsedL5/+avSgvtPXL/r54opB8Ew705p1b8qLmd0kkZ9FZzncXX4F+mFs49fMcly5zbNz/tCF198seLj6tHOUR+OqvhyQZJO7idd8054+xMINos0tcWJ63+aK+UE/iT3tv/cqph171dc6TRYuvKVgD9H1PtknLTm3YrLjdtKNy8Ja3eCzVJu0fz588P3PlNSIE075cT18esD83tqyfPSlw9Jzc6URs5XxeyLEJj7F2nLZxWXO/5BGjozNM8bQmEbM0+dKhUfcL992L+lNueHrh81cfBnacb5kkwVs5bSWgXu2GvekT4ZX3G5QaY07sfAHTtALOUWlX72RfUN6xC/g1e7du3UokUL9e7dW3369FHv3r3Vrl27YPQNwVR2xH1+uuO6l/0OJ5M2qpnHfuyQlBygamdSuhSfWLEuy1F8UsXtkhSX5P44STKnnGhTKc7hekKyh/tdThXg+DyBFu9wwu04c/Cex1V6zonLsfGSuZFsMQkVP5v6tGWvuZHz5VD9/IMqUYpvcGIqboOMoLwuI8nhA2tykyj52YVYrMP/tZi4evAzjA3v+4zre3tK84o1X7V13jipYabUtq/zGudgc3r/8vC7LiqEacwkpXsOXpHweyLrVOnPH1Ysmcg4pfr2/miQceJy5WezOic2MP+vQ8jvXQ137dqlqVOnKikpSU888YROOeUUtWjRQsOGDdOrr1YxbQzhUXpEmn+f9OsKl9uLKhbKO6oMXnvWVlScKlW3gLikQAGb+peUXvG36y+0WIdforFevi/wdHtcgudjVHL9D2vy+7+E7xw/eMUmeG8XaI4/g0AuMI808Q5h3jXYR7LK/zOulwP6HGnBf45oZ4r1fBnB4fbeHqAPZ7FxUre/SGktA3M8XyUke76M2vP2nlYng4YHbftIORcE/rih+N1SD/n9KfOkk07SsGHD9M9//lObNm3Spk2b1L9/f82dO1ejR48ORh9RG18+LH3/gvTqhc63lxVVTFNyVH48eL3cy/n26tZAlRQEbnenyg94rt9WOl6P8fJNmKcw4xi24jzc7xq0ghm8HF+D6+tD8Dl+WImPog8ujl9SOAakQEp0OC6/gGsmJtbzZcAXjl8cRdP7V13g7T0tLkKCV7A4ve+neWsFP/k91bC4uFiLFi3SV199pa+++kqrVq1Sx44dNXbsWPXp0ycIXUSt/PqD59tLj7h/+C/zssVxdaGqtLD66Yi+Mh/fDcb1Dc8xVMV6CV6eAplT2PHwJhrK4OXLawi2xjnVt4lWThWvKPrg4vhNvjk1KE9hOFW80rw1Q1WcKl5BfJ/BCeZUqbTA+/T0SOJU8Yqiin1d4O09rb5/QeoYSB1DGGrF7+CVlpam9PR0DRs2TPfee6969eql9HS+Aa2zXM9/Van0iPt0F2/nsbKVV/0cJYUVuyAGQuUHEteQVG3Fy+T5W2THx3mqiIUteIX4Df26DyoWhF/mujNlPeL4YSU+ij64OE4frepcfLVBxav2qHiF3rD3pPn3SAOnhrsntRcfpRX7usDbLJpoCOy14RhIw/VlcRTyO3gNGjRIixYt0uzZs5Wfn6/8/Hz16dNHp5wS4EV9CIzyY55vLy1y/9bs2CHPbb1VsyoX9ZcUBK7iVcmt4uW4PsrDsI1N8DyHP7a66X2ujwniIs1wVrza9av4I0kWS9VtoxVrJGqOuf615/ilDmu8QqNVz9DtHhtsCVFasa9rYuJPLMOo7xUvAn5Q+P316EcffaQDBw5o/vz5ys3N1YIFC9SrVy/72i/UMd4qXmVH3CtZ3oKXt6mGKc0r/i4tDNwar8rw5PqGF1tNxctbkHFc1+XTVMMgBi/WeIVXtG6uEYIdnQzHihdTTmqGihdqwyl4BemUJ3B+P63va7wcfxYRtnNgXVbjeSmnn366zjvvPOXm5uqss87Svn37NGfOnED2DTW17BXpg1EVYai85MTtjtMBjx2Sdq9yflzlyZTdeNkJrzJ4lfi4xsuX6XWV06aqmmroKWR5C16x/kw1NAX3zSVcuxqigtPmGlEUvEKxU6Vj2HLc1hq+Y1dD1AZTDUOPL0hPqM87IgeY38Fr+vTpGjx4sJo0aaKePXvq3Xff1SmnnKJ///vf2r9/fzD6CH/Nu1NaO0faNE+yOAQvq5fqV6WjB6T9m31/HnvwKqh+50PJtylK3f5S8bfrNq6OQSXGw1RDr3O0q6kyOU3/CfKC9+pCIIIrWqfqnH/8BJedLw/eczjO9W+YFbzniWZUvFAb0fr+VRec+aeKv5ud6RwwqPJIzc6o+PuMa8Pbjyji9xqvd999V71799aoUaPUq1cvpaYGZxctBEDZUeeKV0lh1e2LD0gvnOX78StPrufrVMPkJlJRfsXlgY9Kn9134r6TekiXvyBldqy47nfFy0uQ8WdzjaAHLypeYRWt2zGf+Sep+ZlSk/bBe46YOM0/7Vn169tX8ZFybpu6hl0NURvxUVqxrwvanCfdukxKbSE91jrcvalbrv+fVPCrlNEh3D2JGn4Hr+XLlwejHwiGH15zPleX16mENZR4PHQfO+xbxSu58YnLKSdJ6TnSoe0V180NT4QuycMar2oqXl5PqlzdGi/HOcxB/jDkVH0jeIVctG7HbDJJWacG/WlK49OklGZBf56oRcULtUHFK7gqgwVVLmcJDQhdAVajT5rffvutrrvuOuXm5uq3336TJP3rX//SokWLAto51NKupc7Xjx2u/TEdg0yj7IowYzkq/byw+sc6TjWMMzuf+NU1TPm7nby30FTtVMMQLh512tWQ4BVy8VEavBAZ2NUQtcHmGkBU8Dt4/fvf/9bAgQOVlJSkVatWqbS0Yt1QQUGBHn300YB3EAFUUnDi8i1LvberiuOHh/Q2J9Zkbfq0+sc6Bq/YBOdwVV3wclof5Ueh1vE4YZ9qGMbzeMH5w0o0TTVEZKDihdpgcw0gKvj9SfPhhx/WjBkz9Morryg+/kTl4bzzztPKlSsD2jkEWGXwMsVWTOv7w1P+H8MUI/3f61Lve6XW50nZp/v+2OQmJy7HJTpXvFy/Aa7qPF6eKl7edtypdqphuIIXJyMMOabnIJzY1RC14fjlJOssg4fd+xBkfq/x2rRpky644AK321NTU3X48OFA9AmBltBQKiuSfphZcb3yQ3+PkdIn4/07lilGOu1K5+u+clzjFWd2qXi5Bi+z9+v+hJbq1lWFMnhxHq/wik+W0lpJZcUVfwOh5FTxYnMN+KlhVsWfmDjJzKZmQKTyO3hlZ2dr69atatOmjdPtixYtUtu2bQPVL9SUp29rGmZKB4uknUsqrnvanMJXbicc9uMDRJJD8KpuqqHJVDEdr3IL/Oo21/C2Pqu66X1OwSvYa7zY1TCsTCZp7ApJBhVHhB4VL9RGbJw07kdJJoJ7MLG5BoLM7/+9N910k+644w4tXbpUJpNJu3fv1ttvv60777xTN998czD6CH942tbd9bw7tVlf4Pam5MeblNPmGonO0yU8hSnH+2ta8XI8LufxQlwC1UaERyjfaxCd4szsiAtEOL9LH/fee69sNpv69eun4uJiXXDBBTKbzbrzzjt12223BaOP8IfhIXi5nrg42BWvBpnS0X3Ot8UlOq+xiUuQ4hx3NfQQBuMSJR1flxZbza6GXvvrEAw9fuA2ebkcBFS8gPrLsUrB5hpA3cQaLwSZ31+7mUwm3X///Tp48KDWrVun77//Xvv379dDDz2kY8eOBaOP8Iet3Pm6Kdb9Q74/wcVVdcHr7FHSxI3uj0to4FLxMbtUvDwFLy/rs2o6TazaqYacxwtAkDDVEADqvRp/0kxISFDnzp119tlnKz4+XtOnT1dOTk4g+4aacA1eCQ3cA0WtKl4uHxhcpx6aYjyHqPgG7tP+nKpYHvrktBV8NW29cfz2ytM29GHb1ZDgBdQrbCcP1H1s1Y8g8/mTZmlpqSZNmqQePXro3HPP1UcffSRJev3115WTk6OnnnpK48f7uUMeAs91jVdCA/dwVKs1Xq4VL9fg5eXYriesjTO7bBHvIRQ5bcYR7/lybTmdQDmEwYs1HkD94lTx4v8/UCf9aY7UqJn0xzfD3RNEKZ9LB5MnT9bLL7+s/v37a/Hixbrqqqt0/fXX6/vvv9f06dN11VVXKTaWb/HCzjV4xSfLbe2SY8g5qYf02w++H9/tA4Prsb18oHANgLFm56l3ngJbehtp1/cVl+OqWePlbV52dTsUhaviBaB+oeIF1H2tcz0vlwACxOfg9d577+nNN9/U4MGDtW7dOnXp0kXl5eVas2aNTGy/WXe4TTVMdg8fhu3E5WHvSVvypEPbpa+mVn/86tZ4VVXxcvywERtXfcUrvY1D+xpONfS0rstRuNZ4AahfWOMFAPWez580f/31V3Xv3l2SdNppp8lsNmv8+PGErrrGdVfD+AZyq0o5VsWSG0tnXC3FJ8kn1QUvb9/kJjSQss+Q2g+Qul9fcZvTubk8PK6xw5pBx/VZnqpq3sbhqUOkVrlSr4me7w9l8OJbbqD+YldDAKj3fC4dWK1WJSSc+KAcFxenhg0bBqVTqAWPm2tUUfGq5GvocFvT5ccar5iYigpbpeqqWK3P9a1PVYkzSyPne7/faY1X7Z8OADyi4gUA9Z7PwcswDI0YMUJmc8WH5ZKSEo0ZM0YNGjhvmvDBBx8Etofwj9vmGh7WeHk615evqaM2FS9XTlMNPTwurZV045eSuZFvfauJcJ3U1HHHRgDRjzVeAFDv+Ry8hg8f7nT9uuuuC3hnEABum2t4qHi5tpH8qHjVcI1XvIfgFefDuq0W3X3rV01Pehjq4NX3fmnvOqlt3+A/F4C6g10NAaDe8zl4vf7668HsBwLFdaphfKJUXuZ8m6eKl69r9dy+qfWyq2HWaRUBIyauok8eK17VrPEKhVAHr953B/85ANQ9VLwAoN7ja7do4xqqTLHhqXiNnC+N+ko67f8qrnuaLugUvGpxUmfJ9+Do9rgwTTUEUL+wxgsA6r1aftpFneNa8TKZ5L7Gy8PmGoFa41V53dxIat5V6nG9VFYkdb7c/VhOJxQOV8UrhCdQBlB/sashANR7BK9o41rNMsW4ZyqPFa+aBi+X+10/ULQ6p+KPJ3EBrHh52y6+WiYvlwEggKh4AUC9R/CKNp6Cly+7GvocvFy3j/dxcw1PAjHVsMs1Feummpxcs8cz1RBAKDit8eK9BgDqI979o43bVMMY39Z4BXs7eU+czuNVw2+AY2JrHrokgheA0KDiBQD1nk9lho8//tjnAw4ePLjGnUEAuAYveVrjFcDNNVyP7U94qe48Xr6o6TbylZyCF1MNAQQJX/IAQL3nU/AaMmSITwczmUyyWj1VUxAybrsamtwDhafNNWq8xqsWFS9fzuNVnYxTava4SnwYAhAKbCcPAPWeT592bTZPu+ChTvJljZdHvgYvlw8Mbmu+QrTGa+QCactn0jm3+Pc4VwQvAKHg9F5D8AKA+ojNNaKNx10NfQhVgTqPl19rvGoRvFr1rPhTWwQvAKFAxQsA6r0aBa+jR4/q66+/1s6dO1VWVuZ03+233x6QjqGGPG2u4Us1K1BTDWta8QpX6HE6jxdrvAAEidPmGnzJAwD1kd/Ba9WqVRo0aJCKi4t19OhRNW7cWAcOHFBycrIyMzMJXuHm6QTKwax4uYa6mq7xClfooeIFIBSoeAFAvef3J83x48frsssu06FDh5SUlKTvv/9ev/zyi7p3765//OMfwegj/OG2uUag13hVdx4vPwKU466Gnjb8CAWnihfBC0CQsJ08ANR7fn/SXL16tSZOnKiYmBjFxsaqtLRULVu21BNPPKH77rsvGH2Er8qKfVvj1X6g+2MDtcarplMNa7stfE1R8QIQClS8AKDe8/uTZnx8vGJiKh6WmZmpnTt3SpJSU1O1a9euwPYOPovJ+6v0aDNp11LnOzxVvK582f0AruFs6EypbV8P7VyDlmtHahi8PJ7UOQQIXgBCgV0NAaDe83uNV9euXbV8+XK1b99evXv31uTJk3XgwAH961//0mmnnRaMPsIHsctmVFxY/qrLPS5rvNr2lZLSPRzBoU1mZ+n0/5PKjkrbFro0C2DFy7Ffnk7qHAqcQBlAKFDxAoB6z++v+B999FE1a9ZMkvTII48oPT1dN998s/bv36+XX/ZQSUF4uVa8vIULp9uPX45P8nK8Kq7X9ANFuCpe8vC6ASDQ2NUQAOo9vytePXr0sF/OzMzU/PnzA9ohBJjbroY+BK/Ky467Dnpq5+l4NZ1CE7bNNZhqCCAEqHgBQL3n9yfNCy+8UIcPH3a7vbCwUBdeeGEg+oRA8rni5WHKXZyHildVj5OkmBqGlzox1ZDgBSBI2NUQAOo9vz9pfvXVV24nTZakkpISffvttwHpFALIbVdDb9PpPE01TPTt+E7Xa/iBosXZNXtcbRG8AISCY5WL9aQAUC/5PNVw7dq19svr169Xfn6+/brVatX8+fN10kknBbZ3qD238275UvE6ftmnilctTqAsSRM2Skd2S9lh2piFzTUAhAJf7ABAvedz8DrzzDNlMplkMpk8TilMSkrSc889F9DOIQB8rXh5WuMViopXSrOKP+FC2AIQCqzrAoB6z+fgtX37dhmGobZt22rZsmXKyMiw35eQkKDMzEzFxvKLpc7xdY2Xp6mGcT4EL7fNNSLsW12nLe3DdBJnANGPdV0AUO/5HLxat24tSbLZwrT7HGrG54qXp6mGNah4Rdq3upEWFAFEJsf3Rr7kAYB6ye/t5CXp559/1tNPP60NGzZIkjp37qw77rhDJ598ckA7hwCoyXm87FMNa7DGK9KCTKT1F0Bk4r0GAOo9v38TfPbZZ+rcubOWLVumLl26qEuXLlq6dKlOPfVU5eXlBaOPqBVfz+MV497GY8Wrms06qHgBgDumGgJAved3xevee+/V+PHj9dhjj7ndfs899+iiiy4KWOcQACaT/F7jVRlGfKp4BWg7+XBhV0MAoVDTcxwCAKKG378JNmzYoBtuuMHt9pEjR2r9+vUB6RQCyG2Nl7d2HsKZL9Ur1ngBgH/4kgcA6iW/P3VmZGRo9erVbrevXr1amZmZgegTAqkma7y8nmTZ4wNdrhK8AAAAAFc+TzV88MEHdeedd+qmm27SqFGjtG3bNp177rmSpO+++06PP/64JkyYELSOooZMPq7x8jTV0KfjR3jFS2wnDwAAgODzOXhNmTJFY8aM0d/+9jc1atRI06ZN06RJkyRJzZs31wMPPKDbb789aB1FFYwqtvj3ueJVw7VObmu8IqyCFGn9BQAAQETyOXgZx6sBJpNJ48eP1/jx43XkyBFJUqNGjYLTO/gkxij3fqfrGi9vQcNb2Or6Z2nT/6TiA749LtIqXqy1AAAAQAj4tauhyeVDKoGrbogxrN7vdK14+XMCZUm6/HnJWi491KT6x3m6Xtd52lQEAIIprXW4ewAACAO/PiWfcsopaty4cZV/wqlNmzYymUxOf1y3vV+7dq169eqlxMREtWzZUk888USYehs4puqCl0/hooo2sXHe74v07eQBIFSuny9d+YrUrEu4ewIACAO/Kl5TpkxRampqsPoSEA8++KBuuukm+3XHqlxhYaEGDBig/v37a8aMGfrxxx81cuRIpaWladSoUeHobkBUWfGSy3m8vFa8ari5huvxIm2qIQCESutcSbnh7gUAIEz8Cl7XXHNNnd8yvlGjRsrOzvZ439tvv62ysjK99tprSkhI0KmnnqrVq1dr+vTpER28AlLxcgpb/myuEeHbyTtiV0MAAAAEic/By3V9V1312GOP6aGHHlKrVq30pz/9SePHj1dcXMXLXLJkiS644AIlJCTY2w8cOFCPP/64Dh06pPT0dLfjlZaWqrS01H69sLBQkmSxWGSxWIL8aqpnsViqDF7lNptMNpsq45DNMGT10G+T1WofDDZDbm3iK++z2ZzvK7fa75Mki9Um1YGfiz/sr83LzybaVI7bujB+ERkYM/AXYwb+YszAX3VlzPjz/H7valiX3X777erWrZsaN26sxYsXa9KkSdqzZ4+mT58uScrPz1dOTo7TY7Kysuz3eQpeU6dO1ZQpU9xuX7BggZKTk4PwKvzXoIrgtWr1GjUs3atOx6//9tturZw3z61detEWXXD88r4DB7TUpc3llfft2+d0X5y1WJc6tFvw+Rcqj03y/0WEUeVr279/n7738LOJVnl5eeHuAiIMYwb+YszAX4wZ+CvcY6a4uNjntj4HL5utinNFBdG9996rxx9/vMo2GzZsUMeOHZ1O4NylSxclJCRo9OjRmjp1qsxmc42ef9KkSU7HLSwsVMuWLTVgwAClpKTU6JiBZLFY9P3Hs7ze37VbN5kObpf2VFw/qUULZQ8a5NbO9OtyaUvF5czMTA1ybbPKy32lR6S1J64OGHixlNCgBq8kjI6/towMD687ClksFuXl5emiiy5SfHx89Q9AvceYgb8YM/AXYwb+qitjpnI2nC/8WuMVDhMnTtSIESOqbNO2bVuPt/fs2VPl5eXasWOHOnTooOzsbO3du9epTeV1b+vCzGazx9AWHx9fZ94YqppqGBcXL8WeWHcVExOrGE/9jk+ovo2kmJgY5/tsCU73xyckSnXk5+Ivt9cW5erSGEZkYMzAX4wZ+IsxA3+Fe8z489x1PnhlZGQoIyOjRo9dvXq1YmJi7BuC5Obm6v7775fFYrH/kPLy8tShQweP0wwjRYz8OIGy140zariroWtbdjUEAAAA3ETY2W69W7JkiZ5++mmtWbNG27Zt09tvv63x48fruuuus4eqP/3pT0pISNANN9ygn376SXPmzNEzzzzjNJUwEpmMKqaBup5A2euuhl6vVNWQ83gBAAAAPqjzFS9fmc1mzZ49Ww888IBKS0uVk5Oj8ePHO4Wq1NRULViwQLfeequ6d++upk2bavLkyRG9lbzk53byXs/j5RCg/NnB0m07+cjY/dKjCNhABgAAAJEpaoJXt27d9P3331fbrkuXLvr2229D0KPQ8esEyl5zkQ9VMY8Pc614RXDwAgAAAIIkaqYa1mcmIwBrvGp8AuUoGkKERgAAAARJFH1qrr+qrHj5vMbL14qX63Q8wgoAAABQHYJXFKg6eJmCvKshwQsAAACoDsErClS9q6HrGq9ATzUkeAEAAADVIXhFgcCs8aphxSuasKshAAAAgqSefsKOLoFZ41XD7eQBAAAAVIvgFQUCch4v+dKmuvsiHIETAAAAQULwigKBr3gxLAAAAIBA4hN2FKj2BMp+r/Gi8gMAAAAEEsErCphUXcXL8bovoYrgBQAAAAQSwSsKBGSNF1MN2dUQAAAAQVNPP2FHl8Cs8XJsU9OeUCkDAAAAPCF4RYGqK16+rvGq4QmUvR4jArG2DQAAAEESF+4OoBYK90gFu5Vctt97G5NJPlW8nNrUMEARXAAAAACPCF6R7PsXFL/4ObWqqk2N1nhVEaCqDFcELwAAAMCTCJ8bVs/5UpmqyRqv+jrVEAAAAAgSPilHMlOsD21cK15eGzo/xlXq8bpa5yFVHIKKFwAAAOAJUw0jmU8VJh/XeFU31XDMN9Le9VLrc6t5LgAAAACuCF6RzNephj6t8aqm4pWULrU5r5rnIngBAAAAnjDVMJLF+DjV0N+KV40rVwQvAAAAwBOCVyQLZMXL6Xajhv0heAEAAACeELwimS9Bx9fzeAUiNLGrIQAAAOARn5QjmU+7Gpr8X+PFVEMAAAAgoAhekSyQ5/EKyFTDmj0MAAAAiHYEr0gW0F0NAzAUmGoIAAAAeMQn5UgW0F0NA1GuitCSV9c/V/x9wV3h7QcAAACiFufximS+nkCZilfVBj8nDXxUSkwJd08AAAAQpSL0kzIk+bHGy/G6L2u8aihSt5M3mQhdAAAACCqCVyRzDVVmD+HB5zVeERqaAAAAgAhA8IpkrsGrbW8vbXxZ41WPpxoCAAAAQcYar0jmGnTOHi3l9JYKd0uLpp9o49M5uurx5hoAAABAkFGiiGSuwSvOLJ19k5TZ2aGNSaGreBG8AAAAAE8IXpHMdTv5GA8FTJOvuxoGYnMNhhMAAADgCZ+UI5lr0LEHL8OljWPFy8dj1axDATgGAAAAEH0IXpHMNSzFxntuE7KKF8ELAAAA8ITgFclMrlMNjwcvw3BsJJ/WeAWkPwQvAAAAwBOCVyRzDTqxntZ4+bqrYUA6FMRjAwAAAJGL4BXJ3NZ4eZlq6G/Fy6li5k9/CF4AAACAJwSvSOa6q2Ft1ngFBMELAAAA8ITgFclqtKuhD+GoppUrKl4AAACARwSvSOY1eDm28fE8Xo5qPNWQ4QQAAAB4wiflSOa6q6HXqYYO/8xBrUpR8QIAAAA8IXhFMp8213DZTj6Y4YiphgAAAIBHBK9I5ha8jlfAXKcKOuWuYAYvhhMAAADgCZ+UI1nMiX8+Iya+ilDlZ8WrxuGMihcAAADgCcErkjlWmDxtrGFvx3m8AAAAgHAieEUyx+AVW0XwCtkaL4YTAAAA4AmflCOZ466GThUv1zVefla8at6hIB4bAAAAiFwEr0jm61TDkFW8gndoAAAAIJIRvCKZ0/m5qvinpOIFAAAAhBXBK5LFeJtq6CpEgYjNNQAAAACPCF6RzDHoOAYvt/N4BbniZU6t+PvkfoE/NgAAABAFqiqToK5zWuMV671dsNd43fydtPVz6YxrA39sAAAAIAoQvCJZXdnVMK2l1OP6wB8XAAAAiBJMNYxkQdvVsIYnUAYAAADgEcErkjntaljFVMOQ7WoIAAAAwBOCVyRzWNdlBHSNF+EMAAAACCSCVyTzNtXwpB4u7ah4AQAAAOHE5hqRzFvwyuwo3fSl1DC7sqHjg0LRMwAAAAAOCF6RrKrt5E/q7tDO34oXm2sAAAAAgcRUw0gWtF0NAQAAAAQSwSuSOe1qWEXwcspdbK4BAAAAhFrEBK9HHnlE5557rpKTk5WWluaxzc6dO3XppZcqOTlZmZmZuuuuu1ReXu7U5quvvlK3bt1kNpvVrl07zZo1K/idDxbH6YUB3dWQqYYAAABAIEVM8CorK9NVV12lm2++2eP9VqtVl156qcrKyrR48WK98cYbmjVrliZPnmxvs337dl166aXq27evVq9erXHjxunGG2/UZ599FqqXEVhVrfFyaseuhgAAAEA4RczmGlOmTJEkrxWqBQsWaP369fr888+VlZWlM888Uw899JDuuecePfDAA0pISNCMGTOUk5OjadOmSZI6deqkRYsW6amnntLAgQND9VICpyZrvEwRk7UBAACAqBExwas6S5Ys0emnn66srCz7bQMHDtTNN9+sn376SV27dtWSJUvUv39/p8cNHDhQ48aN83rc0tJSlZaW2q8XFhZKkiwWiywWS2BfhL+sNsUfv2gzTLJ664/Vam9XbrXK8NKuso3VZpMt3K8NQVM5bsM+fhExGDPwF2MG/mLMwF91Zcz48/xRE7zy8/OdQpck+/X8/Pwq2xQWFurYsWNKSkpyO+7UqVPt1TZHCxYsUHJycqC6XyPx5UUadPxy/t69WjFvnsd2KcW/qO/xy6tXr9FvOz33+/Ljf+/85Ret9XIsRI+8vLxwdwERhjEDfzFm4C/GDPwV7jFTXFzsc9uwBq97771Xjz/+eJVtNmzYoI4dO4aoR+4mTZqkCRMm2K8XFhaqZcuWGjBggFJSUsLWL0lSSYH0Y8XF7GbNNWjQIM/t9q6TNlVcPLNrV51xqpd2qyr+atW6tVpc7KUNIp7FYlFeXp4uuugixcfHV/8A1HuMGfiLMQN/MWbgr7oyZipnw/kirMFr4sSJGjFiRJVt2rZt69OxsrOztWzZMqfb9u7da7+v8u/K2xzbpKSkeKx2SZLZbJbZbHa7PT4+PvxvDLYT/YqJjVWMt/7Enbg9Li5OqqbfsTExig33a0PQ1YkxjIjCmIG/GDPwF2MG/gr3mPHnucMavDIyMpSRkRGQY+Xm5uqRRx7Rvn37lJmZKami9JiSkqLOnTvb28xzmUKXl5en3NzcgPQh5Jw2yqhit0ITJ1AGAAAAwilitrjbuXOnVq9erZ07d8pqtWr16tVavXq1ioqKJEkDBgxQ586d9ec//1lr1qzRZ599pr/+9a+69dZb7RWrMWPGaNu2bbr77ru1ceNGvfjii5o7d67Gjx8fzpdWc04nUK7qn5Lt5AEAAIBwipjNNSZPnqw33njDfr1r166SpIULF6pPnz6KjY3VJ598optvvlm5ublq0KCBhg8frgcffND+mJycHH366acaP368nnnmGbVo0UKvvvpqZG4lL0kmh3N3VRWo/K14seU8AAAAEFARE7xmzZrl9RxelVq3bu02ldBVnz59tGrVqgD2LIycAlIAKl59Jkkr/yX1mljbngEAAABwEDHBCx44TTUMQMWrz71S73uYjggAAAAEGHPKIlmMj8HLnzVehC4AAAAg4AheUcKoal0WuxoCAAAAYUXwihoBqngBAAAACDiCV7Sg4gUAAADUWQSvaOFrJYuKFwAAABByBK+oEcDzeAEAAAAIKIJXtKjypMes8QIAAADCieAVLVjjBQAAANRZBK9oUWWeouIFAAAAhBPBK1pQ8QIAAADqLIJXtPB5jVfQewIAAADABcErarCrIQAAAFBXEbyiRZVrt1jjBQAAAIQTwStaVLnGy/E+ghcAAAAQagSvqOHj5hpUvAAAAICQI3hFC1+nGlLxAgAAAEKO4BUtqgpeVLwAAACAsCJ4RQtft5On4gUAAACEHMErShhZp3u/k4oXAAAAEFYErwhnuekbrWx1k4xTLqmiFWELAAAACCeCV6TL7KxdTXr5vsYLAAAAQMgRvAAAAAAgyAhe9QEVLwAAACCsCF71AsELAAAACCeCV31AxQsAAAAIK4JXvUDwAgAAAMKJ4FUfUPECAAAAworgVS8QvAAAAIBwInjVB1S8AAAAgLAieNULBC8AAAAgnAhe9QEVLwAAACCsCF71AsELAAAACCeCV31AxQsAAAAIK4JXvUDwAgAAAMKJ4FUfUPECAAAAworgVS8QvAAAAIBwInjVB1S8AAAAgLAieNULBC8AAAAgnAhe9QEVLwAAACCsCF71AcELAAAACCuCFwAAAAAEGcELAAAAAIKM4AUAAAAAQUbwAgAAAIAgI3gBAAAAQJARvAAAAAAgyAheAAAAABBkBC8AAAAACDKCFwAAAAAEGcGrvsnsHO4eAAAAAPVOXLg7gBC5Z4dkOSYlNw53TwAAAIB6h+BVXySlV/wBAAAAEHJMNQQAAACAICN4AQAAAECQEbwAAAAAIMgIXgAAAAAQZAQvAAAAAAgyghcAAAAABBnBCwAAAACCjOAFAAAAAEFG8AIAAACAIIuY4PXII4/o3HPPVXJystLS0jy2MZlMbn9mz57t1Oarr75St27dZDab1a5dO82aNSv4nQcAAABQr0VM8CorK9NVV12lm2++ucp2r7/+uvbs2WP/M2TIEPt927dv16WXXqq+fftq9erVGjdunG688UZ99tlnQe49AAAAgPosLtwd8NWUKVMkqdoKVVpamrKzsz3eN2PGDOXk5GjatGmSpE6dOmnRokV66qmnNHDgwID2FwAAAAAqRUzw8tWtt96qG2+8UW3bttWYMWN0/fXXy2QySZKWLFmi/v37O7UfOHCgxo0b5/V4paWlKi0ttV8vLCyUJFksFlkslsC/AD9V9qEu9AWRgTEDfzFm4C/GDPzFmIG/6sqY8ef5oyp4Pfjgg7rwwguVnJysBQsW6JZbblFRUZFuv/12SVJ+fr6ysrKcHpOVlaXCwkIdO3ZMSUlJbsecOnWqvdrmaMGCBUpOTg7OC6mBvLy8cHcBEYYxA38xZuAvxgz8xZiBv8I9ZoqLi31uG9bgde+99+rxxx+vss2GDRvUsWNHn473t7/9zX65a9euOnr0qJ588kl78KqJSZMmacKECfbrBQUFatWqlXJzc9WoUaMaHzdQLBaLFi5cqL59+yo+Pj7c3UEEYMzAX4wZ+IsxA38xZuCvujJmjhw5IkkyDKPatmENXhMnTtSIESOqbNO2bdsaH79nz5566KGHVFpaKrPZrOzsbO3du9epzd69e5WSkuKx2iVJZrNZZrPZfr1yqmFOTk6N+wUAAAAgehw5ckSpqalVtglr8MrIyFBGRkbQjr969Wqlp6fbg1Nubq7mzZvn1CYvL0+5ubk+H7N58+batWuXGjVqZF87Fk6FhYVq2bKldu3apZSUlHB3BxGAMQN/MWbgL8YM/MWYgb/qypgxDENHjhxR8+bNq20bMWu8du7cqYMHD2rnzp2yWq1avXq1JKldu3Zq2LCh/vvf/2rv3r0655xzlJiYqLy8PD366KO688477ccYM2aMnn/+ed19990aOXKkvvzyS82dO1effvqpz/2IiYlRixYtAv3yai0lJYU3KviFMQN/MWbgL8YM/MWYgb/qwpiprtJVKWKC1+TJk/XGG2/Yr3ft2lWStHDhQvXp00fx8fF64YUXNH78eBmGoXbt2mn69Om66aab7I/JycnRp59+qvHjx+uZZ55RixYt9Oqrr7KVPAAAAICgMhm+rARDnVVYWKjU1FQVFBSEPe0jMjBm4C/GDPzFmIG/GDPwVySOmZhwdwC1Yzab9fe//91pAxCgKowZ+IsxA38xZuAvxgz8FYljhooXAAAAAAQZFS8AAAAACDKCFwAAAAAEGcELAAAAAIKM4AUAAAAAQUbwimAvvPCC2rRpo8TERPXs2VPLli0Ld5cQBlOnTtVZZ52lRo0aKTMzU0OGDNGmTZuc2pSUlOjWW29VkyZN1LBhQw0dOlR79+51arNz505deumlSk5OVmZmpu666y6Vl5eH8qUgTB577DGZTCaNGzfOfhtjBq5+++03XXfddWrSpImSkpJ0+umn64cffrDfbxiGJk+erGbNmikpKUn9+/fXli1bnI5x8OBBDRs2TCkpKUpLS9MNN9ygoqKiUL8UhIjVatXf/vY35eTkKCkpSSeffLIeeughOe7rxrip37755htddtllat68uUwmkz766COn+wM1PtauXatevXopMTFRLVu21BNPPBHsl+aZgYg0e/ZsIyEhwXjttdeMn376ybjpppuMtLQ0Y+/eveHuGkJs4MCBxuuvv26sW7fOWL16tTFo0CCjVatWRlFRkb3NmDFjjJYtWxpffPGF8cMPPxjnnHOOce6559rvLy8vN0477TSjf//+xqpVq4x58+YZTZs2NSZNmhSOl4QQWrZsmdGmTRujS5cuxh133GG/nTEDRwcPHjRat25tjBgxwli6dKmxbds247PPPjO2bt1qb/PYY48ZqampxkcffWSsWbPGGDx4sJGTk2McO3bM3ubiiy82zjjjDOP77783vv32W6Ndu3bGtddeG46XhBB45JFHjCZNmhiffPKJsX37duO9994zGjZsaDzzzDP2Noyb+m3evHnG/fffb3zwwQeGJOPDDz90uj8Q46OgoMDIysoyhg0bZqxbt8549913jaSkJOPll18O1cu0I3hFqLPPPtu49dZb7detVqvRvHlzY+rUqWHsFeqCffv2GZKMr7/+2jAMwzh8+LARHx9vvPfee/Y2GzZsMCQZS5YsMQyj4o0vJibGyM/Pt7d56aWXjJSUFKO0tDS0LwAhc+TIEaN9+/ZGXl6e0bt3b3vwYszA1T333GOcf/75Xu+32WxGdna28eSTT9pvO3z4sGE2m413333XMAzDWL9+vSHJWL58ub3N//73P8NkMhm//fZb8DqPsLn00kuNkSNHOt125ZVXGsOGDTMMg3EDZ67BK1Dj48UXXzTS09Odfjfdc889RocOHYL8itwx1TAClZWVacWKFerfv7/9tpiYGPXv319LliwJY89QFxQUFEiSGjduLElasWKFLBaL03jp2LGjWrVqZR8vS5Ys0emnn66srCx7m4EDB6qwsFA//fRTCHuPULr11lt16aWXOo0NiTEDdx9//LF69Oihq666SpmZmeratateeeUV+/3bt29Xfn6+05hJTU1Vz549ncZMWlqaevToYW/Tv39/xcTEaOnSpaF7MQiZc889V1988YU2b94sSVqzZo0WLVqkSy65RBLjBlUL1PhYsmSJLrjgAiUkJNjbDBw4UJs2bdKhQ4dC9GoqxIX02RAQBw4ckNVqdfrAI0lZWVnauHFjmHqFusBms2ncuHE677zzdNppp0mS8vPzlZCQoLS0NKe2WVlZys/Pt7fxNJ4q70P0mT17tlauXKnly5e73ceYgatt27bppZde0oQJE3Tfffdp+fLluv3225WQkKDhw4fb/809jQnHMZOZmel0f1xcnBo3bsyYiVL33nuvCgsL1bFjR8XGxspqteqRRx7RsGHDJIlxgyoFanzk5+crJyfH7RiV96Wnpwel/54QvIAocuutt2rdunVatGhRuLuCOmzXrl264447lJeXp8TExHB3BxHAZrOpR48eevTRRyVJXbt21bp16zRjxgwNHz48zL1DXTV37ly9/fbbeuedd3Tqqadq9erVGjdunJo3b864Qb3EVMMI1LRpU8XGxrrtMLZ3715lZ2eHqVcIt7Fjx+qTTz7RwoUL1aJFC/vt2dnZKisr0+HDh53aO46X7Oxsj+Op8j5ElxUrVmjfvn3q1q2b4uLiFBcXp6+//lrPPvus4uLilJWVxZiBk2bNmqlz585Ot3Xq1Ek7d+6UdOLfvKrfS9nZ2dq3b5/T/eXl5Tp48CBjJkrddddduvfee3XNNdfo9NNP15///GeNHz9eU6dOlcS4QdUCNT7q0u8rglcESkhIUPfu3fXFF1/Yb7PZbPriiy+Um5sbxp4hHAzD0NixY/Xhhx/qyy+/dCund+/eXfHx8U7jZdOmTdq5c6d9vOTm5urHH390evPKy8tTSkqK24ctRL5+/frpxx9/1OrVq+1/evTooWHDhtkvM2bg6LzzznM7TcXmzZvVunVrSVJOTo6ys7OdxkxhYaGWLl3qNGYOHz6sFStW2Nt8+eWXstls6tmzZwheBUKtuLhYMTHOHzVjY2Nls9kkMW5QtUCNj9zcXH3zzTeyWCz2Nnl5eerQoUNIpxlKYjv5SDV79mzDbDYbs2bNMtavX2+MGjXKSEtLc9phDPXDzTffbKSmphpfffWVsWfPHvuf4uJie5sxY8YYrVq1Mr788kvjhx9+MHJzc43c3Fz7/ZVbgw8YMMBYvXq1MX/+fCMjI4OtwesRx10NDYMxA2fLli0z4uLijEceecTYsmWL8fbbbxvJycnGW2+9ZW/z2GOPGWlpacZ//vMfY+3atcbll1/ucdvnrl27GkuXLjUWLVpktG/fnm3Bo9jw4cONk046yb6d/AcffGA0bdrUuPvuu+1tGDf125EjR4xVq1YZq1atMiQZ06dPN1atWmX88ssvhmEEZnwcPnzYyMrKMv785z8b69atM2bPnm0kJyeznTz889xzzxmtWrUyEhISjLPPPtv4/vvvw90lhIEkj39ef/11e5tjx44Zt9xyi5Genm4kJycbV1xxhbFnzx6n4+zYscO45JJLjKSkJKNp06bGxIkTDYvFEuJXg3BxDV6MGbj673//a5x22mmG2Ww2OnbsaPzzn/90ut9msxl/+9vfjKysLMNsNhv9+vUzNm3a5NTm999/N6699lqjYcOGRkpKinH99dcbR44cCeXLQAgVFhYad9xxh9GqVSsjMTHRaNu2rXH//fc7bevNuKnfFi5c6PEzzPDhww3DCNz4WLNmjXH++ecbZrPZOOmkk4zHHnssVC/RickwHE4fDgAAAAAIONZ4AQAAAECQEbwAAAAAIMgIXgAAAAAQZAQvAAAAAAgyghcAAAAABBnBCwAAAACCjOAFAAAAAEFG8AIAAACAICN4AQDgYseOHTKZTFq9enXQnmPEiBEaMmRI0I4PAKhbCF4AgKgzYsQImUwmtz8XX3yxT49v2bKl9uzZo9NOOy3IPQUA1Bdx4e4AAADBcPHFF+v11193us1sNvv02NjYWGVnZwejWwCAeoqKFwAgKpnNZmVnZzv9SU9PlySZTCa99NJLuuSSS5SUlKS2bdvq/ffftz/WdarhoUOHNGzYMGVkZCgpKUnt27d3CnU//vijLrzwQiUlJalJkyYaNWqUioqK7PdbrVZNmDBBaWlpatKkie6++24ZhuHUX5vNpqlTpyonJ0dJSUk644wznPoEAIhsBC8AQL30t7/9TUOHDtWaNWs0bNgwXXPNNdqwYYPXtuvXr9f//vc/bdiwQS+99JKaNm0qSTp69KgGDhyo9PR0LV++XO+9954+//xzjR071v74adOmadasWXrttde0aNEiHTx4UB9++KHTc0ydOlVvvvmmZsyYoZ9++knjx4/Xddddp6+//jp4PwQAQMiYDNev3AAAiHAjRozQW2+9pcTERKfb77vvPt13330ymUwaM2aMXnrpJft955xzjrp166YXX3xRO3bsUE5OjlatWqUzzzxTgwcPVtOmTfXaa6+5Pdcrr7yie+65R7t27VKDBg0kSfPmzdNll12m3bt3KysrS82bN9f48eN11113SZLKy8uVk5Oj7t2766OPPlJpaakaN26szz//XLm5ufZj33jjjSouLtY777wTjB8TACCEWOMFAIhKffv2dQpWktS4cWP7ZceAU3nd2y6GN998s4YOHaqVK1dqwIABGjJkiM4991xJ0oYNG3TGGWfYQ5cknXfeebLZbNq0aZMSExO1Z88e9ezZ035/XFycevToYZ9uuHXrVhUXF+uiiy5yet6ysjJ17drV/xcPAKhzCF4AgKjUoEEDtWvXLiDHuuSSS/TLL79o3rx5ysvLU79+/XTrrbfqH//4R0COX7ke7NNPP9VJJ53kdJ+vG4IAAOo21ngBAOql77//3u16p06dvLbPyMjQ8OHD9dZbb+npp5/WP//5T0lSp06dtGbNGh09etTe9rvvvlNMTIw6dOig1NRUNWvWTEuXLrXfX15erhUrVtivd+7cWWazWTt37lS7du2c/rRs2TJQLxkAEEZUvAAAUam0tFT5+flOt8XFxdk3xXjvvffUo0cPnX/++Xr77be1bNkyzZw50+OxJk+erO7du+vUU09VaWmpPvnkE3tIGzZsmP7+979r+PDheuCBB7R//37ddttt+vOf/6ysrCxJ0h133KHHHntM7du3V8eOHTV9+nQdPnzYfvxGjRrpzjvv1Pjx42Wz2XT++eeroKBA3333nVJSUjR8+PAg/IQAAKFE8AIARKX58+erWbNmTrd16NBBGzdulCRNmTJFs2fP1i233KJmzZrp3XffVefOnT0eKyEhQZMmTdKOHTuUlJSkXr16afbs2ZKk5ORkffbZZ7rjjjt01llnKTk5WUOHDtX06dPtj584caL27Nmj4cOHKyYmRiNHjtQVV1yhgoICe5uHHnpIGRkZmjp1qrZt26a0tDR169ZN9913X6B/NACAMGBXQwBAvWMymfThhx9qyJAh4e4KAKCeYI0XAAAAAAQZwQsAAAAAgow1XgCAeodZ9gCAUKPiBQAAAABBRvACAAAAgCAjeAEAAABAkBG8AAAAACDICF4AAAAAEGQELwAAAAAIMoIXAAAAAAQZwQsAAAAAguz/Abw6jvod+EgHAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = GridGame1()\n",
    "agent = NashQLearner(env, alpha=0.1, gamma=0.99, epsilon=0.5)\n",
    "\n",
    "all_episode_rewards = []\n",
    "epsilon_decay = []\n",
    "n_episodes = 1000\n",
    "\n",
    "for episode in range(n_episodes):\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    total_reward = [0, 0]\n",
    "\n",
    "    while not done:\n",
    "        actions = agent.get_action(state)\n",
    "        next_state, rewards, terminated, truncated, _ = env.step(actions)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        agent.update(state, actions, rewards, next_state)\n",
    "        state = next_state\n",
    "\n",
    "        total_reward[0] += rewards[0]\n",
    "        total_reward[1] += rewards[1]\n",
    "\n",
    "    all_episode_rewards.append(total_reward)\n",
    "    agent.decay_epsilon(episode)\n",
    "    epsilon_decay.append(agent.epsilon)\n",
    "\n",
    "    if episode % 10 == 0:\n",
    "        avg_rewards = np.mean(all_episode_rewards[-10:], axis=0)\n",
    "        print(f\"Episode {episode:03d} | Avg Reward: A1: {avg_rewards[0]:.2f}, A2: {avg_rewards[1]:.2f}\")\n",
    "\n",
    "# Plot learning curve\n",
    "agent.plot_learning_curve(all_episode_rewards)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nashpy import Game\n",
    "\n",
    "def print_equilibria(Q1_matrix, Q2_matrix):\n",
    "    game = Game(Q1_matrix, Q2_matrix)\n",
    "    equilibria = list(game.support_enumeration())\n",
    "\n",
    "    if len(equilibria) == 0:\n",
    "        print(\"No Nash equilibrium found.\")\n",
    "    elif len(equilibria) == 1:\n",
    "        print(\"One Nash equilibrium found:\")\n",
    "    else:\n",
    "        print(f\"{len(equilibria)} Nash equilibria found:\")\n",
    "\n",
    "    for i, (pi1, pi2) in enumerate(equilibria):\n",
    "        print(f\"Equilibrium {i+1}:\")\n",
    "        print(f\"  Agent 1 strategy: {pi1}\")\n",
    "        print(f\"  Agent 2 strategy: {pi2}\")\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_q_values(agent, agent_id=1, state=(1, 1, 1, 1)):\n",
    "    q_table = agent.Q1[state] if agent_id == 1 else agent.Q2[state]\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(q_table, annot=True, fmt=\".1f\", cmap=\"viridis\")\n",
    "\n",
    "    action_names = [\"UP\", \"DOWN\", \"LEFT\", \"RIGHT\"]\n",
    "    plt.xticks(ticks=[0.5, 1.5, 2.5, 3.5], labels=action_names)\n",
    "    plt.yticks(ticks=[0.5, 1.5, 2.5, 3.5], labels=action_names)\n",
    "\n",
    "    plt.xlabel(\"Agent 2 Actions\")\n",
    "    plt.ylabel(\"Agent 1 Actions\")\n",
    "    plt.title(f\"Q-Values for Agent {agent_id} in State {state}\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def visualize_trajectory(agent, env):\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    trajectory1 = [state[\"agent1\"]]\n",
    "    trajectory2 = [state[\"agent2\"]]\n",
    "\n",
    "    while not done:\n",
    "        actions = agent.get_action(state, explore=False)\n",
    "        next_state, _, terminated, truncated, _ = env.step(actions)\n",
    "        done = terminated or truncated\n",
    "        trajectory1.append(next_state[\"agent1\"])\n",
    "        trajectory2.append(next_state[\"agent2\"])\n",
    "        state = next_state\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "\n",
    "    def convert_2darray_coords(pos):\n",
    "        return (pos[1], 2 - pos[0])  # Flip row index for display\n",
    "\n",
    "    # Plot agent trajectories\n",
    "    plt.plot(*zip(*[convert_2darray_coords(p) for p in trajectory1]),\n",
    "             marker='o', label=\"Agent 1\", color='blue')\n",
    "    plt.plot(*zip(*[convert_2darray_coords(p) for p in trajectory2]),\n",
    "             marker='s', label=\"Agent 2\", color='red')\n",
    "\n",
    "    # Plot agent initial positions\n",
    "    init1 = convert_2darray_coords(env.init_positions[0])\n",
    "    init2 = convert_2darray_coords(env.init_positions[1])\n",
    "    plt.text(*init1, \"A1\", fontsize=12, ha='center', va='center', color='blue', fontweight='bold')\n",
    "    plt.text(*init2, \"A2\", fontsize=12, ha='center', va='center', color='red', fontweight='bold')\n",
    "\n",
    "    # Axis config\n",
    "    plt.xticks([0, 1, 2], ['0', '1', '2'])\n",
    "    plt.yticks([0, 1, 2], ['2', '1', '0'])\n",
    "    plt.grid(True)\n",
    "    plt.xlim(-0.5, 2.5)\n",
    "    plt.ylim(-0.5, 2.5)\n",
    "\n",
    "    plt.gca().xaxis.set_ticks_position('top')\n",
    "    plt.gca().xaxis.set_label_position('top')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.title(\"Agent Trajectories\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_exploration_decay(epsilon_decay):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(np.arange(len(epsilon_decay)), epsilon_decay)\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Epsilon\")\n",
    "    plt.title(\"Exploration Rate Decay\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_trajectory(agent, env)\n",
    "plot_exploration_decay(epsilon_decay)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
